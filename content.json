{"pages":[],"posts":[{"title":"GNN","text":"GNN图神经网络 在变换之后他的对称性不会改变，即原来连接什么顶点，现在也是 是一个信息传递的神经网络 输入是一个图，输出也是一个图 会更新点，线的信息，但连接性不会改变 下图就是一个GNN的实例 属性发生了变换，但是结构不变 顶点怎么排序都不会改变结果 下图就是最后得到结果的变换 如下图所示 当一个节点的向量信息不清楚，这时候就可以使用附近点的信息来更新这个点的向量信息 类似的，当只有点向量，没有边向量时，可以通过点向量来生成边向量 当只有边向量，没有点向量时同样如此 上图就是一个完整的GNN过程 input Graph先进入多层的mlp层中 然后得到一个属性发生改变但是结构并未改变的Transformed graph 然后根据这些点进行预测或则根据附近点进行补充信息，全连接层之后就可以得到信息了 但是现在GNN不能充分利用图中的信息，这时有一个优化方案 即信息传递 不单单只是将节点向量给到f中，而是将该节点附近所有的节点的信息加起来，一起传递到f中，然后得到结果 因此mlp之前我们可以做很多事情，例如如果有节点信息缺失，这时就可以通过附近的信息对节点信息进行更新，如下图","link":"/hexo_blog/2022/09/12/GNN/"},{"title":"FastDFS","text":"FastDFS 分布式文件系统得系统结构如上图 升级之后 简介功能有文件存储，文件同步，文件访问等 centos环境配置linux所需的 1yum install lrzsz wget vim unzip net-tools -y nginx与fdfs所需的 1yum install gcc perl openssl openssl-devel pcre pcre-devel zlib zlib-devel libevent libevent-devel -y centos命令防火墙1firewall-cmd --state 1systemctl stop firewalld.service 更换yum源123456cd /etc/yum.repos.d/mkdir bakmv * bak/wget https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo -O /etc/yum.repos.d/Centos-vault-8.5.2111.repowget https://mirrors.aliyun.com/repo/epel-archive-8.repo -O /etc/yum.repos.d/epel-archive-8.repoyum clean all &amp;&amp; yum makecache 整体架构由客户端与服务端组成 FastDFS提供了API访问 两个节点 跟踪器（tracker）和存储器（storage） 跟踪器主要负责调度工作，记录集群中storage节点的信息，是前端与后端存储结点的枢纽 安装 linux安装环境 1yum install gcc libevent libevent-devel -y 安装公共函数库 (文件在D;/tool/fastDFS中) 上传文件 rz -y 解压common压缩包 （unzip） 运行make.sh (./make.sh) 编译 安装 ./make.sh install 按照以上顺序在安装fastDFS master 启动修改配置文件 tracker.conf 修改base_path ,就是修改存放日志的位置 storage.conf 修改 base_path = /opt/fastdfs/storage tracker_server = 192.168.183.128:22122 store_path0 = /opt/fastdfs/storage/files 修改之后把所有的文件都移到/etc/fdfs中 启动storage 1fdfs_storaged /etc/fdfs/storage.conf 启动tracker 1fdfs_trackerd /etc/fdfs/tracker.conf 操作文件上传文件 1fdfs_test /etc/fdfs/client.conf upload a.txt 下载文件 12fdfs_test /etc/fdfs/client.conf download group1 M00/00/00/wKi3gGJKVyWALaJRAAAAE_-IeRQ599.txt 删除文件 12fdfs_test /etc/fdfs/client.conf delete group1 M00/00/00/wKi3gGJKVyWALaJRAAAAE_-IeRQ599.txt 安装nginx引入压缩包 压缩包为fastdfs-nginx-module-master.zip 下载nginx 1wget http://nginx.org/download/nginx-1.6.2.tar.gz 解压 1tar -zxvf nginx-1.6.2.tar.gz 配置nginx 来到nginx的解压目录，配置nginx 1./configure --prefix=/usr/local/nginx_fdfs --add-module=/mydata/fastdfs/fastdfs-nginx-module-master/src 然后依次执行以下命令 12makemake install 将扩展的conf文件复制到/etc/fdfs下 1cp mod_fastdfs.conf /etc/fdfs 配置mode_fastdfs.conf (fastdfs-nginx-module-master扩展模块解压之后的src中) 配置basepath 1base_path=/opt/fdfs/nginx_mod 配置trackerserver 1234tracker_server=192.168.183.128:22122url_have_group_name = truestore_path0=/opt/fdfs/storage/files 将conf文件移动到/etc/fdfs中 配置nginx（/usr/local/nginx_fdfs/conf/nginx.conf） 12345location ~ /group[1-9]/M0[0-9] { ngx_fastdfs_module; } 启动nginx12/usr/local/nginx_fdfs/sbin/nginx -c /usr/local/nginx_fdfs/conf/nginx.conf tracker的nginx配置不需要配置fdfs模块 安装 1./configure --prefix=/usr/local/nginx_fdfs 来到nginx的conf，这里只做上传的nginx负载均衡 1234567891011location ~ /group[1-9]/M0[0-9] { proxy_pass http://fastdfs_group_server; }这个放在server外 upstream fastdfs_group_server{ server socket 1; server socket 2; server socket 3; server socket 4;} storage配置nginx其他的与上文一致 1group_count=2 配置一下group的配置","link":"/hexo_blog/2022/04/09/FastDFS/"},{"title":"Harmony开发样例","text":"@[TOC](开发一个可以看小姐姐照片的鸿蒙应用 鸿蒙开发入门) 效果图先整张效果图，丑点是丑点，但可以用，买不起鸿蒙系统手机的我，只配用虚拟机。 前言要说目前最火的手机操作系统，要我来看的话那必然是鸿蒙无疑。16号刚刚结束了第五次鸿蒙内测，在看到这次的内测名单之后，居然有970的机器，这是不是说明俺这手里奋战了三年的荣耀play也可以生鸿蒙了，但现实是970三孤儿果然名不虚传，还是没有我们。那么言归正传，这次突然想做一个基于鸿蒙的小demo，然后又想到了我之前爬取的小姐姐图片链接还没有用武之地，这俩是不是可以结合一下？上次有这样的想法还是上一次，那么就做一个看小姐姐的小demo吧，开整开整。 实现思路之前在网上看到有直接把图片下载下来然后放进项目中的，这个很明显不适合我，不仅占的空间大，而且图片还得手动更新，这可不行 我们采用的是调用图片链接接口获取所有的图片链接，一个链接只是一个字符串要比图片占的空间小太多了，将这些链接存储在内存中，有兴趣的可以存在数据库里，然后每次随机获取一条链接就可以，由这条链接获取图片信息，将图片渲染到页面就可以。 整个流程简单的一塌糊涂，总结一下就是 拿取图片链接 由链接获取图片信息 渲染至显示页面 具体实现建立项目这个比较基础了，就不说了，如果不大了解的兄弟们，直接去官方文档看看就可以，建立流程非常简单。 建立http链接拿取图片链接设置网络权限我们需要访问网络，就必须要设置网络权限，来到config.json文件中，添加以下内容 1234567891011121314&quot;reqPermissions&quot;: [ { &quot;name&quot;: &quot;ohos.permission.INTERNET&quot; }, { &quot;name&quot;: &quot;com.wxr.xiaowpic.DataAbilityShellProvider.PROVIDER&quot; }, { &quot;name&quot;: &quot;ohos.permission.GET_NETWORK_INFO&quot; }, { &quot;name&quot;: &quot;ohos.permission.SET_NETWORK_INFO&quot; } ], 直接在module中添加如上内容，如下图 设置允许http请求这里注意，重点哈，鸿蒙默认的是发起https请求，因此如果我们发起http请求是会报错的，这里需要修改一下还是在config.json下，在deviceConfig中添加以下内容 12345&quot;default&quot;: { &quot;network&quot;: { &quot;cleartextTraffic&quot;: true } } 如下图所示，这里吐槽一下，我再寻找怎么设置允许发起http响应的时候，发现好多文章都一样，而且都不适用于我这个项目，还有的人复制别人的文章也能复制错，绝了。 发起http请求，并获取返回的数据json数据解析这里要使用到alibaba的fastjson工具类，在build.gradle引入如下依赖 1implementation group: 'com.alibaba', name: 'fastjson', version: '1.2.73' 如下图所示 发起请求，获得响应内容这里使用的是大佬封装好的专门用于请求接口的一个工具，ZZRHttp，同样需要引入依赖，引入过程和上面fastjson引入过程一致。 1implementation 'com.zzrv5.zzrhttp:ZZRHttp:1.0.1' 获取接口数据的具体实现如下，这里解释一下，https://2fd8e89d.cpolar.io/getAll这个接口地址，是获取图片链接的，是我本地的服务，所以大家如果需要的话，我可以把我的服务代码发给你们，包括存储图片链接的数据库。 12345678910111213141516171819202122232425ZZRHttp.get(&quot;https://2fd8e89d.cpolar.io/getAll&quot;, new ZZRCallBack.CallBackString() { @Override public void onFailure(int code, String errorMessage) { //http访问出错，此部分在主线程中工作,可以更新UI等操做。 MyLabel.error(&quot;访问图片链接接口出错&quot;); new ToastDialog(getContext()).setText(&quot;网络连接出问题了，请稍后重试&quot;).show(); } @Override public void onResponse(String response) { //http访问成功，此部分在主线程中工作，可以更新UI等操作。 MyLabel.info(&quot;获取图片链接成功&quot;); new ToastDialog(getContext()).setText(&quot;正在初始化，稍后&quot;).show(); //将字符串转换为json对象 JSONObject jsonObject = JSONObject.parseObject(response); //将其中返回的图片链接转换为列表 JSONArray info = (JSONArray) jsonObject.get(&quot;info&quot;); MyLabel.info(&quot;拿取数据量：&quot; + info.size()); info.forEach(item -&gt; { srcs.add(item.toString()); }); new ToastDialog(getContext()).setText(&quot;初始化成功，开始你的快乐吧&quot;).show(); MyLabel.info(&quot;内存中数据数量&quot; + srcs.size()); } }); 将获取的图片链接放入内存中就是声明一个静态列表变量，目的是为了下次获取图片链接时可以直接在这里拿取图片链接 123456789101112131415161718192021222324252627282930313233343536373839public void initData() { PicDao picDao = new PicDaoImpl(getContext()); //使用后台线程进行初始化 TaskDispatcher refreshUITask = createParallelTaskDispatcher(&quot;&quot;, TaskPriority.DEFAULT); refreshUITask.syncDispatch(() -&gt; {// List&lt;PicSrc&gt; list = picDao.list(); //判断内存中有无数据 if (srcs.size() == 0) { MyLabel.info(&quot;内存中没东西，第一次打开应用&quot;); MyLabel.info(&quot;调用图片接口获取图片链接列表&quot;); ZZRHttp.get(&quot;https://2fd8e89d.cpolar.io/getAll&quot;, new ZZRCallBack.CallBackString() { @Override public void onFailure(int code, String errorMessage) { //http访问出错，此部分在主线程中工作,可以更新UI等操做。 MyLabel.error(&quot;访问图片链接接口出错&quot;); new ToastDialog(getContext()).setText(&quot;网络连接出问题了，请稍后重试&quot;).show(); } @Override public void onResponse(String response) { //http访问成功，此部分在主线程中工作，可以更新UI等操作。 MyLabel.info(&quot;获取图片链接成功&quot;); new ToastDialog(getContext()).setText(&quot;正在初始化，稍后&quot;).show(); JSONObject jsonObject = JSONObject.parseObject(response); JSONArray info = (JSONArray) jsonObject.get(&quot;info&quot;); MyLabel.info(&quot;拿取数据量：&quot; + info.size()); info.forEach(item -&gt; { srcs.add(item.toString()); }); new ToastDialog(getContext()).setText(&quot;初始化成功，开始你的快乐吧&quot;).show(); MyLabel.info(&quot;内存中数据数量&quot; + srcs.size()); } }); } else { MyLabel.info(&quot;已经有内容了&quot;); } }); } 获取网络图片并展示在页面上http请求工具类这个类的主要作用就是发起http请求，并返回响应字节流，其实就是获取图片的字节流，代码如下 12345678910111213141516171819202122232425262728293031323334353637package com.wxr.xiaowpic.util;import com.wxr.xiaowpic.label.MyLabel;import com.zzrv5.mylibrary.ZZRCallBack;import com.zzrv5.mylibrary.ZZRHttp;import ohos.hiviewdfx.HiLog;import ohos.utils.net.Uri;import java.io.InputStream;import java.net.HttpURLConnection;import java.net.MalformedURLException;import java.net.URL;import java.net.URLConnection;public class HttpUtils { //url就是要访问的网络资源，methodType就是请求方式 public static InputStream getInput(String url,String methodType){ InputStream inputStream = null; try { URL url1=new URL(url); HttpURLConnection urlConnection = (HttpURLConnection) url1.openConnection(); urlConnection.setRequestMethod(methodType); urlConnection.connect(); int rescode=urlConnection.getResponseCode(); if(rescode==HttpURLConnection.HTTP_OK){ inputStream=urlConnection.getInputStream(); } } catch (Exception e) { HiLog.error(MyLabel.LABEL_LOG,e.getMessage()); HiLog.error(MyLabel.LABEL_LOG,e.getCause().toString()); } return inputStream; } } 字节流转图片工具类没啥好说的，复制就可以用 1234567891011121314151617181920212223242526272829303132333435package com.wxr.xiaowpic.util;import com.wxr.xiaowpic.label.MyLabel;import ohos.hiviewdfx.HiLog;import ohos.hiviewdfx.HiLogLabel;import ohos.media.image.ImageSource;import ohos.media.image.PixelMap;import java.io.InputStream;public class ImageUtils { private static final HiLogLabel LABEL_LOG = new HiLogLabel(3, 0xD001100, &quot;XiaoW&quot;); public static PixelMap createPixelMap(String imageUrl) {//获取图片字节流信息 InputStream inputStream = HttpUtils.getInput(imageUrl,&quot;GET&quot;); PixelMap pixelMap=null; ImageSource.SourceOptions sourceOptions = new ImageSource.SourceOptions(); sourceOptions.formatHint = &quot;image/jpeg&quot;; HiLog.info(MyLabel.LABEL_LOG,(inputStream==null)+&quot;&quot;); try { ImageSource imageSource = ImageSource.create(inputStream,sourceOptions); pixelMap = imageSource.createPixelmap(null); } catch (Exception e){ HiLog.info(LABEL_LOG,e.getMessage()); } return pixelMap; }} 图片展示在页面这里采用的是按钮点击之后进行图片的渲染，其中图片链接是在我们获取的图片链接随机读取一个，然后将该照片渲染至页面 123456789101112131415161718192021222324button.setClickedListener(new Component.ClickedListener() { @Override public void onClick(Component component) { TaskDispatcher refreshUITask = createParallelTaskDispatcher(&quot;&quot;, TaskPriority.DEFAULT); refreshUITask.syncDispatch(() -&gt; { //在链接列表中随机取一个数据 int index = (int) (Math.random() * srcs.size()); MyLabel.info(srcs.get(index)); String url=srcs.get(index); MyLabel.info(&quot;开始获取图片&quot;); //访问线上图片 PixelMap pixelMap = ImageUtils.createPixelMap(url); getContext().getUITaskDispatcher().asyncDispatch(new Runnable() { @Override public void run() { //Image组件填充位图数据，ui界面更新 image.setPixelMap(pixelMap); pixelMap.release(); } }); }); } }); 总结之前没有自己做过移动端的demo，总之收获还是不少的，所以期间出了不少问题，需要全部代码的兄弟们私信就好。","link":"/hexo_blog/2021/08/19/Harmony%E5%BC%80%E5%8F%91%E6%A0%B7%E4%BE%8B/"},{"title":"Python爬取美女图片","text":"简述作为一个考研狗，每天除了日复一日的复习外，偶尔也想给自己寻找一些生活的小乐趣，今天突然想到了自己曾经稍微接触的爬虫，想看看可以爬取些图片放到电脑上，就花了些时间改了改之前的爬虫代码，爬取了一部分照片先量一下战绩吧。照片不多但也算是自己的一次爬虫小经验。 实现思路爬虫的网页很简单，照片真实路径都在页面中直接可以拿到主要流程就是先进入照片浏览的主页，每个照片的主页都会链接几个照片页面，像下面这样，每个图片都会链接一个网页图片链接的网页如下图所示但是这个页面显示的图片还是不够高清，这个网站有一个规律，更高清的照片存放的网页就在现在这个页面的路径后跟一个 -1920x1080 的htm中，进入这个htm之后展示的照片才是我们要的，拿到图片的url就直接下载就好，就这样一直循环，所有的照片就都下载下来了。 关键代码文件下载1234567891011121314151617181920212223242526272829303132333435363738394041import requestsimport timedef downloadFile(name, url): try: headers = {'Proxy-Connection': 'keep-alive'} r = requests.get(url, stream=True, headers=headers) print(&quot;=========================&quot;) print(r) length = float(r.headers['Content-length']) f = open(name, 'wb') count = 0 count_tmp = 0 time1 = time.time() for chunk in r.iter_content(chunk_size=512): if chunk: f.write(chunk) # 写入文件 count += len(chunk) # 累加长度 # 计算时间 两秒打印一次 if time.time() - time1 &gt; 2: p = count / length * 100 speed = (count - count_tmp) / 1024 / 1024 / 2 count_tmp = count print(name + ': ' + formatFloat(p) + '%' + ' Speed: ' + formatFloat(speed) + 'M/S') time1 = time.time() f.close() return 1; except: print(&quot;出现异常&quot;) return 0;def formatFloat(num): return '{:.2f}'.format(num)if __name__ == '__main__': downloadFile('D://file//photo//hd.jpg', 'https://browser9.qhimg.com/bdr/__85/t01753453b660de14e9.jpg') 文件下载没什么好说的，复制就可以用，这里做了一个异常捕获的处理，因为可能出现连接不上资源，或则目标服务器强制关闭连接的可能，做这个异常处理就是为了判断有没有异常出现，从而进行相应的处理 爬虫代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# -*- codeing = utf-8 -*-# @Time : 2021/6/19 23:01# @Author : xiaow# @File : PhotoSpider.py# @Software : PyCharmfrom bs4 import BeautifulSoup # 网页解析import xlwt # excel操作import sqlite3 # 数据库操作from api import spider2 as spiderimport timefrom api import FileDownload as fdimport re # 正则表达式imglink = re.compile(r'&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot; title=&quot;.*?&quot;&gt;&lt;img alt=&quot;.*?&quot; src=&quot;.*?&quot;/&gt;&lt;b&gt;.*?&lt;/b&gt;&lt;/a&gt;', re.S)img2link = re.compile(r'&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot;&gt;.*?&lt;span&gt;（1680x1050）&lt;/span&gt;&lt;/a&gt;', re.S)img3link = re.compile(r'&lt;img alt=&quot;.*?&quot; src=&quot;(.*?)&quot; title=&quot;.*?&quot;/&gt;', re.S)# 获取照片页面路径def getPhoto(url): srcs = [] html = spider.askURL(url); bs = BeautifulSoup(html, &quot;html.parser&quot;); for item in bs.find_all('a', target=&quot;_blank&quot;): item = str(item) src = re.findall(imglink, item) if (len(src) != 0): srcs.append(&quot;http://www.netbian.com&quot; + src[0]) return srcs;# 照片主页显示的照片不够清楚，这里根据这个网站存储照片的规律，拼接了一个地址，这个地址的照片比较高清一些def getPhotoUrl(url): purls = []; url3 = &quot;http://www&quot;; url2 = url.split(&quot;.&quot;) for j in range(1, len(url2)): if j == len(url2) - 2: url3 = url3 + &quot;.&quot; + url2[j] + &quot;-1920x1080&quot; else: url3 = url3 + &quot;.&quot; + url2[j] return (url3)# 下载照片def downloadPhoto(url): html = spider.askURL(url); bs = BeautifulSoup(html, &quot;html.parser&quot;); for item in bs.find_all(&quot;img&quot;): item=str(item) itemsrc=re.findall(img3link,item) if(len(itemsrc)!=0): return itemsrc[0]if __name__ == '__main__': src = &quot;http://www.netbian.com/mei/index_&quot;; # 拼接照片主页的路径 for i in range(2,163): time.sleep(5) src2 = &quot;&quot;; src2=src+str(i)+&quot;.htm&quot; urls=getPhoto(src2) for j in range(len(urls)): time.sleep(3) fd.downloadFile('D://file//photo//hd'+str(time.time())+&quot;.jpg&quot;,downloadPhoto(getPhotoUrl(urls[j]))) 成果展示几张照片吧 更新解析网页的封装类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#-*- codeing = utf-8 -*-#@Time : 2021/3/1 16:16#@Author : xiaow#@File : spider2.py#@Software : PyCharmimport re # 正则表达式import sysimport urllib.request, urllib.error # 指定url，获取网页数据from bs4 import BeautifulSoup # 网页解析import xlwt # excel操作import sqlite3 # 数据库操作baseurl = 'https://movie.douban.com/top250?start='imglink = re.compile(r'&lt;a href=&quot;.*?&quot; title=&quot;.*?&quot;&gt;', re.S)# titlelink = re.compile(r'&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;')# findlink = re.compile(r'&lt;a href=&quot;(.*?)&quot;&gt;') # 创建正则表达式 表示规则# 1.爬取网页def getData(url): urllist = [] valuelist = [] # 2.解析数据 img = [] src = [] title = [] for i in range(0, 10): url = baseurl + str(i * 25) html = askURL(url) bs = BeautifulSoup(html, &quot;html.parser&quot;) print(bs) # urllist.append(bs.a.attrs[&quot;href&quot;]) # valuelist.append(bs.a.string) # return urllist, valuelist for item in bs.find_all('div', class_=&quot;item&quot;): # 查找div 并且该div应满足class=item # print(item) item = str(item) # titlel = re.findall(titlelink, item) # title.append(titlel) # srcl = re.findall(findlink, item) # 正则表达式进行筛选 # for s in srcl: # src.append(s) imgl = re.findall(imglink, item) # 正则表达式进行筛选 for i in imgl: img.append(i) return title, img, src;# 得到一个url的网页内容1def askURL(url): head = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36&quot;, &quot;Cookie&quot;: '_ga=GA1.2.1191993538.1623990557; _gid=GA1.2.176559558.1623990557; HstCfa3699098=1623990557028; HstCmu3699098=1623990557028; HstCnv3699098=1; HstCns3699098=1; newurl=0; __dtsu=10401623990557D693AE61F09F524965; pbnfgecookieinforecord=%2C64-32128%2C64-32129%2C; HstCla3699098=1623991353818; HstPn3699098=7; HstPt3699098=7' } req = urllib.request.Request(url=url, headers=head) html = &quot;&quot; try: response = urllib.request.urlopen(req) html = response.read() except Exception as result: print(result) return html# 3.保存数据def savaData(savepath): pass","link":"/hexo_blog/2021/08/20/Python%E7%88%AC%E5%8F%96%E7%BE%8E%E5%A5%B3%E5%9B%BE%E7%89%87/"},{"title":"SVM","text":"SVM什么是SVM 就是分类，实线被称为决策面，虚线上的样本点被称为支持向量，两条虚线之间的距离称为分类间隔 SVM的目的就是求出分类间隔最大的最优决策面 公式在优化之后，就是求以下d最大值，w是参数向量，x是属性向量，如上图所示，两个坐标轴可以认为是x1，x2 该公式就是求当w为何值时，d可以达到最大 这里为了求导方便，将问题转换为","link":"/hexo_blog/2022/09/30/SVM/"},{"title":"SpringSecurity","text":"SpringSecurity Username———处理认证问题 Exception———处理认证过程中的出现的异常 FilterSecurity———处理授权问题 认证认证过程","link":"/hexo_blog/2022/09/14/SpringSecurity/"},{"title":"arima","text":"ARIMA AR其实就是用历史的数据来预测现在的数据。 举个实例，例如七天的消费记录，p可以认为是前p天，当p为1即数据与前一天的数据有关，当p为2则与前两天的数据有关 其他参数可以看解释 平稳性：就是数据波动不大 **区别:**ACF会计算t到t-k数据的关系，PACF只考虑t和t-k的关系","link":"/hexo_blog/2022/07/08/arima/"},{"title":"csdn记录","text":"opencv+蔡徐坤 芜湖","link":"/hexo_blog/2022/07/25/csdn%E8%AE%B0%E5%BD%95/"},{"title":"juc","text":"JUC进程与线程 并行与并发并行（parrallel）：真正同时运行多个任务 并发（current）：同一时间应对多个任务，处理多个任务是轮流的，实际上还是串行的，每个进程轮流占据时间片，使用cpu，时间片很小，人对此无感 一般都是既有并行又有并发 异步调用 需要等待结果返回再进行，同步 不需要等待结果返回再进行，异步 小结 线程创建 Thread 1234567Thread thread=new Thread(){ @Override public void run() { log.info(&quot;thread&quot;); }};thread.start(); 使用Runable配合Thread 1234567Runnable runnable=new Runnable() { public void run() { log.debug(&quot;123&quot;); } }; Thread thread=new Thread(runnable,&quot;t1&quot;); thread.start(); lamda表达式 12345Thread thread = new Thread(() -&gt; { System.out.println(&quot;123&quot;);});thread.start(); FutureTask配合Thread 可以返回任务的执行结果 1234567891011FutureTask futureTask=new FutureTask(new Callable&lt;Integer&gt;() { @Override public Integer call() throws Exception { log.debug(&quot;right&quot;); Thread.sleep(1000); return 1; }});Thread thread=new Thread(futureTask,&quot;t1&quot;);thread.start();System.out.println(futureTask.get()); 这里call可以返回结果 get则是获取该结果，这个过程中，该线程是阻塞的，一直到返回结果之后 Thread与runable的关系二者最终还是走thread的run方法 推荐使用runable，这样可以与线程池配合 查看进程线程的方法 windows ​ tasklist查看进程 ​ taskkill 杀死进程 taskkill /F /PID 进程id linux ​ ps -fe 列出所有的运行进程信息 ​ ps -fe | grep java 根据java关键字查找所有的进程 ​ kill 进程号 ​ top 动态的方式查看进程的信息 ​ top -H -p 进程id 查看所有的线程信息 java ​ jps 查看进程 ​ jstack 进程号 该进程的所有线程 查看线程的工具 jconsole 直接win + R 输入jconsole就可以进入jconsole工具中。 线程运行原理每个线程启动后，会分配一个栈，每次方法调用又会生成一个栈帧，线程私有的，因此是线程安全的 栈帧的结构如上 线程上下文切换（Context Switch）就是线程切换另一线程运行的过程 时间片使用结束 垃圾回收时 更高优先级线程需要运行 线程自己调用了sleep，yield，wait，join，park，synchronized，lock等方法 切换时，操作系统需要保存线程的状态，并恢复另一进程的状态，对应的就是程序计数器，作用就是记住下一条jvm指令的执行地址，线程私有的 线程数过多也不好，一旦超过内核数，就会导致线程切换 方法start（） 启动一个新线程，至于是否运行由任务调度器决定，只可以调用一次start，不可重复调用 run（） 线程运行时执行的操作 join（） 等待线程结束 进入阻塞状态 join(Long d) 设置最大等待时间 setPriority（int grade） 设置优先级 getState（） 获取线程的状态 isInterupted（） 判断线程是否被打断 isAlive（）判断线程是否存活 interupt（） 打断线程 currentThread（） 获取当前正在执行的线程 yield（） 提示线程调度器让出该线程对cpu的占用 sleep（long n） 线程睡一会 进入阻塞状态 start与run不可以由主线程直接调用run，这样就相当于主线程去跑run了 不能调用两次start，只可以调用一次 sleep与yield setPriority（int grade） 设置优先级数字越大，优先级越高 join1234567891011121314public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(() -&gt; { while (true) { try { Thread.sleep(50); } catch (InterruptedException e) { e.printStackTrace(); } } }); thread.start(); thread.join();} 这样，主线程就需要等待thread结束之后才可以继续进行 join（long n） n为最多等待时间 interrupt打断sleep，wait，join的线程 打断阻塞的线程 抛出被打断的异常，并且停止运行 但是当被打断时，线程的打断状态会被置为真，可以通过该种方式停止线程 打断运行中的线程 并不会结束线程 但是当被打断时，线程的打断状态会被置为真，可以通过该种方式停止线程 1Thread.interrupted() 两阶段终止模式 在一个线程中优雅的停止另一个线程 整个过程其实就是，在中断时，给被中断线程一段时间来处理一下需要处理的内容 防止cpu占用100%123456789new Thread(()-&gt;{ while(true) { try { Thread.sleep(50); } catch (InterruptedException e) { e.printStackTrace(); } }}).start(); 经常睡眠一下就可以","link":"/hexo_blog/2022/02/12/juc/"},{"title":"python爬取b站视频","text":"[TOC] 起因不知道兄弟们有没有遇到过b站视频经常下架的问题，就比如我现在想在b站找一个老师的课程，运气好可以找到，但经常看了一段时间之后可能就会下架，然后继续找，过不了多久又会下架，这样的循环搞得我好烦呀，这时候我突然想起来万能的python，python爬b站视频咋样？说干就干 前期准备python环境必不可少，同时需要ffmpeg，一台可以上网的电脑 分析首先随便打开一个视频，然后右键查看网页源代码由于展示的源代码格式有点乱，所以我选择复制到vscode中查看这时候看着就舒服多了，细心的朋友可能已经发现baseUrl，然后我们复制一下这个baseUrl，在浏览器中打开康康，果然不出所料还是老实巴交的用python访问吧，header头设置好之后就可以直接访问了，至于header头怎么设置，相信兄弟们都懂，一切就绪之后可以拿到这个文件，值得注意的是b站的视频文件和音频文件是分开存储的，因此我们还需要下载音频文件，音频文件url的位置和视频文件url的位置靠很近，就在audio这里存放着 这样音频文件和视频文件的地址我么都拿到了，随后就可以开始下载了，下载之后使用ffmpeg将两个文件操作一下就可以了。这里有ffmpeg的安装教程，大家可以看一下，ffmpeg安装教程，欢迎三连 代码音频和视频整合的工具类1234567891011import os# 声音视频结合def videoMixAudio(videourl, audiourl, mp4url): com = f'D:\\\\tool\\\\ffmpeg\\\\bin\\\\ffmpeg.exe -i &quot;{audiourl}&quot; -i &quot;{videourl}&quot; ' \\ f'-acodec copy -vcodec copy &quot;{mp4url}&quot;' os.system(com) os.remove(videourl) os.remove(audiourl) 这里解释一下。videourl是视频文件的全路径，audiourl是音频文件的全路径，mp4url则是生成的有声音的视频的位置，然后通过调用ffmpeg来对音频和视频文件进行合成，没有使用ffmpeg的同学们可以采用格式工厂的方式对这两个文件操作进行操作也是可以的。 分析页面使用到了这三个匹配规则 123456# 拿到&lt;script&gt;中的内容valink = re.compile(r'&lt;script&gt;(.*?)&lt;/script&gt;')# 拿到window.__playinfo__后面的内容infoink = re.compile(r'window.__playinfo__=(.*)')# 这个是为了拿到视频的名字 可以选择不用nameink = re.compile(r'window.__INITIAL_STATE__=(.*);\\(function') valink用于取出下图这个部分的内容infoink是为了把window._playinfo_去掉，这样拿到的就是可以转换为json的字符串这样就可以拿到视频和音频的url了代码如下 12345678910111213141516171819def downloadVideo(url): # 获取网页源代码 html = requests.get(url).text # 拿到script的内容 info = re.findall(valink, html) # 第一个就是playinfo的那个script info2 = str(info[0]) # 这一个是存放视频信息的scripte info3 = str(info[1]) nameAnd=re.findall(nameink,info3) # 把window._playinfo_去掉，拿到一个可以转换为json的字符串 videoAndAudio = re.findall(infoink, info2) # str转json jsonobject = json.loads(videoAndAudio[0]) nameobject = json.loads(nameAnd[0]) name=nameobject[&quot;videoData&quot;][&quot;title&quot;] # 获取视频和音频的链接 videoFile = jsonobject[&quot;data&quot;][&quot;dash&quot;][&quot;video&quot;][0][&quot;baseUrl&quot;] audioFile = jsonobject[&quot;data&quot;][&quot;dash&quot;][&quot;audio&quot;][0][&quot;baseUrl&quot;] 拿到链接之后就可以开始下载了，记住下载的时候要设置好header 成果 总结全部代码就不贴了，兄弟们按照这个流程就可以实现，过程还是比较简单的，自己用用就好。希望和大家一起进步","link":"/hexo_blog/2021/08/20/python%E7%88%AC%E5%8F%96b%E7%AB%99%E8%A7%86%E9%A2%91/"},{"title":"jvm续集2","text":"jvm续集2java内存模型（JMM）JMM定义了一套多线程读写共享数据时，对数据的可见性，有序性和原子性的规则与保障。 原子性对于i++，字节码如下 共享的数据，例如静态变量，都是存放在主内存中，线程在使用静态变量时，现需要在主内存中取出数据，在线程内存中进数据的操作，然后操作结束之后在放入主内存中 在单线程下是没有问题的，但是在多线程下会出现多个线程的指令交错的情况出现，这就导致了A线程首先取出了数据，但是时间片到了，这时B线程再取出数据，二者相加再放入之后，只进行了一次加一，结果只是加一，并非加二。 如何保证原子性对对象进行加锁，从而保证原子性，其他线程需该部分进行完，才可以对加锁对象加锁 123synchronized (对象){ 原子操作代码} 通过这种方式实现安全的i++ 12345678910111213141516171819202122232425262728public class Test { static int i=0; static Object o=new Object(); public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(() -&gt; { for (int j = 0; j &lt; 100; j++) { synchronized (o){ System.out.println(&quot;a&quot;); i++; } } }); Thread thread2 = new Thread(() -&gt; { for (int j = 0; j &lt; 100; j++) { synchronized (o){ System.out.println(&quot;b&quot;); i--; } } }); thread.start(); thread2.start(); thread.join(); thread2.join(); System.out.println(i); }} synchronized也可以保证可见性可见性12345678910111213141516public class Test2 { static boolean run=true; public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(() -&gt; { while (run) { } }); thread.start(); Thread.sleep(1000); System.out.println(&quot;end&quot;); run=false; }} 如上述代码，尽管run在主线程中已经修改为false，但是由于在十秒之后，jvm做了优化，使得不再在主内存中取出run的值，而是将run存放在高速缓存中，从而使得主线程修改的run，并没有更新到高速缓存中，如下图 解决方法volatile修饰变量 使得程序必须在主内存中获取值 1234567891011121314151617public class Test2 { static volatile boolean run=true; public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(() -&gt; { while (run) { } }); thread.start(); Thread.sleep(1000); System.out.println(&quot;end&quot;); run=false; }} 注意只可以解决可见性，不可解决原子性，适用与一个写，多个读 有序性因为会出现指令重排，可能会出现我们意向不到的结果 设置对象为volatile，可以阻止指令重排 同一线程下，不影响结果时，会进行调整 1234567891011121314151617public class Singleton { private Singleton() { } private static Singleton instance = null; public static Singleton getInstance(){ if(instance==null){ synchronized (Singleton.class){ if(instance==null){ instance=new Singleton(); } } } return instance; }} 如上述代码，就可能出现指令重排问题，观察字节码 在instance=new Singleton();这里，一旦系统认为21与24这两条命令执行顺序没差，进行指令重排时，就会导致先进行了24，此时instance已经非空对象，其他线程就可能直接返回instance，但此时的instance还未初始化结束 happen-before 这个意思就是t2线程截断了t1线程，并修改了x值，此时外部是可以获取到x的最新值 CAS与原子类CAS体现乐观锁的思想 上述代码的思想就是，将变量值先取出，修改之后，比较旧值与现在的值是否是我们期望的，若是则退出循环，否则继续循环 获取过程中，可由volatile对变量进行修饰，这样乐观锁适用于竞争不激烈，多核cpu的场景下 未采用synchronized，因此不会出现线程阻塞 竞争激烈，会进行多次重试，此时效率会受到影响 底层 乐观锁与悲观锁 原子操作类 使用案列 1234567891011121314151617public class Test { public static void main(String[] args) throws InterruptedException { AtomicInteger integer=new AtomicInteger(0); Thread thread = new Thread(() -&gt; { integer.getAndIncrement(); }); Thread thread2 = new Thread(() -&gt; { integer.getAndDecrement(); }); thread.start(); thread2.start(); thread.join(); thread2.join(); System.out.println(integer.get()); }} sychronized优化 轻量级锁有多线程访问，且各线程都是依次进行加锁，并没有竞争，就可以采用轻量级锁 每个线程的栈帧都包含一个锁记录的结构，内部可以存储锁定对象的mark word 这个过程当被锁对象中保存了线程锁记录地址之后，才被认为枷锁成功，若失败可升级为重量级锁 此时线程1有进行加锁，但是该对象已经被加锁，加锁失败，但是由于发现是自己加的锁，所以执行锁重入即可 锁膨胀 以上可以认为 线程1对对象加锁，此时还是轻量级锁 线程2想要对对象加锁，却发现已经被锁住了，此时就会将锁标记修改为重量级锁，并且保留重量锁指针（目的是为了唤醒阻塞的线程），此时线程二就处于阻塞中 待线程1解锁时，会失败，但会释放重量锁，唤起阻塞线程竞争 重量锁自旋 自旋其实就是线程不阻塞，反而是重试加锁，这样就省去了阻塞到唤醒的上下文切换 当然也会有自旋失败，即自旋多次后仍无法加锁，就阻塞 偏向锁 其他优化","link":"/hexo_blog/2022/02/12/jvm%E7%BB%AD%E9%9B%862/"},{"title":"nginx_安装","text":"nginx安装安装语句 12docker run -d -p 8085:80 --name nginx-web2 --restart=always --privileged=true -v /docker/nginx/www:/usr/share/nginx/html -v /docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /docker/nginx/logs:/var/log/nginx nginx 这里的 -v 就是挂载目录","link":"/hexo_blog/2022/04/26/nginx-%E5%AE%89%E8%A3%85/"},{"title":"python爬取十万张表情包","text":"前言事情要从几天前说起，我有一个朋友，他在和他喜欢的小姐姐聊天时，聊天的气氛一直非常尬，这时他就想发点表情包来缓和一下气氛，但一看自己的表情包收藏都是这样的。。。。。。这发过去，基本就直接和小姐姐说拜拜了，然后他就向我求救问我有没有表情包，表情包我是没有，但网站有呀，来来，爬虫整起。 分析页面今天爬取的网站是斗图吧，有一说一表情包是真的多，看这惊人的页数接下来就该看看怎么拿到表情包图片的url了，首先打开谷歌浏览器，然后点F12进入爬虫快乐模式然后完成下图的操作，先点击1号箭头，然后再选中一个表情包即可，红色框中就是我们要爬取的对象，其中表情包的src就在里面现在我们就搞清楚了怎么拿到表情包的url了，就开始写代码了 具体实现解析页面获取网页内容这里就是获取爬取网页的信息 123456789101112def askURL(url): head = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36&quot; } req = urllib.request.Request(url=url, headers=head) html = &quot;&quot; try: response = urllib.request.urlopen(req) html = response.read() except Exception as result: print(result) return html 解析网页内容123456789101112131415161718192021222324# 取出图片src的正则式imglink = re.compile( r'&lt;img alt=&quot;(.*?)&quot; class=&quot;img-responsive lazy image_dta&quot; data-backup=&quot;.*?&quot; data-original=&quot;(.*?)&quot; referrerpolicy=&quot;no-referrer&quot; src=&quot;.*?&quot;/&gt;', re.S)def getimgsrcs(url): html = askURL(url) bs = BeautifulSoup(html, &quot;html.parser&quot;) names = [] srcs = [] # 找到所有的img标签 for item in bs.find_all('img'): item = str(item) # 根据上面的正则表达式规则把图片的src以及图片名拿下来 imgsrc = re.findall(imglink, item) # 这里是因为拿取的img标签可能不是我们想要的，所以匹配正则规则之后可能返回空值，因此判断一下 if (len(imgsrc) != 0): imgname = &quot;&quot; if imgsrc[0][0] != '': imgname = imgsrc[0][0] + '.' + getFileType(imgsrc[0][1]) else: imgname = getFileName(imgsrc[0][1]) names.append(imgname) srcs.append(imgsrc[0][1]) return names, srcs 到现在为止，已经拿到了所有的图片的链接和名字，那么就可以开始下载了 文件下载多线程下载因为文件实在有点多，所以最好采用多线程的方式下载，我这里只是给了一个样例，大家按照这个逻辑写一下就好 1234pool = ThreadPoolExecutor(max_workers=50) for j in range(len(names)): pool.submit(FileDownload.downloadFile, urls[j], filelocation[j]) 成果 总共是爬了十万多张表情包，这次咱也是表情包大户了 总结很简单的一个爬虫，适合我这样的初学者练练手，如果对爬虫有兴趣的话可以看看我的爬虫专栏的其他文章，说不定也有你喜欢的 爬虫专栏，快来点我呀 两行代码爬取微博热搜，并实现邮件提醒功能，妈妈再也不用担心我吃不到瓜了 爬虫基础 python爬取4k小姐姐图片 人生苦短 我用python python爬b站视频 人生苦短 我用python Python爬取美女图片 爬虫基础 有缘再写，侵权立删","link":"/hexo_blog/2021/08/24/python%E7%88%AC%E5%8F%96%E5%8D%81%E4%B8%87%E5%BC%A0%E8%A1%A8%E6%83%85%E5%8C%85/"},{"title":"redis","text":"Redisredis并非关系型数据库，而是key-value存储的，有更好的扩展能力 命令 后台启动 redis.conf中的daemonize的值改为yes redis-server /etc/redis.conf 常见指令select db： 切换数据库 dbsize： 查看数据库中的key的数量 flushdb：清空当前库 flushall：通杀全部库 特点单线程+多路io复用 黄牛到火车站买票是单线程的 而多个游客可以同时去买票 就是所谓的单线程+多路IO复用 数据类型 String 基本命令1234567891011121314151617set key value 设置值 同时可以修改值get key 获取值append k1 value 追加值strlen key 获取长度setnx key value key不存在时设置值，否则失败incr key 只能是数字值，加一操作decr key 只能是数字值 减一操作incrby key 10 加10 decrby key 10 减10 下面的指令体现原子性，即一个设置失败，全员失败 12345678910111213mset k1 v1 k2 v2 设置多个kv值mget k1 k2 获取多个valuemset k1 v1 k2 v2 在不存在key的情况下设置多个kv值getrange key start end 根据开始位置和结束位置获取内容 类似subStringsetrange key start end 覆写其中内容setex key time value 设置过期时间getset key value 设置新值，同时获取旧值 特点 List单键多值 底层是双向链表。操作两端效率比较高，索引操作效率比较差 基本指令123456789101112lpush/rpush key value value value 从左边或右边插入一个或多个值 两者的区别就在于新加值是在哪一个方向开始添加lpop/rpop key 从左边或右边吐出一个值 lrange key start end 根据下标获取值 end为-1代表最后的位置rpoplpush k1 k2 从k1的右边取出一个值放在k2的左边 可以操作一个列表，从而表头变表尾lindex key index 获取index下标的元素llen key 获取列表的长度linsert key before/after value newvalue 在value的前面或则后面添加newvaluelrem key n value 从左边删除n个值lset key index value 将列表中的index处的内容替换为value 数据结构 Set可以自动排重 底层是一个hash表，增删查都是O（1） 常见指令 数据结构hash表实现的，所有的value都指向同一个值 Hash 存储方式 第三种是最好的 第一种不方便修改值 第二种数据可能存储的比较分散 常见指令 数据结构 Zset可以说是Set的加强版，这里的元素都是不可重复的，并且每一个元素对应一个score，根据这个score开始排序 score可以重复，但是元素不能重复 访问中间元素很快 常见指令、 数据结构 跳跃表的数据结构如上图所示 过程 找51： 与1比较，小，往前推进，比21大，下个节点为空，来到下一层 41比51小 推进，61比51大，返回41到下一层 推进找到了51 配置文件远程访问注释掉 bind 127.0.0.1 -::1 protected-mode 设置为no 发布订阅 这里发布者有多个频道，此时s1若只是订阅了1频道，当1频道有消息时，s1可以拿到。 但若是其他频道有消息则不可以 基本指令12subscribe channel1 订阅频道publish channel1 hello 发布消息 新数据类型Bitmapsbitmaps本身就是一个字符串，但是是可以进行位操作的字符串 常见指令12345setbit key 1 1 #设置1位置的值为1getbit key 1 #取出1位置的值bitcount key start end #获取key中start到end的1的数量,不设置start和end就默认是整个字符串bitop and（or/not/xor） key1 key2 #进行交，或，非和异或操作 HyperLogLog主要用于基数的计算，类似set 常见指令123pfadd key value valuepfcount key #获取其中基数的数量pfmerge k1 k2 k3 #将k2，k3合并到k1中 Geospatial就是经纬度 提供了经纬度设置，查询，范围查询，距离查询，经纬度hash等常见操作 常见指令 Jedisjava来操作redis Redis事务redis的事务就是保证所有指令串行进行，不会被其他指令打断 悲观锁与之前的机制的相同 每次操作时，就是直接上锁，其他人不可以操作，当操作结束后，其他人才可以进行操作 乐观锁这里提到了版本号 可以同时进行，当一个线程进行完之后，会更新版本号 另一个线程再进行操作时，如果发现了版本号出现了差别，就会停止操作，即操作失败 三大特性单独的隔离操作所有命令都会序列化进行，不会被其他命令打断 没有隔离级别的概念命令提交前都不会执行 不保证原子性一条命令执行失败，不会回滚 解决连接超时问题连接池 持久化操作在指定的时间间隔内将数据集快照写入硬盘中 写时复制技术： 复制一个与现在相同的进程，专门用于数据的写入，会先将文件写入到一个临时区域中，然后再写入硬盘中 父进程与子进程一般使用同一个物理区域，只有内容发生变化时，才会复制给子进程 AOF以日志的形式记录写操作 将redis执行的写指令记录下来，并且日志文件不能修改，只能追加 redis在启动时会以该日志文件为基础进行恢复 AOF的优先级较高 aof文件损坏时 aof同步频率 压缩（重写） 将上面的两个操作合并为一个操作 主从复制 有一个是主服务，其他都是从服务，从服务复制主服务中的数据 master/slave机制 master以写为主，slave以读为主 主写数据，从读数据 容灾快速恢复 有一个从机挂掉了，其他从机顶上 一主多从 基本指令12info replication #查看是否为主机slaveof ip port #设置为ip上port端口运行redis的从机 原理一主两仆即使一个从服务器挂掉，当在启动时，也会完整复制主服务器中的所有数据 当主服务器挂掉之后，仍然是主服务器，两个从服务器不会做任何操作 复制原理 薪火相传 主服务器连接的还是只有两台从服务器 但每一台从服务器下仍链接了两台服务器 这样依次向下传递，即除了第一层之外，所有服务器既是下一层的主服务器，也是上一层的从服务器 反客为主主服务器挂掉了，从服务器可以立即顶上主服务器 1slaveof no one #当主服务器挂掉之后，直接替代主服务器 哨兵模式1sentinel monitor mymaster 127.0.0.1 6379 1 #设置监视6379端口的哨兵 只要有一个从服务器同意就可以变成主服务器 怎么选择哪一个更新为主服务器 redis.conf 中的replica-priority 值越小优先级越高 当主服务器从新启动之后也是从服务器了，不再是主服务器 java代码 集群 代理主机就类似于反向代理，由一台服务器代理所有的redis服务，即所有请求先需要通过代理服务器 无中心化集群 任何一个redis服务都可以作为redis服务的入口 并且各个redis服务之间可以互相通信 实现了redis的水平扩容，每个节点可以存储其中一部分 提供一定程度的可用性，当一些服务出现问题时，其他服务仍可以工作 合体打开/mydata/redis/src的文件夹，然后执行以下命令 1redis-cli --cluster create --cluster-replicas 1 192.168.183.132:6379 192.168.183.132:6380 192.168.183.132:6381 192.168.183.132:6382 192.168.183.132:6383 192.168.183.132:6385 在replicas后的1代表以最简单的方式搭建集群，即一个主机一个从机，这样刚好三组 1redis -c -p 6379 #-c就代表以集群方式连接 redis如何分配节点尽量保证每个主数据库在不同ip，每个从库和主库不在同一个ip 问题缓存穿透 也就是需要查询的数据多不在redis中，需要每次都访问数据库，因此出现了缓存穿透 缓存击穿 出现场景 雪崩 解决方法 分布式锁123setnx user 123 #上锁的方式设置值，未释放锁之前，其他人不可以操作del user #释放锁set user 123 nx ex 12 #设置值的同时设置过期时间 防止锁长期不解锁，可以对上锁的对象设置过期时间，当过期之后，锁就没了","link":"/hexo_blog/2022/07/28/redis/"},{"title":"slam","text":"slam相机 单目相机移动相机，根据图像的移动程度，计算出图像中物体的距离 双目相机 RGBD相机 适用与室内，光线不是很好的地方 共同点 运动方程 观测方程 三维空间刚体运动向量 这里a1.。。是实数，e1.。。。。是三个线性无关的向量，从而表示真正的向量 怎么将一个坐标轴转换为另一个坐标轴？ 首先将各轴旋转到和目标轴一样的方向，然后平移坐标原点即可 那么旋转前后的关系？ 由图中可以看出，a就是向量，e就是常数 R称为旋转矩阵 R是正交矩阵（乘自己的矩阵转置结果为E） R的行列式是+1 满足上述两个性质被称为旋转矩阵 三阶R称为三维空间的旋转矩阵 平移前后的关系？ 计算时过于复杂？ a撇就是原来的旋转加平移的公式 这是对应的逆矩阵 TW：世界坐标系 TR：机器人坐标系 世界坐标和机器人坐标的转换 TRW是旋转矩阵 Eigen主要使用Matrix这个模板类，就是一个矩阵模板类，可以进行矩阵的运算 旋转向量（角轴） 旋转向量就是坐标轴旋转时绕的轴，如下图，a到b，是绕着w旋转的，w就是旋转变量，这里的旋转变量是一个三维向量 角轴和旋转矩阵的不同 由右面的公司就可以得出角度和角轴 Rn=n 这就是特征值为1的特征向量呀 欧拉角 四元数 是一种扩展的复数 像二元的复数，乘上一个i之后，相当于逆时针旋转90° 四元数有三个虚部 虚部之间还有关系 计算法则 四元数和角轴的关系 四元数的旋转 实部为0时，为虚四元数，此时可以表示三维空间的所有坐标 李群和李代数 群是什么？ 李群是什么？ 连续：他的元素可由一组参数组成的，且这些参数中至少有一个在一定的区域中是连续的 李代数是什么？ 引入反对称矩阵的概念 上图首先进行了泰勒展开 R上一个.代表是求导后的 假设那个符号函数不变，就变成了一个微分方程，解完之后就是下面的这个 上图就引出了李代数是什么东西，就是这个fai 李代数 问题李代数不明白 指数映射和对数映射so（3） se(3) 李代数求导和扰动模型 上图不成立，下图才成立。。。。。 导数模型 扰动模型 相机和图像相机模型 小孔成像 单目相机 由于小孔成像生成的图像是倒着的，所以这里我们直接将原本生成的图像映射到物体这里，这样就是正的 这里再代入像素之后 fx就是f*a fy就是f*b Pw是世界坐标 K是中间矩阵 R是旋转矩阵 T是变换矩阵 畸变 径向畸变 在极坐标下，中间点为原点 即R发生变化 切向畸变 极坐标下，度数发生变化 双目相机 RGBD相机 非线性优化状态估计问题 先回顾一下，X是运动方程，Z是观测方程 w和v都是噪声。 这两个方程对应运动和观测方程 P（x）是先验 p（z|x）是似然 不懂了再去看csdn 贝叶斯收藏夹里的解释 举个例子来引出最小二乘 上图中因为负对数形式的前半部分式子中都是常量，后半段才有变量，因此只求后半部分的最小值，就是原函数的最大值 梯度下降方向的选择 视觉里程计特征点法 特征点有以下要求 ORB FAST就是比较目标点和周围点的差异值，若是差异值在都超过一个阈值，则称该点为关键点 这里区分特征点的方式就是BRIEF 取附近的一部分点对，然后计算出像素值的比对大小，从而赋值0或1，然后拼接起来作为一个二进制的数 这个点对可以不包括目标点 这里计算两个特征点的关系时，要使用汉明距离 汉明距离的计算公式 对于两个数字来说，汉明距离就是转成二进制后，对应的位置值不相同的个数。 对极几何 无敌大懵逼 这里的x1和x2是在某个程度上的归一化，至于归一到那个地方不知道 这里不知道为什么x1和x2的旋转平移也符合p1和p2， 这就直接用吧，不说啥了 PNP3D到2D的变化 在上图中可以看到，一对点可以获得两个方程，第二个方程由第一个方程得到 DLT就是求出相机的外参 s和参数的关系就是第二个方程 pnp就是求相机到世界坐标的变化，也就是求外参（R和t） 求出结果之后不一定满足旋转矩阵，投影到SO3中 OR分解不知道是啥玩意？？？？？？ K就是进行求到相机内的投影的变换 p3p 已知a，b，c在平面的坐标 并且也知道A，B，C的世界坐标，此时只需要根据关系把abc的相机坐标求出即可 然后就可以计算外参 上述就是所谓p3p的原理 知道A，B，C和a，b，c的坐标，求外参 最后可以求出世界坐标和相机坐标 pnp的优化解法 此时已知相机的外参，但是由于不是很准确，这时就可以通过导数之类的方式使得误差尽可能减小 下图就是看怎么优化 ICP 视觉里程计——不使用特征匹配寻找配对点直接法和光流 光流 LK光流 这一部分的结论都是基于这个灰度不变的假设展开的，但是事实上这个是不可能的 由于灰度值不变，也就是说无论x，y，t怎么变化，灰度值不会发生变化，因此两边的I函数是相等的 后面的多项式就是0 由于前面得到的方程 是一个二元一次方程，在一个方程的情况下，不可能求出值 这时就用一个假设，即在一个窗口内，灰度值都不会变化，这样就可以求出多个方程 dy/dt和dx/dt是通用的 所以可以求出 直接法 后端EKF滤波器 上图的意义就是讲了为什么要使用滤波器，因为我们获取的数据总会有偏差 这里和之前学的一样 和上面的公式一样，只不过是使用xk代替了x与y 这里是开始对运动方程和状态方程进行估计 Ak，R和Q都没有给出相应的求值公式 非线性优化（补充） 这里要估计的量y，x 6.4给出的公式就是估计再z，u情况下，x的概率分布 当只有一张张的图像时，相当于只获得了观测数据，这时估计x就是 这里的概率公式给我感觉求的是总体的，而并非具体时间点的 例如6.6中的，我希望求的在Z的情况下，X的分布，这里的Z应当是一个集合 P(X)也是相似的，这个的概率并不能求出左边的概率 雅可比矩阵：","link":"/hexo_blog/2022/08/07/slam/"},{"title":"个人介绍","text":"博主介绍XiaoW 下面就是我的照片，献丑了 说明该个人博客展示的博客多来自我个人的csdn博客","link":"/hexo_blog/2021/08/19/%E4%B8%AA%E4%BA%BA%E4%BB%8B%E7%BB%8D/"},{"title":"优化器","text":"优化器Adam","link":"/hexo_blog/2022/10/27/%E4%BC%98%E5%8C%96%E5%99%A8/"},{"title":"图象识别","text":"图像识别 图像在计算机中都是由这样的一个个小块组成，每一个小块对应rgb三个表（三通道）中的一个数值，数值越小越暗，越大越亮 0—黑色 255—白色 灰度图只有一个通道（黑白照片） 边界填充 图像融合 图像阈值 图像平滑处理 其实就是去除图片中这些小白点（噪声） 高斯滤波（正态分布） 中值滤波 腐蚀操作可以认为是将图片转换为矩阵，然后可以选择3*3的方式依次取，一旦矩阵中数据有不同则设为全0 从而达到去除多于元素的功能 膨胀操作与腐蚀操作刚好相反 开运算先腐蚀再膨胀 闭运算先膨胀再腐蚀 礼帽原图-开运算 黑帽闭运算-原图 梯度运算 这里计算是内积，我个人理解为最后求绝对值，有最大绝对值的就是梯度存在的点 测试图如下 Canny边缘检测 非极大值抑制： 其实就是判断该点的梯度值是不是大于它梯度线上相邻两个点的梯度值大小，若是，则留下，否则不是边缘点 双阈值： 图像金字塔 先卷积，然后去除偶数行和列，就成功缩小了，卷积忘了再去搜 代码如下 拉普拉斯金字塔 这里GI就是原图，pyrUp就是那个金字塔的变大 图像轮廓 需要是二值图像 转换为灰度图 模板匹配其实就是卷积神经网络，进行匹配图片 直方图 右边就是直方图，表示像素相同的个数，以柱状图的形式表示 直方图均衡化 如图，那第一行来说，概率是&lt;=50的概率，然后再概率*255就得到均衡化之后的值 可采用自适应的均衡化，效果比较好，有兴趣在学 傅里叶不考虑时间，只考虑频率 不是很明白，怎么用正弦波堆积出来的 harris角点检测角点：边界上的角，就称为角点，在角点处，无论水平还是竖直方向都是变化很大的 SIFT尺度空间 模糊或则清楚都可以分辨一个物体 大或小也都能判断这个物品 视频读取视频是由一帧帧组成，每一帧都可以认为是一张图片 就是同层金字塔下，获取图片之间的差异在哪 DOG 关键点的精确定位 特征匹配背景建模目的就是识别到前面的物体，剔除背景 帧差法（用的比较少） 当两帧的差的绝对值大于一个阈值之后，就被设为255，小于则设为0 混合高斯模型 可以认为是对每一个点进行统计，按照正态分布，如果新的点的像素值和原来的u相差不超过标准差，就可以认为是一个分布 光流估计 TESSERACT识别图片内容1tesseract xxx.jpg result 这里的result就是输出文件，默认跟上txt后缀名","link":"/hexo_blog/2022/07/08/%E5%9B%BE%E8%B1%A1%E8%AF%86%E5%88%AB/"},{"title":"ts视频下载","text":"前言之前一直爬取的内容都是完整的文件，例如一整个mp3或则mp4，但是目前很多视频网站都开始采用ts流媒体视频的方式进行视频的展示，不知道你有没有这样的体验，兴致勃勃的打开一个电影网站，准备开始施展爬虫大法查看xhr请求之后，本以为可以找到一个返回mp4的接口，没想到返回的是这一堆ts文件今天我们就来聊一聊怎么下载这些ts文件并将他们拼接为一个mp4 开发工具ffmpeg，pycharm 解决思路首先打开谷歌浏览器，F12，查看xhr请求，这一步相信兄弟们已经轻车熟路了。如下图有两个诡异的m3u8，木错，这就是今天我们的突破口，一般第一个m3u8中存储的都是第二个m3u8文件的url，第二个m3u8文件则是存储的ts文件的urll。因为我们这次主要是讲怎么下载ts文件，所以直接用解析第二个m3u8文件，即可。双击这个请求，就可以查看详情，其中Request URL就是调用的接口或则远程文件，直接调用则会下载该m3u8文件，然后解析一下，拿到ts的url列表就可以进行下载了。先看一下这个m3u8文件的内容很明显文件中存储的不是ts文件的完整地址，需要我们根据实际情况进行拼接就可以，查看的方式就是点击ts文件xhr请求进行查看如下图，很明显，红框圈中的就是我们要拼接在文件名之前的。这就拿到了真实的ts文件地址。那么开整代码吧 代码实现解析m3u8文件，获取ts下载列表要使用到m3u8这个库来解析m3u8文件 123456789101112import m3u8tss = []order = []#realurl就是存储ts文件地址的m3u8文件的url ，这样返回的数据是json格式的，方便读取数据data = m3u8.load(realurl).data# appendurl就是要拼接在前面的那个地址 这样存入tss的ts文件地址都是真实地址# order的作用是在将多个ts文件合成一个mp4时，由这个order提供各ts文件拼接的顺序for i in data[&quot;segments&quot;]: tss.append(appendurl + &quot;/&quot; + i[&quot;uri&quot;]) order.append(i[&quot;uri&quot;]) 到现在为止，ts文件拼接的顺序以及ts文件的真实地址就全部拿到了 多线程下载ts文件，以及ts文件顺序的存储有一说一，这些ts文件不仅多，而且小，如果我们只是一个线程下载文件，未免太浪费时间了，而且效率太低了，这次我们采用多线程的方式进行大量ts文件的下载 总代码12345678910111213141516171819202122232425262728293031323334def download(url, name): #记录创立的线程 task_list = [] # 获取ts的真实地址和顺序 tss, order = getTss(url) # 这里将ts文件顺序存储在m3u8，至于为啥这么做，因为ts文件数量太多了 file = open(&quot;E://file//order.m3u8&quot;, 'w') # 这里将下载ts文件的本地路径输入到order.m3u8之中 for i in order: file.write(f&quot;file 'E:\\\\file\\\\ts\\\\&quot; + i + &quot;'&quot;); file.write(&quot;\\n&quot;) #线程池的创立 pool = ThreadPoolExecutor(max_workers=50) for i in range(0, len(order)): # 启动多个线程下载文件 task_list.append(pool.submit(FileDownload.downloadFile, 'E://file//ts//' + order[i], tss[i])) # 判断所有下载线程是否全部结束 while (True): if len(task_list) == 0: break for i in task_list: if i.done(): task_list.remove(i) # 进行多个ts文件的合并 VideoUtil.mixTss(name) # 合并结束之后把ts文件都删了，不然太占空间了 for u in order: turl = f&quot;E:\\\\file\\\\ts\\\\&quot; + u os.remove(turl) ts文件顺序存储到本地文件中主要代码 1234# 这里将下载ts文件的本地路径输入到order.m3u8之中 for i in order: file.write(f&quot;file 'E:\\\\file\\\\ts\\\\&quot; + i + &quot;'&quot;); file.write(&quot;\\n&quot;) 最终文件中存储的内容最好按照这种格式存入，之前在网上找的其他格式都会报错，但这个是ok的 多线程下载ts文件yysy，多线程真的强，尤其是下载这些小文件，多线程真的是绝了 本文采用线程池的方式，为什么采用线程池呢，因为线程池可以帮我们保留一段时间空闲线程，可以减少线程创建和销毁所耗费的时间，大大提高多线程的效率，同时可以帮助我们限制线程的数量主要代码 123456789101112#线程池的创立 pool = ThreadPoolExecutor(max_workers=50) for i in range(0, len(order)): # 启动多个线程下载文件 task_list.append(pool.submit(FileDownload.downloadFile, 'E://file//ts//' + order[i], tss[i])) # 判断所有下载线程是否全部结束 while (True): if len(task_list) == 0: break for i in task_list: if i.done(): task_list.remove(i) ts文件合成mp4主要思路就是利用刚刚生成的那个ts顺序文件（order.m3u8），按照文件中的顺序进行ts文件的拼接。 这里拼接ts文件时还是要使用ffmpeg，没有的兄弟们可以看下这个安装一下ffmpeg安装教程主要代码 12345def mixTss(name): com = r'D:\\\\tool\\\\ffmpeg\\\\bin\\\\ffmpeg.exe -f concat -safe 0 -i E:\\\\file\\\\order.m3u8 -c copy E:\\\\file\\\\video2\\\\{}.mp4'.format( name) os.system(com) 这里解释一下D:\\tool\\ffmpeg\\bin\\ffmpeg.exe ： 本地ffmpeg的位置，设置了环境变量直接ffmpeg即可 E:\\file\\order.m3u8：刚刚生成的存储ts文件的顺序的文件路径 E:\\file\\video2\\{}.mp4：视频最终合成之后存放的位置 至此，ts视频的下载以及合成一个mp4就实现了 成果ts文件这是下载过程中截的图，有一说一，看着这些文件爆炸式增加，还挺爽 mp4文件具体就不给你们康了，你们猜猜是啥 总结总之没有想象的这么难，做之前以为很复杂，其实还好，最后欢迎各位大佬指点。","link":"/hexo_blog/2021/08/20/ts%E8%A7%86%E9%A2%91%E4%B8%8B%E8%BD%BD/"},{"title":"基于ARM和OpenCV的视频监控系统的设计与实现","text":"关键词Motion JPEG 算法静态背景下的移动监测算法——帧差法嵌入式系统设计 实现编译 交叉环境搭建使用的 代码移植 pc上ubuntu的库路径和开发板上的是不同的 总结usb摄像头捕获图像，存储到usb设备上，实时监控，使用帧差法判断是否有物体移动，有则保存，否则丢弃 实时跟踪，在论文中没有找到相关内容 ARM架构的处理器 v4l2驱动接口传输给核心处理器 4412开发板，系统为linux 4412处理器支持usb接口的设备 摄像头的设备节点标识为/dev/video*这种类型的，对文件的读写来实现摄像头的读写操作，V4l2是为视频设备设计的的驱动，提供操作的接口 QT是GUI设计框架 解压完opencv源码包之后，每一层都有一个cmakeList.txt用来配置Cmake选项，从而实现将需要的库导入 基于C++搞得 videocapture函数获取摄像头资源，根据设备号选择获取哪一个摄像头的视频数据 常数K+索引值 帧差法之上优化的，其实也没咋优化，就是做了开运算，消除噪声从而凸显差值区域 之后进行绘制图像，然后根据特点进行对象的跟踪 安装opencv和v4l2的库 ffmpeg来播放视频 系统移植： ​ 内核剪裁： 添加相关的驱动以及组件功能 算法移植： 其实都是利用pc机进行算法编写，然后再移植","link":"/hexo_blog/2022/08/02/%E5%9F%BA%E4%BA%8EARM%E5%92%8COpenCV%E7%9A%84%E8%A7%86%E9%A2%91%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"title":"","text":"","link":"/hexo_blog/2022/04/26/%E5%AE%89%E8%A3%85/"},{"title":"微博明星更新实时提醒","text":"前言掐指一算，已经好久没搞过爬虫了，恰好昨天看了一下评论区，看到了这条评论突然觉得很有搞头，那就来试试吧 解决思路首先先来到明星个人博客的界面，看一下源代码，你就发现明星的博客信息并不是直接放在源代码里，如下图所示。那接下来思路就很明确了，就是去找一下哪一个接口去获取明星博客的信息列表了。 在google浏览器按下F12就可以进入爬虫快乐模式。 通过看各个接口的返回信息，就可以找到返回微博信息列表的接口了。如下图所示 这就很清楚了，就是直接调用这个接口，然后解析返回的数据，将需要的信息拿出来即可，那现在就可以开始写代码了。 代码实现通过以上步骤，我们可以知道所需的接口是 1https://weibo.com/ajax/statuses/mymblog?uid=2607803303&amp;page=1&amp;feature=0 uid后跟得数据就是对应明星id 来到明星微博主页，就可以找到明星对应的id了，如下图 然后就是进行调用接口，并进行解析数据了 1234567891011while True: r = requests.get(&quot;https://weibo.com/ajax/statuses/mymblog?uid=5945434467&amp;page=1&amp;feature=0&quot;, headers=header) print(&quot;进行查询&quot;) for info in r.json()['data']['list']: y = formatDate(info['created_at']) # 获取发布日期 gap = dateGap(datetime.now(), y).__float__() if (gap.__abs__() &lt; 60): # 设置时间差 若微博发布时间与现在时间相差在一分钟内则发邮件提醒 print(y) print(info['text_raw']) time.sleep(60) # 休眠一分钟 实现一分钟查询一次 这里需要设置header头，header头中需要加入你的登录信息，其实就是把cookie中的内容放入即可。 上述代码中得info[‘text_raw’]就是微博的详情，再配合邮件发送就可以实现明星微博实时更新提醒了。 总结通过以上方法就可以实现我们想要的功能了，如下图这样就可以接收到明星微博更新提醒 整个爬虫很简单，完整代码等我整理一下会分享出来。 仅供学习，侵权必删","link":"/hexo_blog/2022/04/27/%E5%BE%AE%E5%8D%9A%E6%98%8E%E6%98%9F%E6%9B%B4%E6%96%B0%E5%AE%9E%E6%97%B6%E6%8F%90%E9%86%92/"},{"title":"当opencv遇上打篮球","text":"@TOC 前言这一段时间没怎么写博客，偶尔写一次也是比较正经的博客，感觉自己都不正常了。今天看课的时候突然来了灵感，那就整个烂活玩一玩。 打篮球配上rgb显示是一种什么体验呢，来试试。 内容首先我们需要一个打篮球的视频，你猜是哪个打篮球视频。 想要完成的功能就是像下图这样，在视频中把一个人的轮廓画出来，然后随着视频一起播放。画的轮廓如下图 接下来就可以聊聊解决思路了。 第一步其实先把图像给整成灰度图，直接使用opencv的cvtColor函数即可 为了方便提取人物的轮廓，我们需要做一下二值处理，这里使用的函数是threshold，这个函数中会设置一个的阈值，当像素值超过这个阈值时，会直接将像素值设置为我们预定的像素值，当小于阈值时，则会将像素值设置为0，这样就可以得到下面这张图 相对于原视频，这个时候的图像已经很清楚的展示出了我们需要的内容，并且很容易就可以提取出轮廓 这之后，再将轮廓画在视频上就可以了 接下来就可以开始搞代码了 实现完整代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npfrom 图像识别 import picutilsimport cv2# 读取视频cap = cv2.VideoCapture('cxk.mp4')# 设置标志位 超过5就变色m = 0while (1): m += 1 # 颜色数组的下标 l = 0 # 颜色数组 实现rgb变化效果 colos = [[0, 0, 255], [0, 255, 0], [255, 0, 0]] # frame存储每一帧的图像 ret, frame = cap.read() # 转换为灰度图 gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # 二值处理 这里的阈值设置为130可能不是最好的，可以试着再改改 超过130就会变255，否则就是0，从而实现二值图像的生成 thresh = cv2.threshold(gray, 130, 255, cv2.THRESH_BINARY)[1] # 边缘检测，检测出边缘方便轮廓提取 edged = cv2.Canny(thresh, 75, 150) # 获取所有的轮廓，这里没有在进行处理，有兴趣可以提取出需要的 cnts = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0] # 设置一个白板图片，作为人物的背景板 back = np.zeros((480, 720, 3), dtype=&quot;uint8&quot;) + 255 # 展示5帧修改一次颜色数组的下标 l就是下标值 if m == 5: l = (l + 1) % 3 m=0 # 把轮廓绘制在白色的背景板上 cv2.drawContours(back, cnts, -1, colos[l], 3) cv2.imshow('frame', back) k = cv2.waitKey(60) &amp; 0xff # 等待退出键，就可以直接退出 if k == 27: breakcap.release()cv2.destroyAllWindows() 成果最后生成的图像如下图所示 毕竟谁能拒绝一个有rgb变换效果的男人呢 弄的很粗糙，刚刚接触opencv，以后再改进","link":"/hexo_blog/2022/07/26/%E5%BD%93opencv%E9%81%87%E4%B8%8A%E6%89%93%E7%AF%AE%E7%90%83/"},{"title":"推荐系统","text":"推荐系统算法基于人口学的推荐算法就是将不同的人进行分类，相同类别的人被认为是有着相同的兴趣爱好 基于内容的推荐算法根据电影的内容对电影进行分类，依据这个分类推荐电影 基于协同过滤的推荐机器学习分类有监督学习提供数据以及结果的学习过程 无监督学习提供数据但不提供结果 强化学习通过与环境交互并获取延迟返回而改进行为的过程 监督学习 三要素模型：总结数据的内在规律，用数学函数描述的系统 策略：选取最优模型的评价准则 算法：选取最优模型的具体方法 模型评估策略训练集和测试集训练模型的称为训练集 测试模型的称为测试集 有一种设计方式：首先训练集输入到模型中，形成模型，然后可以将相同的训练集输入模型中，从而判断模型的的训练效果 损失函数衡量模型预测误差的大小 经验风险平均损失称为经验风险 训练误差训练集的平均损失 测试误差测试集的平均损失 过拟合和欠拟合以树叶为例： 过拟合：认为符合特例的所有的特征才能被称为树叶，学习的过度了，就例如认为只有带有的锯齿的树叶才是树叶，就是学习的过度了，这就要求训练集应当多元化。 欠拟合：认为符合特例的一个特性就可以称为树叶，学习较为欠缺，例如学习之后认为绿色的就是叶子，就是学习太欠缺了。 正则化为了防止过拟合提出的策略 分类与回归分类：预测数据是离散的 连续：预测数据是连续的 监督学习从数据中学习一个分类模型或分类决策函数，称为分类器，分类器对新的玉树进入预测称为分类 精确率和召回率评级分类器性能的指标一般是分类准确率，定义为对测试集正确的样本数和总样本数之比 对于二类分类问题，常用的评价指标是精确率和召回率 通常关注的类为正类，不关注的为负类 TP 将正类预测为正类的数目 FN 将正类预测为负类的的数目 FP 负类预测为正类的数目 TN 负类预测为负类的数目 例如在推荐系统中，应该推荐的就是正类，不该推荐的就是负类 精确率 TP/（TP+FP） 就是推荐中的正确的所在比例 召回率 TP/（TP+FN） 正类中成功被预测的比例 回归分类 未知数的个数：一元回归和多元回归 模型分类：线性回归和非线性回归 回归学习的损失函数： 平方损失函数 梯度下降算法： 梯度方向：函数变化增长最快的方向 负梯度方向：函数变化减少最快的方向 沿着负梯度方向不断变化，知道求到最小的损失值 机器学习模型线性回归x与y存在线性关系 例如y=ax+b F(x)=ax1+bx2+cx3………. 就是看自变量是不是最高一次幂，若是的话，则说明是线性的 最小二乘法（误差损失函数）来评估模型是否符合 其实就是判断实际的点到我们的模型公式距离的总和，求出最小的值，来求出最优的模型 一元线性回归的公式 多元线性回归的公式 梯度下降法求解线性回归 以上是公式，这里的谁他和x都是向量 谁他就是自变量旁边那个 下面介绍一下公式的原理 其实就是将函数值减去真实值，将这些差的平法加起来然后求平均，因为各个点的坐标都是知道的，这时候哪些参数就是自变量，对这些参数求偏导即可 梯度不懂？ 分类KNNk个最接近的邻居，即若一个样本在特征空间中k个最相似的样本的大多数都属于一个类别，这个样本也属于这个类别 选择的邻居都是已分类的 k一般选奇数 knn的结果很大程度取决于k的选择 两个计算公式中的x与y可能不只是两个数，可能是两个向量 knn算法详情 逻辑斯蒂回归 其实就是找分界线 上图是压缩函数的应用，将xy轴认为是x1，x2，找出x1与x2的关系，即图中的绿线，为f，f&gt;0时，带入压缩函数，刚好可以接近一， 反之一样，下图是另一个使用实例 损失函数 损失函数借用以上图像，这里hx是预测值，y是实际值 下面是损失函数的公式，我认为后面有问题，应该是y-1 前面两图直接蒙了 决策树 特征选择根据哪个特征来划分特征空间 随机变量M可以等于1，也可以等于2，M就是随机变量 熵越小，不确定越小，决策树的选择应当是熵越来越小 无监督学习聚类 k均值 基于密度的聚类 最大期望聚类 降维 潜语义分析 主成分分析 奇异值分解 K均值（k就是聚类的数量）随机指定两个中心点，或在已知的两个点中选择两个点 然后基于KNN进行分类 更新中心点，在分类好的两个分类中分别找一个质心点，在进行KNN 依次这样迭代，当质心点不再变化或到达迭代次数最大值时就停止迭代 推荐系统项目相似度计算一般常用的是余弦相似度 为什么使用余弦相似度？ 特征工程数据中抽取出的对结果预测有用的信息 特征的个数就是数据的观测维度 数值型特征处理归一化/幅度调整 特征之间应是平等的，区别应体现特征内部 可做幅度调整，其实就是让各特征的取值应差不多，公式如下 离散化 等步长 实时性比较高 等频 准确性比较高 类别型特征处理做到公平，并且要区分开他们 One-hot编码/哑变量 时间型特征处理连续值 持续时间 间隔时间 离散值 一天的那几个时间段 星期几 月/星期 工作日/周末 统计型特征处理 加减平均 分位线 次序性 比例类 基于UGC的推荐UGC 用户生成标签，反应用户兴趣的重要数据源 TF-IDF词频-逆文档频率 TF是词频 词出现的次数/总词数 IDF随着词在所有的文章中出现频率的增加而下降 CF 基于协同过滤基于近邻的推荐 基于模型的协同过滤 LFM降维方法，矩阵因子分解如下图所示，可以认为P的行向量是用户是否喜欢哪一类电影的数值 Q的列向量则是一个电影所含的类别 损失函数平方损失函数 需要加入正则项，防止过拟合 ALS交替最小二乘法 两个矩阵P，Q,先随机指定一个矩阵的值，然后再求另一个矩阵的值，然后在使用求出的矩阵的值再求另一个矩阵的值 反复迭代，直到阈值到了或则达到了损失函数收敛","link":"/hexo_blog/2022/04/20/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"title":"机器学习","text":"机器学习什么是机器学习？可以浅显的认为机器学习，就是在以往数据的基础上，搭建出一个模型，从而预测新数据 数据集构成特征值+目标值 算法分类分类目标值为离散型的数据 回归目标值为连续型的数据 监督学习没有目标值的学习 算法分类：k紧邻法，贝叶斯分类，决策树和随机森林，逻辑回归 回归：线性回归，岭回归 无监督学习有目标值的学习 聚类：kmeans sklearn数据集sklearn.datasets load_ 加载 搞来小的数据集 fetch_ 拿来 搞来大的数据集 训练集与测试集的比例： 测试集在20-30%左右 特征工程特征抽取得分函数设置好的参数，与特征值相乘之后得到的值，就是得分 损失函数损失函数有不同的公式，具体是哪个具体问具体分析 也就是求预测值与真实值之间的差距 加入正则化惩罚项的原因：就是防止过拟合等其他情况 Softmax分类器 整个过程分为以下三步： 得分带入到e的x次方中获得新的值（exp） 将所有新值求和，然后算出每个新值在其中所占的比例 将比例带入到-log函数中，正确选项的比例带入之后越接近1，代表做得越好 这样做就使得，得分与得分之间的差距更大了，从而更好判断损失 前向传播 将特征值和参数值带入其中，然后得到scores（得分），在使用损失函数计算损失，加上正则项，从而得到真正的损失函数值 反向传播 神经网络 input layer就是输入的参数，参数有多少，这层神经元就有多少 全连接 hidden layer1 就是处理上一层的数据，目的是为了得到更容易处理的数据 为了达到非线性，一般会在每一层处理之后，使用一个非线性函数进行处理，例如sigmod函数 激活函数 sigmod容易出现梯度为0的情况，即梯度消失，在反向传播时出现问题 参数初始化通常使用随机值初试化参数值 *0.001的原因就是让各个参数值之间的大小差距尽可能小一点 去过拟合DROP-OUT ​ 在训练阶段，每一次梯度下降的过程中，每一层中，随机不使用一部分神经元，等到下一次再去随机选择新的神经元不使用 卷积神经网络 其实就是选择一个核，这个核可以是3*3，可以自己指定，与图像中像素值做内积，从而提取出图像中的特征值 最右边的矩阵就是提取之后的值 tips 可以进行多次卷积，卷积的核可以不同，从而得到不同的特征值，从而获得图像的更多信息 步长和卷积核大小对于效果的影响 核不一定非是正方形，可以是矩形 这里的步长，水平方向和垂直方向的步长是相同的 卷积核一般最小是3*3的 边缘填充为了能够获取到矩阵中的全部信息，将四周都填上一列或者多列0 文本填充也是直接填充0就可以了 卷积计算公式 偏置参数就是在卷积之后，使用偏置参数重新调整下结果 池化层压缩 将原来很大的矩阵变成较小的矩阵，就可以减少处理的数据数量 压缩方式 最大池化 （max pooling） 根据核的大小，取出核覆盖的范围中最大的值 平均池化 得出平均值 总结在进行足够的卷积，池化，激活之后，得到是多个矩阵，无法得到我们想要的信息 这时将矩阵中信息拿出来，做成一个向量，带入全连接神经网络中，从而得到最后的结果 卷积网络实例Alexnet 卷积核太大了 步长太大 不加边缘扩充 Vgg 优势 在每次池化之后，会将矩阵的大小扩大，从而弥补池化损失的信息 劣势 时间很长 Resnet 把不好的层权重设置为0 感受野​ RNN（递归神经网络） RNN主要用于nlp中 图中可以看出在hidden layer中有一个循环的箭头 这个循环的箭头其实就是将之前输入的数据的中间值与现在输入的值一同输入hy中 这个中间值可能包含之前输入所有值的部分信息 根据这个特性，很适合nlp 流程图如下 例如处理 i am chinese 首先将这几个单词转换为对应的特征向量，然后再进行处理 rnn会将之前的所有信息留下来，可能会有错误 lstm有遗忘门，从而忘记一些不必要的特征 lstm Word2Vec文本向量化50-300维的向量一般就足够了 维度越高，提供的信息越多，计算之后的结果越可靠","link":"/hexo_blog/2022/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"title":"水下图像清晰化和目标识别技术研究 笔记","text":"水下图像清晰化和目标识别技术研究 笔记摘要针对水下图像能见度低和颜色失真等缺陷，提出基于颜色衰减曲线分解的水下图像复原算法 首先，研宄水下图像背景区域特性，提出背景区域评分公式，并结合四叉树层级搜索方法，计算背景光值。其次，研究水下图像颜色衰减曲线先验，创新性地分解颜色衰减曲线至ＲＧＢ坐标轴，计算初始透射图；再结合ＲＧＢ透射图通道间的数学关系以及几何约束，修正初始透射图，实现水下图像复原。 针对水介质对光线的不平衡吸收效应，提出基于亮通道的色彩补偿算法，平衡图像色彩 针对水下图像复原过程中噪声放大的问题，提出基于鲁棒水下成像模型的图像复原算法，在复原水下图像的同时抑制噪声。 针对静态卷积核不能动态适应变化的输入特征的问题，设计内部特征和卷积核校准模块，提高水下目标识别准确率。 特征金字塔中差异较大的特征层通常共用检测端，该方式将削弱检测端的特征转变能力。针对该问题，设计金字塔解耦模块，提高水下目标检测精度。","link":"/hexo_blog/2022/10/04/%E6%B0%B4%E4%B8%8B%E5%9B%BE%E5%83%8F%E6%B8%85%E6%99%B0%E5%8C%96%E5%92%8C%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6-%E7%AC%94%E8%AE%B0/"},{"title":"特征提取算法汇总","text":"ORB 具有旋转不变性 HOG这里是通过计算梯度来算出特征的 这里x轴和y轴的梯度就是直接相减就可以 以x轴为例，一般直接使用[-1,0,1]计算像素的梯度，并且效果是最好的，相对于sobel算子之类的","link":"/hexo_blog/2022/09/30/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/"},{"title":"深度学习","text":"深度学习","link":"/hexo_blog/2022/07/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"title":"深度学习","text":"Logsitic Regression （Logstic 回归算法）是一个二分类问题，即输出的结果只有0和1 这里回归的公式就是 w就是参数向量，b就是偏移量 就是激活函数 因为我们所需的数据应该是介于0，1之间的，而 很难保证是在0，1之间，这个时候就需要激活函数来控制这个式子的值域 sigmod函数 损失函数衡量yhat和y的之间的误差， 可以使用差的平方，但不建议，因为使用这种方式可能会导致梯度下降出现问题 以下是logsitic使用的损失函数 注意，这个损失函数仅仅适用于Sigmod激活函数这种返回值为（0，1）的 可以试一下，当y=1时，此时得到 即logyhat越大越好，此时就是yhat越接近1越好 当y=0时，得到 此时log（1-yhat）越大越好，即yhat越小越好，就是yhat越接近0越好 这时就得到了全部样本的损失函数 梯度下降 三个轴分别代表w，b，j（w，b），可以看出J（w，b)是凹函数 这就代表着我们可以找到损失函数的最小值 首先我们需要随即设置一个w和b的值，然后根据梯度下降的方式进行迭代，知道取到最小值 这里使用二维的方式来理解一下梯度下降 梯度下降的公式 这里alpha是我们指定的，称为学习率，也就是梯度下降的步伐，这个步伐不应该设置太小，也不能设置太大 Dj（w）/dw就是导数， 由上面的图可以知道，-alpha*dj/dw总是会使得J（w）的值更小，从而达到获取一个最小的损失函数值的目标 Logisitic回归的计算公式dw代表，J对w求导，其他的以此类推 z是根据特征计算的函数 a是激活函数 L是损失函数，损失函数是上面改进的那个损失函数 对全部样本做总体的梯度下降 这里的 就是损失函数 a是预测值，y是实际值 可以看出，全部样本的导数就是每个小样本导数的和在求平均 激活函数这里提到一点，神经网络中每一层的激活函数可以不同，这里的激活函数都是非线性函数 sigmod激活函数 一般用于输出函数，平常几乎不用 sigmod函数的导数 tanh激活函数 上述两种激活函数都有一个缺点，就是在z值很大或则很小时，梯度会很小，接近于0，这就导致了梯度下降时的速度会减慢 以上是tanh函数的导数 Relu函数 Leaky Relu 为什么要使用激活函数 上图表示的就是如果不适用激活函数，那么正向传播计算时，公式就是一个隐藏层的计算公式，这时就不如直接去掉这些隐藏层 两层隐藏神经网络的计算 这里的多样本反向传播的公式 注意这里的dw2和dw1实际上已经完成了所有样本的求和，最终求出的dw是所有样本的和 超参数 学习率 隐藏单元数 隐藏层数 激活函数 梯度下降次数 为什么称为超参数，就是因为这些超参数会影响w，b这些参数的取值 神经网络数据集的划分 训练集 训练模型 验证集/交叉集 用于选出哪一个模型更合适 测试集 测试训练的模型并评估 数据越大，训练集设置的比重越大 欠拟合拟合曲线并不能很好的分类数据 过拟合分类的太好了，太贴近测试数据了 高偏差和高方差 第一个是高方差，例如过度拟合 第二个是高偏差，例如欠拟合 对于神经网络来说，可以采用扩大神经网络的规模来解决以上问题 正则化目的就是防止过拟合 l2正则化 这里的w就是参数，m是样本总数 以下就是在神经网络中如何进行正则化，可以看到区别，这里的w已经是矩阵，而并非w向量了，n[l-1]是隐藏层层数，n[l]是节点数 dropout正则化 随机选择哪些节点需要去掉，顺便把他的连线也去掉，然后会得到一个精简的神经网络，在进行梯度下降 为什么正则化可以减少过拟合 单纯从这个公式来看，当lamdba越大时，w会越小，消除了一部分这些隐藏单元的影响 这种效果如果太强时也会导致欠拟合，因此还是需要找到一个中间的值，使得效果可以达到恰好拟合的状态 归一化 0均值 归一化方差 神经网络 看到p55","link":"/hexo_blog/2022/09/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-1/"},{"title":"融合图像评价指标","text":"融合图像评价指标EN L表示灰度级的数量，我们在实验中将其设置为256。pl是融合图像中对应灰度级的归一化直方图。熵越大，融合图像所包含的信息越多，融合效果越好。 SD 其中F是尺寸为M×N的融合图像，µ是融合图像F的平均值。具有高对比度的区域总是吸引人的注意，而具有较高对比度的融合图像往往导致更大的SD，这意味着融合图像获得更好的视觉效果 SSIMSSIM主要由三个分量组成：相关性损失、亮度失真和对比度失真。三个分量的乘积是融合图像的评估结果。 σx和σf表示标准偏差，σxf是源图像和融合图像的标准协方差相关，µx和µf表示源图像和融合图像的平均值，C1和C2以及C3是使算法稳定的参数。 SSIM值越大表示性能越好。 CCCC测量融合图像和源图像的线性相关度 CC越大越好 SFSF设计用于测量图像的梯度分布 SF越大，融合图像的边缘和纹理越丰富。 VIF测量融合图像的信息保真度，它由四个步骤计算：首先，将源图像和融合图像划分为不同的块；然后评估具有和不具有失真的每个块的视觉信息；随后评估每个子带的VIF；最后，基于VIF计算总体度量。","link":"/hexo_blog/2022/10/23/%E8%9E%8D%E5%90%88%E5%9B%BE%E5%83%8F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"},{"title":"玩爬虫不能不知道you-get，快进来看看","text":"@TOC 什么是you-get首先咱们先从字面上分析一下，各位兄弟搬好小板凳，接下来小王利用三年级英语水平给大家翻译一下 首先you的意思是你，get有拿的意思，众所周知英语擅长倒装，没错连起来就是–拿来吧你引入结束，现在回归正题，其实you-get就是一个别人替我们写好的一个下载神器，可以特别方便的在网络上下载资源，具体怎么使用，咱么继续往下看。 you-get的安装前提条件，电脑需要安装python环境，看过博主之前的爬虫文章的兄弟们相信都具备了这个条件，那么接下来就是安装you-get随便打开一个cmd，输入以下内容 1pip3 install --upgrade you-get 安装没有报错，显示内容与与下图显示大致相同，即成功 you-get的使用复制资源，然后在你想要存储文件的目录下打开cmd，也就是在地址栏输入cmd即可，如下图在cmd中输入以下内容 1you-get 视频链接 如下图所示即下载成功打开下载的文件就可以使用了 更多高级操作大家可以在cmd中输入以下内容进行查看 1you-get -h 总结一个小工具的使用，很简单，之前文章中很多人评论，我就专门看了看，最近有时间就写成了博客，下次有缘再写 推荐下自己的爬虫专栏，有几篇基础的爬虫文章，有兴趣的兄弟们可以看看基础爬虫专栏","link":"/hexo_blog/2021/10/22/%E7%8E%A9%E7%88%AC%E8%99%AB%E4%B8%8D%E8%83%BD%E4%B8%8D%E7%9F%A5%E9%81%93you-get%EF%BC%8C%E5%BF%AB%E8%BF%9B%E6%9D%A5%E7%9C%8B%E7%9C%8B/"},{"title":"视频划分为ts片段","text":"1ffmpeg -y -i ***.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb ***.ts 1ffmpeg -i ***.ts -c copy -map 0 -f segment -segment_list ***.m3u8 -segment_time 15 15s_%3d.ts 以上两步就可以将视频划分为ts，并生成m3u8文件 linux方式 1/data/software/ffmpeg-5.0-amd64-static/ffmpeg -y -i *.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb *.ts 1/data/software/ffmpeg-5.0-amd64-static/ffmpeg -i ***.ts -c copy -map 0 -f segment -segment_list ***.m3u8 -segment_time 15 15s_%3d.ts 后台启动服务 1java -jar tiangou-0.0.1-SNAPSHO.jar &gt;tiangou.log 2&gt;&amp;1 &amp;","link":"/hexo_blog/2022/02/07/%E8%A7%86%E9%A2%91%E5%88%92%E5%88%86%E4%B8%BAts%E7%89%87%E6%AE%B5/"},{"title":"jvm","text":"JVMjava二进制字节码运行环境 一次编写，到处运行的基础 jvm对外提供了一致的运行环境 自动内存管理机制—垃圾回收功能 数组下标越界检查 有些语言无法进行检查，可能导致越界的数组数据占据了其他程序的空间 多态 jre与jvm jvm+基础类库——–jre jre+编译工具——–jdk 内存结构 java源码—–》二进制字节码——–》解释器翻译为机器语言——–》cpu来执行 程序计数器：记录下一条jvm指令的执行地址， 例如指令如下 121 lalalal2 啦啦啦啦啦 1执行时，会将2放入程序计数器中，待1执行结束之后就在程序计数器中取得2进行执行，这样依次进行执行 一般是使用寄存器来实现的 特点 线程私有 每一个线程都有自己的程序计数器，当分给该线程的时间片结束之后，假如线程还未完成，则需要进行记录下一条指令的地址，等到重新分配时间片时可以继续执行该程序 不会内存溢出 栈每个线程需要一个栈，存放着多个栈帧，一个栈帧对应一个方法，每个方法运行时需要的内存 栈帧 参数 局部变量 返回地址 一个栈中可以有多个栈帧 每个线程只能有一个活动栈帧（） 这里方法调用的栈可以在idea中直观看到 tips 垃圾回收不涉及到栈内存 栈内存大小可通过 -Xss size 来设置程序栈大小 windows取决于虚拟内存大小 linux/macos/Oracle默认为1024kb 并不是栈越大越好，栈越大可运行的线程越少 idea通过以下方式进行设置 局部变量是否线程安全 就看变量是线程私有的还是共享的 线程私有就不需要考虑线程是否安全，共享的话就需要考虑 如果方法内的局部变量未逃离方法作用范围，就是线程安全的，例如如果该变量作为返回值，那么其他线程就可能会拿到这个变量，那么就会导致不安全 栈内存溢出 栈内存放的栈帧数量超出了栈的大小就回导致占内存溢出===》递归就可以做到，一直递归不停，就会产生栈内存溢出 栈帧过大也会导致栈内存溢出 线程运行诊断定位 1234top：定位哪一个进程对cpu占用高ps H -eo pid,tid,%cpu | grep 进程id：进一步定位哪一个线程引起的 jstack 进程id：列出该进程的所有的线程的信息 长时间未输出结果 jstack 进程id 列出所有的线程信息，可以展示出死锁信息 本地方法栈本地方法不是由java编写的，因为java无法直接与计算机底层进行交互，因此需要通过本地方法来进行对底层的交互，一般本地方法是由c或c++编写的。 这些本地方法利用的就是本地方法栈 堆 线程共享的，需要考虑线程安全问题 new创建的对象都是存放在堆 有垃圾回收机制 堆内存溢出不断生成新对象，并且所有对象一直在使用，就会导致堆内存溢出 修改堆空间大小 1-Xmx 8m 以下代码可以用来测试堆空间是否溢出的问题 1234567891011121314151617public static void main(String[] args) { int count=0; String s=&quot;123&quot;; try { List&lt;String&gt; list =new LinkedList&lt;&gt;(); count=0; while(true){ s=s+s; list.add(s); count++; } } catch (Exception e){ e.printStackTrace(); System.out.println(count); } } 启示 服务器内存越跑越小，可能是因为有一些内存未被来得及回收 堆内存诊断 jps工具：查看系统中有哪些java进程 1jps jmap工具：查看堆内存占用情况 1jmap -heap 进程id 测试代码 12345678910111213141516public void testThread(){ try { System.out.println(&quot;1&quot;); Thread.sleep(30000); byte array[]=new byte[1024*1024*10]; System.out.println(&quot;2&quot;); Thread.sleep(30000); array=null; System.gc(); System.out.println(&quot;3&quot;); Thread.sleep(30000); } catch (InterruptedException e) { e.printStackTrace(); } } jconsole工具：有ui的，多功能的检测工具 1jconsole jvisualvm 方法区存放方法，构造器，成员属性之类的数据 方法区在虚拟机启动时就创建，逻辑上是堆的组成部分，但不同的厂商不一定按照这个实现 方法区溢出运行时常量池常量池： 就是一张常量表，虚拟机指令根据这张常量表找到要执行的类名和方法名，参数类型，字面量等信息 运行时常量池： 常量池是*.class中的，当该类被加载，他的常量池信息就会放入运行时常量池中，并且把里面的符号地址变为真实地址 反编译1javap -v Main.class 这里编译的class文件在out文件夹下 如下图就是常量池 常量池加载过程 最开始时常量池中是没有数据的，是在一步步加载中填入的，是一种懒加载机制 常量池存放常量的结构是hash表，每次需要常量时就会以常量在hash表中查找，若不存在则创建 常量池与串池的区别运行常量池（constant pool）中存放的仅仅是符号，而并非对象，串池（StringTable）中存放的则是字符串对象，作用就是防止创建重复的字符对象 1.6和1.8中常量池和串池存放位置的差别 StringTable（串池）的垃圾回收1-Xmx16m -XX: +PrintStringTablestatistics -XX: +PrintGCDetails -verbose:gc -Xmx16m ：设置堆的大小 -XX: +PrintStringTablestatistics ：打印串池中的对象信息 -XX: +PrintGCDetails -verbose:gc : 若存在垃圾回收，则进行打印信息 -XX: StringTableSize=200000 : 因为串池的结构是数组加链表这种方式，数组中的一个关键字称为一个桶，这里就是设计桶的数量，桶的数量越大性能越好，但相对的占用空间就可能过大，造成资源浪费 StringTable性能调优 可以适当调大STringTable的数组长度也就是桶的数量，可以减少冲突从而使得查找效率得到提升 使用串池可对系统性能进行调优，若是new出来的字符串对象只存在堆中，并不会进入串池中，这时若是存在大量的重复的字符串对象，可以采用串池来对这些数据进行去重，所谓去重就是将利用串池的特性将大量的重复的字符串对象只存储一个字符串对象，其他对象只是对其的引用 直接内存操作系统内存 ByteBuffer为什么读写更快使用ByteBuffer实际上就是通过直接内存进行读取 传统io操作 因为java无法直接访问系统资源，因此需要再建立一个java缓冲区，整个过程就是：本地文件==》系统缓存==》java缓存==》使用 直接内存的io方式 此时文件直接放入直接内存缓冲区中，java可以直接读取，减少了一层缓冲区，从而使得速度得到提升 直接内存的溢出因为DM不受java垃圾机制管理，因此可能会出现内存溢出问题 测试代码 直接内存分配与释放的原理通过代码来申请直接内存的大小，这里直接内存不受jvm管理，因此需要在任务管理器里查看 1234ByteBuffer byteBuffer=ByteBuffer.allocateDirect(1024*1024*1024); System.out.println(&quot;try&quot;); System.in.read(); System.out.println(&quot;end&quot;); 直接内存的回收是通过unsafe对象来进行回收的 禁用显示回收的影响1System.gc（） //显式的垃圾回收 关闭显示垃圾回收机制，即System.gc（）无效 1-XX:+DisableExplicitGC 垃圾回收如何判断对象可以回收引用计数法即有一个引用该对象，则计数器加一，为0则释放， 弊端 循环引用：即A引用B,B也引用A，没有其他引用他们，但是他们互相引用，都无法释放，就会导致内存泄漏 可达性分析算法（java中使用的垃圾回收机制）根对象：肯定不可以当作垃圾回收的对象 如果一个对象没有被根对象引用，就可以回收 解析 扫描堆中的对象，看是否能够沿着GC Root对象为起点的引用链找到该对象，找不到，表示可以回收 抓取当前堆使用的快照 1jmap -dump :format=b,live,file=1.bin 21384 -dump ==》存储 format=b ==》存储二进制文件 live ==》只记录那些未被垃圾回收的内容 file=1.bin 设置存储文件 21384 进程id（jps获取活动的java的进程id） mat查看gc root对象 System class 系统对象 Busy Monitor 加锁的对象 Thread 活动线程中的对象，局部对象所引用的对象可左gcroot，同时参数中对象也是可以作为gcroot对象 可以作为GC Root的对象 System class 系统对象 Busy Monitor 加锁的对象 Thread 活动线程中的对象，局部对象所引用的对象可左gcroot，同时参数中对象也是可以作为gcroot对象 四种引用 强引用例如new出来的就是强引用 特点 只要沿着gc root链可以找到该对象，就无法被垃圾回收，例B对A-A4，以及ByteBuffer 只要没有直接或则间接对其强引用之后就可以垃圾回收了 软引用特点 只要未被gc root直接引用，垃圾回收时就会自动回收，例从C到软引用再到A2，当然此时需要B不在引用A2时，就可以发生垃圾回收 应用场景 强引用下导致堆空间溢出 12345678/** * 强引用会导致堆空间不够用 */ int _1M=1024*1024; List&lt;byte[]&gt; list=new LinkedList&lt;&gt;(); for(int i=0;i&lt;5;i++){ list.add(new byte[_1M*2]); } 软引用下 在这种方式下其实就是使用软引用进行嵌套强引用，也就是SoftReference嵌套byte数组，从而达到软引用的目的，这样一旦出现堆内存不够就会进行释放软引用对象 12345List&lt;SoftReference&lt;byte[]&gt;&gt; list=new LinkedList&lt;&gt;();for (int i = 0; i &lt;100 ; i++) { SoftReference softReference=new SoftReference(new byte[_1M*2]); list.add(softReference);} 这个过程中一旦出现了堆空间不够，就会清理软引用对象引用的对象，但是此时软引用对象还在，虽然占据内存比较小，但最好还是清理一下 使用引用队列进行处理，下方代码，关联了软引用队列，软引用关联的对象回收时，软引用对象会加入队列中，从而实现回收 这里我个人的理解就是判断这些软引用有没有引用其他对象，如果没有，则将其在队列中删除，从而将队列对软引用对象的强引用解除掉，从而实现对象的回收 1234567891011121314/** * 关联了软引用队列，软引用关联的对象回收时，软引用对象会加入队列中，从而实现回收 */ ReferenceQueue&lt;byte[]&gt; referenceQueue=new ReferenceQueue&lt;&gt;(); List&lt;SoftReference&lt;byte[]&gt;&gt; list=new LinkedList&lt;&gt;(); for (int i = 0; i &lt;10 ; i++) { SoftReference softReference=new SoftReference(new byte[_1M*2],referenceQueue); list.add(softReference); } Reference&lt;? extends byte[]&gt; poll = referenceQueue.poll(); while(poll!=null){ referenceQueue.remove(); poll=referenceQueue.poll(); } 弱引用特点 当没有强引用时，若内存不够会回收软引用的对象，无论够不够都会回收弱引用对象 释放之后，因为软弱引用仍占用空间，因此需将二者放入引用队列中，进行循环依次释放空间 应用实例 虚引用（必须配合引用队列） 之前的bytebuffer就是需要一个虚引用对象Cleaner，因为ByteBuffer若是在强引用引用结束之后，会对其进行回收，但是此时直接内存不由jvm管理，这就需要把虚引用对象放置在引用队列中，从而实现对直接内存的回收（虚引用对象就是Cleaner，来调用Unsafe的Free memory（）来进行释放） 终结器引用（必须配合引用队列）例如A对象重写了finalize，并且A即将被垃圾回收，会调用finalize方法，将放置一个终结器引用到队列中，会有一个优先级很低的线程会来检查队列中有无需要释放的引用，从而实现对象的回收 垃圾回收算法标记清除算法 判断哪些对象未被gcroot对象引用，对其进行标记 对标记对象进行清除，将对象的首地址存储在队列中，在新的对象分配地址时，会在队列中进行查找，判断有无空间，在进行分配 优点 清除速度快 缺点 会产生大量的碎片空间，导致总剩余空间虽然足够，但有些大空间对象仍无法分配到足够的内存，导致内存溢出 标记整理 判断哪些对象未被gcroot对象直接或间接引用，对其进行标记 清楚时，将可用的对象向前移动，从而使得内存空间更见紧凑，从而实现空间更加连续 优点 没有内存碎片 缺点 耗费时间较多，例如如果有引用对象引用就是将移动的对象，需要修改大量内容，造成浪费时间 复制算法 划分成两片区域，将from中存活的对对象复制到to中，待复制结束之后就对from所有的对象进行回收，然后交换from与to的位置 优点 没有碎片空间 缺点 需要占用双倍的内存空间 小结三种算法都会协同工作 大对象直接到老年代超过新生代大小时，直接到老年代中存放 分代回收 长时间使用的放在老年代中，用完即弃的放在新生代中，也可以认为重要的，常用的在老年代中，而不常使用的在新生代中 清理时先清理新生代，如果内存实在不够，再开始清理老年代 新生代 最开始对象存放在伊甸园中 一旦伊甸园中内存占满之后，就会开始触发垃圾回收（Minor GC，新生代回收） 先进行标记，然后将存活的对象复制到幸存区，将复制的对象寿命+1.然后交换from与to的位置，伊甸园剩余的对象就可以销毁了 然后又可以向伊甸园中分配对象，直到伊甸园又满了，继续执行上述操作，并且也需要测试from中有没有可以回收的，最后在进行回收伊甸园中与幸存区的对象 幸存区中的寿命超过一个阈值（最大寿命15次，存放寿命的数据是4bit，存放在对象里，4位）之后就会晋升到老年代中，若新生代承受不下了，即使没有到达阈值，也会放在老年代中 minor gc会引发 stop the world，在垃圾回收时需暂停其他用户的线程，直到垃圾回收之后在恢复其他线程的运行 老年代 当老年代中内存不足，会先触发minor gc，如果之后空间仍不足，则会触发老年代回收（Full GC），这次回收会将老年代中和新生代中的对象进行回收，也会引起stop the world，并且持续时间更长 老年代中存活的对象很多，并且采用的算法可能是标记清除或标记清理，时间会长。 若full gc 之后仍无从充足空间，则full of mememory error GC参数 含义 参数 堆初始大小 -Xms 堆最大大小 -Xmx或-XX:MaxHeapSize =size 新生代大小 -Xmn或(-XX:NewSize-size + -XX:MaxNewSize-size ) 幸存区比例(动态) -XX:InitialSurvivorRatio-ratio和-XX:+UseAdaptiveSizePolicy 幸存区比例（ratio是指伊甸园所在比例） -XX:SurvivorRatio= ratio 晋升阈值 -XX:MaxTenuringThreshold=threshold 晋升详情 -XX:+PrintTenuringDistribution GC详情 -XX:+PrintGCDetails -verbose:ge FullGC前MinorGC -XX:+ScavengeBeforeFullGC 幸存区比例不会变化的垃圾回收器 -XX: +UseSerialGC 串行垃圾回收器（新生代是复制算法，老年是标记整理算法） -XX:+UseSerialGC= Serial + Serialold 解析垃圾回收信息设置的虚拟机参数： -Xms20M -Xmx20M -Xmn10M -XX:+UseSerialGC -XX:+PrintGCDetails -verbose:ge 控制台信息 new generation : 新生代 这里总容量9m的原因是因为默认认为to不可以被占用，因此就默认减去了1m eden ： 伊甸园 from ：幸存区的from区 to ： 幸存区的 to 区 tenured generatioin ： 老年代 meta space : 元空间 后面的数字就是内存地址 默认的伊甸园占的比例是0.8 内存溢出在子线程会不会引起主线程结束子线程的内存溢出并不会引起主线程结束 垃圾回收器串行 单线程 适用于堆内存小，适合个人电脑 虚拟机参数 -XX:+UseSerialGC= Serial + Serialold 所有的线程须达到安全点之后才可以执行垃圾回收 其他线程需要等到垃圾回收线程结束之后才可以开始继续运行 吞吐量优先（垃圾回收时间占用总时间越少，吞吐量越高） 优先 适用于堆内存较大，多核cpu 单位时间内stw时间最短（总体时间） 虚拟机配置（1.8默认的） 并行的 新生代的（复制算法） 老年代的（标记整理算法） -XX:+UseParallelGC -XX: +UseParallelOldGC 开启上述其中一个，另一个会自动开启 设置垃圾回收线程个数 -XX: ParallelGCThreads=n 自适应调整新生代的大小，晋升阈值也会受影响 -XX;+UseAdaptivesizePolicy 调整吞吐量的目标，调整垃圾回收与总时间的占比 -XX:GCTimeRatio=ratio（垃圾回收时间比例=1/（1+ratio）） 最大暂停毫秒数，最大是200ms -XX:MaxGCPauseMillis=ms 也是所有线程到达安全点之后，就会开始多线程开始回收，线程个数与cpu核数有关，核数有多少，线程的上限就是多少 响应时间优先 多线程 适用于堆内存较大，多核cpu 注重垃圾回收单次stop the world（stw）时间尽可能短 并发进行执行垃圾回收 垃圾回收与普通线程同时进行，两类线程互相争夺时间片 老年代 （标记清除） 新生代 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC - SerialOld 一旦出现并发问题，老年代GC会退化为串行GC 设置并行线程数 设置并发线程数，建议设置为并行线程数的四分之一 -XX :ParallelGCThreads=n -XX:ConcGCThreads=threads 只要老年代到达percent之后，就进行清理，是给浮动垃圾留空间 -XX:CMSInitiating0ccupancyFraction=percent 做重新标记之前，先把新生代做一次垃圾回收 这里的原因是因为有新生代要回收的有很多，而且可能新生代还引用了老年代，但这些新生代本身就已经要被清除了，所以即使我们通过他们发现了一些老年代不能进行回收，但是后期这些新生代本身就要进行回收，实际上了做了无用功，因此可以提前对其进行清理，从而达到减少时间的目的 -XX:+CMSScavengeBeforeRemark 初始标记只会标记根对象，此时很快，但是会引发stw 并发标记会标记间接或直接引用的对象，此时与用户线程并发运行 并发标记后会及逆行重新标记，会引起stw（因为并发标记时，标记的内容可能地址会进行改变，因此需重新标记） 重新标记之后会进行并发处理 并发清理过程中，其他线程可能又会有新垃圾（浮动垃圾），这些垃圾下次处理，因此需要专门设置浮动垃圾空间 笔记 字符串字符串拼接123String a=&quot;123&quot;String b=&quot;55&quot;String c=a+b c的赋值其实是先调用Stringbuilder的toString方法生成一个新的String对象，然后返回给c 但是如下图这样 1234String a=&quot;a&quot;String b=&quot;b&quot;String c=&quot;a&quot;+&quot;b&quot;;String d=&quot;ab&quot;; 此时，c==d是true，因为在编译时，javac会默认认为”a”+”b”就是”ab”，因此直接调用常量池的内容就可以 主动将字符串对象放入串池itern（）：将字符串对象放入串池，若不存在，则放入，否则不进行放入 垃圾回收 原来的占用内存-&gt;回收后的内存，Full GC表示垃圾回收资源太少，因此采用更加强烈的垃圾回收，即软链接垃圾回收 初次回收时会将所有的弱引用对象引用的对象回收掉，若是回收之后内存依然不够，会对软引用在进行回收 并发与并行并发：并发是指两个或多个事件在同一时间间隔发生 并行：并行是指两个或者多个事件在同一时刻发生。","link":"/hexo_blog/2022/01/21/jvm/"},{"title":"jvm续集","text":"JVMG1（garbage first）垃圾回收器 同步注重吞吐量和地言辞，默认暂停目标是200ms 超大堆内存，会将堆划分为多个大小相同的Region，每个region可独立为老年代+新生代，划分多个region有利于加快速度 整体上是标记+整理算法，两个region之间是复制算法 g1在堆内存越来越大时，优势越大 -XX:+UseG1GC 使用G1垃圾回收器（JDK9之后默认为G1） 回收阶段 YC–》YC+CM–》MC 三者是循环进行的 Young Collection下图为多个region，最开始为空，E代表伊甸园，最开始对象分配到伊甸园区 E区占满之后会进行minor gc，将幸村对象存放在幸存区 当幸存区空间不足时，会将寿命达到阈值的移动到老年去，而那些未到年龄的则到新的幸存区 Young Collection + CM(concurrent mark) YOUNG GC进行初始标记 老年代占用空间比例到达阈值时，会进行并发标记（不会stw），阈值可由下面的参数决定 -XX: InitiatingHeapOccupancyPercent=percent（默认45%） o为老年区 Mixed Collection 对E，S，O三个区进行整体的回收 E中对象分配到S中，S中年龄达到阈值的会晋升到老年区 在整理o区时，系统会选择进行回收之后，可以拿出更多空间的o区，进行回收，将其中非垃圾的复制到另一个o中，从而实现在最大时间下可以完成这些垃圾回收 会进行最终标记，会stw 进行拷贝存活，会stw Full GC SerialGC 新生代内存不足发生的垃圾收集- minor gc 老年代内存不足发生的垃圾收集 - full gc ParallelGc 新生代内存不足发生的垃圾收集- minor gc 老年代内存不足发生的垃圾收集 - full gc CMS 新生代内存不足发生的垃圾收集- minor gc 老年代内存不足 G1 新生代内存不足发生的垃圾收集- minor gc 老年代内存不足 内存比例达到阈值(默认45%)，就会出发第二阶段和第三阶段回收 如果这两个阶段回收速度可以赶上产生垃圾的速度，就不需要发生full gc 若赶不上就会开始串行收集，导致stw 新生代垃圾回收的跨代引用根对象有一部分是来自老年代，新生代回收在进行标记时会需要查找这些根对象来进行判断那些需要回收，即垃圾，这时对老年代中所有对象进行扫描，有些耗费时间，这时就开始采用卡表的方式来对那些老年代中可做根对象的区域进行标记，称为脏卡,这时在进行回收时，直接扫描脏卡区域就可以，从而减小扫描范围，提高速度。 这里为了方便，老年区会划分为多个区域，方便来对脏卡的标记。 这里E区中也有Remember Set记录有哪些脏卡来引用当前E中的对象。 当引用变化时会及时更新脏卡，这里更新是一个异步过程，放在一个队列中进行处理 concurrent refinement threads更新 Remembered Set Remark（重新标记阶段） 黑色已经处理完成，灰色尚在处理，白色未被处理 处理结束之后，黑色代表不是垃圾，白色是垃圾 为什么重新标记因为在标记阶段是并发进行的，这时我们已经检测完A对象为垃圾，需要进行回收，但此时B对象作为不可回收的对象，有引用了A对象，此时A不应回收，但是我们已经标记A为可回收，此时就出现了错误，因此需要remark来对其进行重新标记 原理如果检测到在并发标记过程中，某对象的引用发生了变化，就需要对其进行写屏障，并将该对象放在队列中，在重新标记时对队列中的对象进行处理，重新对其进行标记。 字符串去重 jdk8中是用char数组来存放String对象 将所有新分配的字符串放入一个队列中 新生代回收时，G1并发检查是否有字符串重复 若值相同，则俩者引用同一个char数组 与串池的区别 串池关注的是String对象 字符串关注的是char[] 二者在jvm内部使用不同的字符串表 默认开启，模拟机参数为 -XX:+UseStringDeduplication（默认启用） 略微占用了cpu时间，回收时间稍长，但是总体性能有所提高 并发标记类卸载-XX:+ClassUnloadinglwithConcurrentMark 默认启用 所有对象都经过并发标记后，就能知道哪些类不再被使用，当一个类加载器的所有类都不再使用，则卸载它所加载的所有类 回收巨型对象 一个对象大于一个region的一半，成为巨型对象 G1不会拷贝巨型对象 回收优先考虑巨型对象 G1会跟踪老年代所有incoming引用，这样老年代 incoming引用为o的巨型对象就可以在新生代垃圾回收时处理掉（人话：如果巨型对象不被引用时，就可以在新生代进行回收，总之回收越早越好） JDK 9 并发标记起始时间的调整 垃圾回收的调优查看使用的GCjava -XX:+PrintFlagsFinal -version / findstr “GC” 垃圾回收调优最快的gc是不进行gc 数据是否太多 数据是否表示太臃肿 尽可能使用基本类型，从而减少内存使用 是否存在内存泄漏 新生代调优 TLAB是每个线程独自占有的伊甸园中的一片区域 新生代越大越好吗 太小，会导致可用空间太少，一旦发现空间不足，就会开始新生代回收 太大，会导致老年代空间较小，即使新生代很空闲，但是老年代已经不够了，此时就会触发full gc，此时浪费的时间更多 新生代占的大小为堆的四分之一到二分之一最好 幸存区的大小设置 要可以保留当前活跃对象和需要晋升的对象，一旦存储不下之后，老年区也会存储需要晋升的的对象，这就导致了有些对象生存时间并不长，但是这些对象进入了老年代，就导致了他活得时间太长了 长时间存活对象尽早晋升，要设置合理的晋升阈值 设置阈值 -XX:MaxTenuringThreshold=threshold 打印对象寿命信息 -XX:+PrintTenuringDistribution 老年代调优 调优案例gc频繁若是minor gc过于频繁，可能原因是新生代内存设置过小，此时可能导致gc频繁，从而使得响应时间增大，同样当新生代幸存区已经无力存放幸村对象时，就会导致这些对象放置在老年区，使得有些原本在 m gc阶段就可以清理的对象，必须要到full gc时期清理，从而导致了资源的浪费 解决方案 增大幸存区的大小即可 请求时期发生了full gc，单次时间较长remark之前先进行新生代的回收 老年代充裕下，发生了full gc1.7之前，元空间不足也会引起full gc 类加载与字节码技术 字节码文件生成字节码文件 javac -parameters -d . test.java 类文件结构 魔数 0-3字节，表示为class类型文件 ca fe ba be 版本 4-7字节 （16进制）00 34（52）–》jdk 8 常量池 8-9 常量池的长度， 00 23（35），表示#1-#34，#0不计入，也没有值 而后一个字节一项，方法字节后面留有4个字节，作为方法信息 这个东西，字节码是十六进制的，因此两位就是一个字节，前两位是类型，后面的内容就看情况而定 访问标识和继承信息 成员变量 以下为了更好查找类名，在常量池中存放以下信息，一一对应 方法信息 方法属性，就像方法中的代码，就被称为code属性，代码属性 如上图，按顺序来 00 01 是访问修饰，是一个公共的方法还是其他的 00 07 是方法的名字 00 08 方法参数 00 01 方法属性的数量 00 09 常量池中对应属性 00 00 00 2f 27 代表后面内容的长度 字节码指令 javapjavap -v test.class -v代表详细信息 dscriptor 参数类型 flags 代表方法是public和static修饰的（访问修饰符） stack 最大操作栈深度 locals 局部变量表的长度 args_size 参数的个数 前面的数字代表字节码的行号 line 这里的行号：对应字节码中的 执行流程 类加载器将main所在的类进行类加载 常量池的内容到运行时常量池（属于方法区） 比较小的数字，存储在方法的字节码里，但一旦超过了Short的范围，就存储在常量池中 方法字节码存入到方法区 main线程开始运行分配栈帧内存，绿色的为局部变量表，蓝色为操作数栈，分别对应上述字节码文件中的stack（数量）与locals（深度） 之后执行引擎就开始执行字节码的命令 bipush10，将10压入操作数栈中 istore1 将操作数栈中的内容送到栈帧局部变量的1号槽位（槽位编号由左向右，由0开始） ldc #3 取出运行时常量池中的第三项，放入操作数栈 istore 2 将操作数栈的内容放入局部变量表中2号槽位 iload_1 将局部变量表中的一号槽位的内容加载至操作数栈 iload_2 与上面同理 iadd 弹出两个变量，计算后，放入操作数栈中 getstatic #4 通过常量池的内容，来获取堆中out对象的引用，将该对象引用（地址）放入至操作数中 invokevirtual #5 在常量池中找出5号内容，找到新的方法，再分配一个新的栈帧，将栈顶的内容传递给新的栈帧，新方法结束之后，会将栈帧将方法栈中弹出，然后清空操作数栈，return就结束，同时主方法的栈帧在方法栈中也被弹出 a++ iinc 是在局部变量上直接进行加一 a++ 是先load在iinc ++a 实现iinc在load iinc 槽位，自增值 条件判断 循环 构造方法cinit() V 整个类的构造方法 会将所有的静态代码块进行以下整合，整合之后，在类创建时，会从上至下进行执行 init（）V 每个实例对象的构造方法 方法调用 字节码如下 new 分配内存，分配成功将对象的引用放入操作数栈 dup 复制栈顶的地址，都是刚刚new出的引用 invokespecial #3 调用栈顶引用的方法 （#3对应常量池的方法），调用结束后，引用会被删除 astore_1 将另一个对象引用存储到局部变量表中 aload_1 在局部变量表中引入 pop是将刚刚调入的引用删除掉，因为后面要执行静态方法，不需要该对象 invokestatic和invokespecial都是调用确定的方法，例如像是静态方法，构造方法，私有方法，这一类直接可以找到方法的入口地址。 invokevirtual则是去调用那些不确定的方法，例如public这一类的，这一类可被重写，需要多次访问获取方法的入口地址，因此会动态进行调用，即动态绑定 多态原理来到jdk目录下输入以下命令，启动hsdb工具 1java -cp ./lib/sa-jdi.jar sun.jvm.hotspot.HSDB 进入图形界面attach 进程id 禁用指针压缩 1-XX:-UseCompressedOops -XX:-UsecompressedClassPointers file下连接到Hotspot process tools下选择 find object by query 这里就是class的底层结构 分为对象头和成员变量，该对象只有对象头 前八字节是对象markword，锁标记等 后八字节是类型指针 总结 异常处理代码 123456789public static void main(String[] args) { int i =0; try{ i=10; } catch (Exception e){ i=20; } } 上图为字节码文件 exception table中就是存储着处理异常的位置 由2到5，监管位置不包括尾，即不对第五行进行异常监督 若出现异常会与监管的异常相匹配则会跳到target指向的位置 八号这里的astore_2就是将异常对象存入到变量表中 关于多个异常处理和判断异常这里写出代码反编译就可以理解 finally代码 1234567891011public static void main(String[] args) { int i =0; try{ i=10; } catch (Exception e){ i=20; }finally { i=30; } } 字节码文件 astore_3 是将并非我们要补获的异常存储到3号槽中，然后aload3则是将该异常取出来，在由athrow进行抛出 面试题11234567891011public static int test(){ try { return 1; } catch (Exception e){ } finally { return 2; }} 返回2 由字节码可以看出，仅仅将1存储到局部变量表，最终返回的却是2， 出现异常时也类似解读 这也出现了问题，finally中出现了return，就会吞掉异常这就导致了异常不被捕获 面试题212345678910111213public static int test(){ int i=0; try { return i; } catch (Exception e){ return i; } finally { i=1; } } 字节码文件 整体的思路就是将要返回的值保存到局部变量表中，等到finally时也还是会返回刚刚要返回的值 symchronized代码 123456public static void main(String[] args) { Object o = new Object(); synchronized (o){ System.out.println(&quot;test&quot;); }} 字节码文件 dup对o进行复制两次引用 第一个引用用于调用构造方法 第二个放入局部常量表中 编译期处理语法糖java编译器将*.java源码编译为class文件的过程中，自动生成和转换的以写代码，主要为了减轻程序员的负担，成为语法糖 构造器尽管我们没有设置构造方法，jvm会自动帮我们生成一个构造方法，在字节码中可以体现 这里的字节码文件里还是给我们生成了构造方法 自动拆装箱即java的基本类型与包装类型之间会有自动的类型转换，是在JDK5之后加入的 如下面的代码 12345public static void main(String[] args) { Integer x=0; int y=x; } 装箱：基本类型到封装类型 拆箱：封装到基本类型 泛型集合取值泛型也是在JDK 5开始加入的特性，但java在编译泛型代码后会执行泛型擦除的动作，即泛型信息在编译为字节码之后就丢失了，实际的类型都当做了Object类型来处理; 如下图 12345public static void main(String[] args) { List&lt;Integer&gt; list=new ArrayList&lt;&gt;(); list.add(10); Integer integer = list.get(0);} 字节码文件 由14行命令可以看出，当泛型对象生成结束之后，而后在对泛型对象操作时并不会按照泛型的类型来调用方法，而是直接按照一律object来处理，这就是泛型的擦除 在返回时也是直接返回object，但是返回之后会在27这里进行一下类型的转换，也就是checkcast 类加载阶段加载 例如访问String时，先访问String.class（这就是_java_mirror），通过这个class在调用instanceKlass在进行操作 链接 验证：验证类是否符合JVM规范，安全性检查 准备：为static变量分配空间，设置默认值（分配空间与赋值是分开的） 测试代码如下 解析 将常量池中的符号引用解析为直接引用 初始化其实就是cinit阶段，虚拟机需保证这个各类的构造方法的线程安全 类加载器 由上而下，加载器的优先级依次减小，优先级小的加载器加载类时需要先询问优先级高的是否加载过，从而判断是否要加载，例如ac问ec，ec问bc。这种委托方式称为双亲委派的加载模式。 bc无法直接由java调用，因为bc是c++写的。 Bootstrap Classloader（启动器加载类） /a后追加的路径就是将该路径追加至jre/lib下，从而调用bc Extention classloader（扩展器加载类）打jar包 因为需要打成jar包放在ec地文件目录下 1jar -cvf my.jar com/xiaow/**.class 放在ec的目录下，这样在new时，会一级一级向上查询，优先级高的进行加载 就不会使用ac加载 双亲委派委派上级优先做类的加载，若上级没有再由本级进行加载 大致的过程就是先是一步步向上级调用，来查找目标class，一旦parent为null，代表上级已是bc，此时直接调用特定方法，由bc进行加载，若加载为空，则由本层加载，然后返回加载的值，下级收到之后在进行判断是否进行查找或直接返回 每级被调用时都需要在本级中查找一下是否已经加载了 线程上下文类加载器这里仔细看看 目前理解：并未采取双亲加载机制进行加载，而是采用直接调用ac进行加载 自定义类加载器 不同的类加载器加载的类并不是同一个 加载器相同，包名和类名相同才被认为是一个class 自定义加载器这里主要是要重写findclass方法 1234567891011121314151617181920212223import java.io.ByteArrayOutputStream;import java.nio.file.Files;import java.nio.file.Paths;public class MyClassLoader extends ClassLoader { @Override protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { try { String path = &quot;D:\\\\java\\\\&quot; + name + &quot;.class&quot;; ByteArrayOutputStream out=new ByteArrayOutputStream(); Files.copy(Paths.get(path),out); byte[] bytes = out.toByteArray(); Class&lt;?&gt; aClass = defineClass(name, bytes, 0, bytes.length); return aClass; } catch (Exception e) { e.printStackTrace(); return null; } }} 加载自定义位置的类 这里采用了暴力反射 12345678910111213141516public class Test4 { public static void main(String[] args) { MyClassLoader myClassLoader = new MyClassLoader(); try { Class&lt;?&gt; xiaoW = myClassLoader.loadClass(&quot;xiaow&quot;); Field[] declaredFields = xiaoW.getDeclaredFields(); Field a = xiaoW.getDeclaredField(&quot;a&quot;); a.setAccessible(true); Object o = xiaoW.newInstance(); a.setInt(o,2); System.out.println(a.getInt(o)); } catch (Exception e) { e.printStackTrace(); } }} 运行期优化若多次进行相同的操作，运行的时间会慢慢减少 热点代码：就是执行次数较多的代码 逃逸分析：观察对象是否会被使用，如果外层不调用该对象，称为不逃逸，这时就不创建对象，C2对字节码进行修改 方法内联 如上，如果热点方法的内容不是太长，会直接把代码取出来，直接执行，这就称为内联 打印内联信息 1-XX:+unlockDiagnosticvoptions -XX:+PrintInlining 字段优化反射优化前十六次调用时会调用本地方法（c++编写），速度比较慢，一旦第十七次之后就开始采用由运行期间动态生成的访问器，这时苏幅就快了 例如反射执行对象的方法时，最开始invoke是调用本地方法进行，在调用次数达到一个阈值之后就会开始调用动态生成的访问器 这里可以看出来，十七次时调用的invoke实际上是已经开始直接调用该方法了，并未通过反射，效率自然就快要赶上直接调用了。","link":"/hexo_blog/2022/01/22/jvm%E7%BB%AD%E9%9B%86/"}],"tags":[{"name":"神经网络","slug":"神经网络","link":"/hexo_blog/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"Harmony","slug":"Harmony","link":"/hexo_blog/tags/Harmony/"},{"name":"爬虫","slug":"爬虫","link":"/hexo_blog/tags/%E7%88%AC%E8%99%AB/"},{"name":"人工智能","slug":"人工智能","link":"/hexo_blog/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"安全","slug":"安全","link":"/hexo_blog/tags/%E5%AE%89%E5%85%A8/"},{"name":"python","slug":"python","link":"/hexo_blog/tags/python/"},{"name":"nginx","slug":"nginx","link":"/hexo_blog/tags/nginx/"},{"name":"随笔","slug":"随笔","link":"/hexo_blog/tags/%E9%9A%8F%E7%AC%94/"},{"name":"深度学习","slug":"深度学习","link":"/hexo_blog/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"工具","slug":"工具","link":"/hexo_blog/tags/%E5%B7%A5%E5%85%B7/"},{"name":"论文阅读","slug":"论文阅读","link":"/hexo_blog/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"},{"name":"论文阅读笔记","slug":"论文阅读笔记","link":"/hexo_blog/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"categories":[{"name":"java","slug":"java","link":"/hexo_blog/categories/java/"},{"name":"爬虫","slug":"爬虫","link":"/hexo_blog/categories/%E7%88%AC%E8%99%AB/"},{"name":"随笔","slug":"随笔","link":"/hexo_blog/categories/%E9%9A%8F%E7%AC%94/"},{"name":"python","slug":"python","link":"/hexo_blog/categories/python/"}]}