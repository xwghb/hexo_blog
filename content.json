{"pages":[],"posts":[{"title":"Harmony开发样例","text":"@[TOC](开发一个可以看小姐姐照片的鸿蒙应用 鸿蒙开发入门) 效果图先整张效果图，丑点是丑点，但可以用，买不起鸿蒙系统手机的我，只配用虚拟机。 前言要说目前最火的手机操作系统，要我来看的话那必然是鸿蒙无疑。16号刚刚结束了第五次鸿蒙内测，在看到这次的内测名单之后，居然有970的机器，这是不是说明俺这手里奋战了三年的荣耀play也可以生鸿蒙了，但现实是970三孤儿果然名不虚传，还是没有我们。那么言归正传，这次突然想做一个基于鸿蒙的小demo，然后又想到了我之前爬取的小姐姐图片链接还没有用武之地，这俩是不是可以结合一下？上次有这样的想法还是上一次，那么就做一个看小姐姐的小demo吧，开整开整。 实现思路之前在网上看到有直接把图片下载下来然后放进项目中的，这个很明显不适合我，不仅占的空间大，而且图片还得手动更新，这可不行 我们采用的是调用图片链接接口获取所有的图片链接，一个链接只是一个字符串要比图片占的空间小太多了，将这些链接存储在内存中，有兴趣的可以存在数据库里，然后每次随机获取一条链接就可以，由这条链接获取图片信息，将图片渲染到页面就可以。 整个流程简单的一塌糊涂，总结一下就是 拿取图片链接 由链接获取图片信息 渲染至显示页面 具体实现建立项目这个比较基础了，就不说了，如果不大了解的兄弟们，直接去官方文档看看就可以，建立流程非常简单。 建立http链接拿取图片链接设置网络权限我们需要访问网络，就必须要设置网络权限，来到config.json文件中，添加以下内容 1234567891011121314&quot;reqPermissions&quot;: [ { &quot;name&quot;: &quot;ohos.permission.INTERNET&quot; }, { &quot;name&quot;: &quot;com.wxr.xiaowpic.DataAbilityShellProvider.PROVIDER&quot; }, { &quot;name&quot;: &quot;ohos.permission.GET_NETWORK_INFO&quot; }, { &quot;name&quot;: &quot;ohos.permission.SET_NETWORK_INFO&quot; } ], 直接在module中添加如上内容，如下图 设置允许http请求这里注意，重点哈，鸿蒙默认的是发起https请求，因此如果我们发起http请求是会报错的，这里需要修改一下还是在config.json下，在deviceConfig中添加以下内容 12345&quot;default&quot;: { &quot;network&quot;: { &quot;cleartextTraffic&quot;: true } } 如下图所示，这里吐槽一下，我再寻找怎么设置允许发起http响应的时候，发现好多文章都一样，而且都不适用于我这个项目，还有的人复制别人的文章也能复制错，绝了。 发起http请求，并获取返回的数据json数据解析这里要使用到alibaba的fastjson工具类，在build.gradle引入如下依赖 1implementation group: 'com.alibaba', name: 'fastjson', version: '1.2.73' 如下图所示 发起请求，获得响应内容这里使用的是大佬封装好的专门用于请求接口的一个工具，ZZRHttp，同样需要引入依赖，引入过程和上面fastjson引入过程一致。 1implementation 'com.zzrv5.zzrhttp:ZZRHttp:1.0.1' 获取接口数据的具体实现如下，这里解释一下，https://2fd8e89d.cpolar.io/getAll这个接口地址，是获取图片链接的，是我本地的服务，所以大家如果需要的话，我可以把我的服务代码发给你们，包括存储图片链接的数据库。 12345678910111213141516171819202122232425ZZRHttp.get(&quot;https://2fd8e89d.cpolar.io/getAll&quot;, new ZZRCallBack.CallBackString() { @Override public void onFailure(int code, String errorMessage) { //http访问出错，此部分在主线程中工作,可以更新UI等操做。 MyLabel.error(&quot;访问图片链接接口出错&quot;); new ToastDialog(getContext()).setText(&quot;网络连接出问题了，请稍后重试&quot;).show(); } @Override public void onResponse(String response) { //http访问成功，此部分在主线程中工作，可以更新UI等操作。 MyLabel.info(&quot;获取图片链接成功&quot;); new ToastDialog(getContext()).setText(&quot;正在初始化，稍后&quot;).show(); //将字符串转换为json对象 JSONObject jsonObject = JSONObject.parseObject(response); //将其中返回的图片链接转换为列表 JSONArray info = (JSONArray) jsonObject.get(&quot;info&quot;); MyLabel.info(&quot;拿取数据量：&quot; + info.size()); info.forEach(item -&gt; { srcs.add(item.toString()); }); new ToastDialog(getContext()).setText(&quot;初始化成功，开始你的快乐吧&quot;).show(); MyLabel.info(&quot;内存中数据数量&quot; + srcs.size()); } }); 将获取的图片链接放入内存中就是声明一个静态列表变量，目的是为了下次获取图片链接时可以直接在这里拿取图片链接 123456789101112131415161718192021222324252627282930313233343536373839public void initData() { PicDao picDao = new PicDaoImpl(getContext()); //使用后台线程进行初始化 TaskDispatcher refreshUITask = createParallelTaskDispatcher(&quot;&quot;, TaskPriority.DEFAULT); refreshUITask.syncDispatch(() -&gt; {// List&lt;PicSrc&gt; list = picDao.list(); //判断内存中有无数据 if (srcs.size() == 0) { MyLabel.info(&quot;内存中没东西，第一次打开应用&quot;); MyLabel.info(&quot;调用图片接口获取图片链接列表&quot;); ZZRHttp.get(&quot;https://2fd8e89d.cpolar.io/getAll&quot;, new ZZRCallBack.CallBackString() { @Override public void onFailure(int code, String errorMessage) { //http访问出错，此部分在主线程中工作,可以更新UI等操做。 MyLabel.error(&quot;访问图片链接接口出错&quot;); new ToastDialog(getContext()).setText(&quot;网络连接出问题了，请稍后重试&quot;).show(); } @Override public void onResponse(String response) { //http访问成功，此部分在主线程中工作，可以更新UI等操作。 MyLabel.info(&quot;获取图片链接成功&quot;); new ToastDialog(getContext()).setText(&quot;正在初始化，稍后&quot;).show(); JSONObject jsonObject = JSONObject.parseObject(response); JSONArray info = (JSONArray) jsonObject.get(&quot;info&quot;); MyLabel.info(&quot;拿取数据量：&quot; + info.size()); info.forEach(item -&gt; { srcs.add(item.toString()); }); new ToastDialog(getContext()).setText(&quot;初始化成功，开始你的快乐吧&quot;).show(); MyLabel.info(&quot;内存中数据数量&quot; + srcs.size()); } }); } else { MyLabel.info(&quot;已经有内容了&quot;); } }); } 获取网络图片并展示在页面上http请求工具类这个类的主要作用就是发起http请求，并返回响应字节流，其实就是获取图片的字节流，代码如下 12345678910111213141516171819202122232425262728293031323334353637package com.wxr.xiaowpic.util;import com.wxr.xiaowpic.label.MyLabel;import com.zzrv5.mylibrary.ZZRCallBack;import com.zzrv5.mylibrary.ZZRHttp;import ohos.hiviewdfx.HiLog;import ohos.utils.net.Uri;import java.io.InputStream;import java.net.HttpURLConnection;import java.net.MalformedURLException;import java.net.URL;import java.net.URLConnection;public class HttpUtils { //url就是要访问的网络资源，methodType就是请求方式 public static InputStream getInput(String url,String methodType){ InputStream inputStream = null; try { URL url1=new URL(url); HttpURLConnection urlConnection = (HttpURLConnection) url1.openConnection(); urlConnection.setRequestMethod(methodType); urlConnection.connect(); int rescode=urlConnection.getResponseCode(); if(rescode==HttpURLConnection.HTTP_OK){ inputStream=urlConnection.getInputStream(); } } catch (Exception e) { HiLog.error(MyLabel.LABEL_LOG,e.getMessage()); HiLog.error(MyLabel.LABEL_LOG,e.getCause().toString()); } return inputStream; } } 字节流转图片工具类没啥好说的，复制就可以用 1234567891011121314151617181920212223242526272829303132333435package com.wxr.xiaowpic.util;import com.wxr.xiaowpic.label.MyLabel;import ohos.hiviewdfx.HiLog;import ohos.hiviewdfx.HiLogLabel;import ohos.media.image.ImageSource;import ohos.media.image.PixelMap;import java.io.InputStream;public class ImageUtils { private static final HiLogLabel LABEL_LOG = new HiLogLabel(3, 0xD001100, &quot;XiaoW&quot;); public static PixelMap createPixelMap(String imageUrl) {//获取图片字节流信息 InputStream inputStream = HttpUtils.getInput(imageUrl,&quot;GET&quot;); PixelMap pixelMap=null; ImageSource.SourceOptions sourceOptions = new ImageSource.SourceOptions(); sourceOptions.formatHint = &quot;image/jpeg&quot;; HiLog.info(MyLabel.LABEL_LOG,(inputStream==null)+&quot;&quot;); try { ImageSource imageSource = ImageSource.create(inputStream,sourceOptions); pixelMap = imageSource.createPixelmap(null); } catch (Exception e){ HiLog.info(LABEL_LOG,e.getMessage()); } return pixelMap; }} 图片展示在页面这里采用的是按钮点击之后进行图片的渲染，其中图片链接是在我们获取的图片链接随机读取一个，然后将该照片渲染至页面 123456789101112131415161718192021222324button.setClickedListener(new Component.ClickedListener() { @Override public void onClick(Component component) { TaskDispatcher refreshUITask = createParallelTaskDispatcher(&quot;&quot;, TaskPriority.DEFAULT); refreshUITask.syncDispatch(() -&gt; { //在链接列表中随机取一个数据 int index = (int) (Math.random() * srcs.size()); MyLabel.info(srcs.get(index)); String url=srcs.get(index); MyLabel.info(&quot;开始获取图片&quot;); //访问线上图片 PixelMap pixelMap = ImageUtils.createPixelMap(url); getContext().getUITaskDispatcher().asyncDispatch(new Runnable() { @Override public void run() { //Image组件填充位图数据，ui界面更新 image.setPixelMap(pixelMap); pixelMap.release(); } }); }); } }); 总结之前没有自己做过移动端的demo，总之收获还是不少的，所以期间出了不少问题，需要全部代码的兄弟们私信就好。","link":"/hexo_blog/2021/08/19/Harmony%E5%BC%80%E5%8F%91%E6%A0%B7%E4%BE%8B/"},{"title":"Python爬取美女图片","text":"简述作为一个考研狗，每天除了日复一日的复习外，偶尔也想给自己寻找一些生活的小乐趣，今天突然想到了自己曾经稍微接触的爬虫，想看看可以爬取些图片放到电脑上，就花了些时间改了改之前的爬虫代码，爬取了一部分照片先量一下战绩吧。照片不多但也算是自己的一次爬虫小经验。 实现思路爬虫的网页很简单，照片真实路径都在页面中直接可以拿到主要流程就是先进入照片浏览的主页，每个照片的主页都会链接几个照片页面，像下面这样，每个图片都会链接一个网页图片链接的网页如下图所示但是这个页面显示的图片还是不够高清，这个网站有一个规律，更高清的照片存放的网页就在现在这个页面的路径后跟一个 -1920x1080 的htm中，进入这个htm之后展示的照片才是我们要的，拿到图片的url就直接下载就好，就这样一直循环，所有的照片就都下载下来了。 关键代码文件下载1234567891011121314151617181920212223242526272829303132333435363738394041import requestsimport timedef downloadFile(name, url): try: headers = {'Proxy-Connection': 'keep-alive'} r = requests.get(url, stream=True, headers=headers) print(&quot;=========================&quot;) print(r) length = float(r.headers['Content-length']) f = open(name, 'wb') count = 0 count_tmp = 0 time1 = time.time() for chunk in r.iter_content(chunk_size=512): if chunk: f.write(chunk) # 写入文件 count += len(chunk) # 累加长度 # 计算时间 两秒打印一次 if time.time() - time1 &gt; 2: p = count / length * 100 speed = (count - count_tmp) / 1024 / 1024 / 2 count_tmp = count print(name + ': ' + formatFloat(p) + '%' + ' Speed: ' + formatFloat(speed) + 'M/S') time1 = time.time() f.close() return 1; except: print(&quot;出现异常&quot;) return 0;def formatFloat(num): return '{:.2f}'.format(num)if __name__ == '__main__': downloadFile('D://file//photo//hd.jpg', 'https://browser9.qhimg.com/bdr/__85/t01753453b660de14e9.jpg') 文件下载没什么好说的，复制就可以用，这里做了一个异常捕获的处理，因为可能出现连接不上资源，或则目标服务器强制关闭连接的可能，做这个异常处理就是为了判断有没有异常出现，从而进行相应的处理 爬虫代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# -*- codeing = utf-8 -*-# @Time : 2021/6/19 23:01# @Author : xiaow# @File : PhotoSpider.py# @Software : PyCharmfrom bs4 import BeautifulSoup # 网页解析import xlwt # excel操作import sqlite3 # 数据库操作from api import spider2 as spiderimport timefrom api import FileDownload as fdimport re # 正则表达式imglink = re.compile(r'&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot; title=&quot;.*?&quot;&gt;&lt;img alt=&quot;.*?&quot; src=&quot;.*?&quot;/&gt;&lt;b&gt;.*?&lt;/b&gt;&lt;/a&gt;', re.S)img2link = re.compile(r'&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot;&gt;.*?&lt;span&gt;（1680x1050）&lt;/span&gt;&lt;/a&gt;', re.S)img3link = re.compile(r'&lt;img alt=&quot;.*?&quot; src=&quot;(.*?)&quot; title=&quot;.*?&quot;/&gt;', re.S)# 获取照片页面路径def getPhoto(url): srcs = [] html = spider.askURL(url); bs = BeautifulSoup(html, &quot;html.parser&quot;); for item in bs.find_all('a', target=&quot;_blank&quot;): item = str(item) src = re.findall(imglink, item) if (len(src) != 0): srcs.append(&quot;http://www.netbian.com&quot; + src[0]) return srcs;# 照片主页显示的照片不够清楚，这里根据这个网站存储照片的规律，拼接了一个地址，这个地址的照片比较高清一些def getPhotoUrl(url): purls = []; url3 = &quot;http://www&quot;; url2 = url.split(&quot;.&quot;) for j in range(1, len(url2)): if j == len(url2) - 2: url3 = url3 + &quot;.&quot; + url2[j] + &quot;-1920x1080&quot; else: url3 = url3 + &quot;.&quot; + url2[j] return (url3)# 下载照片def downloadPhoto(url): html = spider.askURL(url); bs = BeautifulSoup(html, &quot;html.parser&quot;); for item in bs.find_all(&quot;img&quot;): item=str(item) itemsrc=re.findall(img3link,item) if(len(itemsrc)!=0): return itemsrc[0]if __name__ == '__main__': src = &quot;http://www.netbian.com/mei/index_&quot;; # 拼接照片主页的路径 for i in range(2,163): time.sleep(5) src2 = &quot;&quot;; src2=src+str(i)+&quot;.htm&quot; urls=getPhoto(src2) for j in range(len(urls)): time.sleep(3) fd.downloadFile('D://file//photo//hd'+str(time.time())+&quot;.jpg&quot;,downloadPhoto(getPhotoUrl(urls[j]))) 成果展示几张照片吧 更新解析网页的封装类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#-*- codeing = utf-8 -*-#@Time : 2021/3/1 16:16#@Author : xiaow#@File : spider2.py#@Software : PyCharmimport re # 正则表达式import sysimport urllib.request, urllib.error # 指定url，获取网页数据from bs4 import BeautifulSoup # 网页解析import xlwt # excel操作import sqlite3 # 数据库操作baseurl = 'https://movie.douban.com/top250?start='imglink = re.compile(r'&lt;a href=&quot;.*?&quot; title=&quot;.*?&quot;&gt;', re.S)# titlelink = re.compile(r'&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;')# findlink = re.compile(r'&lt;a href=&quot;(.*?)&quot;&gt;') # 创建正则表达式 表示规则# 1.爬取网页def getData(url): urllist = [] valuelist = [] # 2.解析数据 img = [] src = [] title = [] for i in range(0, 10): url = baseurl + str(i * 25) html = askURL(url) bs = BeautifulSoup(html, &quot;html.parser&quot;) print(bs) # urllist.append(bs.a.attrs[&quot;href&quot;]) # valuelist.append(bs.a.string) # return urllist, valuelist for item in bs.find_all('div', class_=&quot;item&quot;): # 查找div 并且该div应满足class=item # print(item) item = str(item) # titlel = re.findall(titlelink, item) # title.append(titlel) # srcl = re.findall(findlink, item) # 正则表达式进行筛选 # for s in srcl: # src.append(s) imgl = re.findall(imglink, item) # 正则表达式进行筛选 for i in imgl: img.append(i) return title, img, src;# 得到一个url的网页内容1def askURL(url): head = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36&quot;, &quot;Cookie&quot;: '_ga=GA1.2.1191993538.1623990557; _gid=GA1.2.176559558.1623990557; HstCfa3699098=1623990557028; HstCmu3699098=1623990557028; HstCnv3699098=1; HstCns3699098=1; newurl=0; __dtsu=10401623990557D693AE61F09F524965; pbnfgecookieinforecord=%2C64-32128%2C64-32129%2C; HstCla3699098=1623991353818; HstPn3699098=7; HstPt3699098=7' } req = urllib.request.Request(url=url, headers=head) html = &quot;&quot; try: response = urllib.request.urlopen(req) html = response.read() except Exception as result: print(result) return html# 3.保存数据def savaData(savepath): pass","link":"/hexo_blog/2021/08/20/Python%E7%88%AC%E5%8F%96%E7%BE%8E%E5%A5%B3%E5%9B%BE%E7%89%87/"},{"title":"jvm续集","text":"","link":"/hexo_blog/2022/01/22/jvm%E7%BB%AD%E9%9B%86/"},{"title":"python爬取b站视频","text":"[TOC] 起因不知道兄弟们有没有遇到过b站视频经常下架的问题，就比如我现在想在b站找一个老师的课程，运气好可以找到，但经常看了一段时间之后可能就会下架，然后继续找，过不了多久又会下架，这样的循环搞得我好烦呀，这时候我突然想起来万能的python，python爬b站视频咋样？说干就干 前期准备python环境必不可少，同时需要ffmpeg，一台可以上网的电脑 分析首先随便打开一个视频，然后右键查看网页源代码由于展示的源代码格式有点乱，所以我选择复制到vscode中查看这时候看着就舒服多了，细心的朋友可能已经发现baseUrl，然后我们复制一下这个baseUrl，在浏览器中打开康康，果然不出所料还是老实巴交的用python访问吧，header头设置好之后就可以直接访问了，至于header头怎么设置，相信兄弟们都懂，一切就绪之后可以拿到这个文件，值得注意的是b站的视频文件和音频文件是分开存储的，因此我们还需要下载音频文件，音频文件url的位置和视频文件url的位置靠很近，就在audio这里存放着 这样音频文件和视频文件的地址我么都拿到了，随后就可以开始下载了，下载之后使用ffmpeg将两个文件操作一下就可以了。这里有ffmpeg的安装教程，大家可以看一下，ffmpeg安装教程，欢迎三连 代码音频和视频整合的工具类1234567891011import os# 声音视频结合def videoMixAudio(videourl, audiourl, mp4url): com = f'D:\\\\tool\\\\ffmpeg\\\\bin\\\\ffmpeg.exe -i &quot;{audiourl}&quot; -i &quot;{videourl}&quot; ' \\ f'-acodec copy -vcodec copy &quot;{mp4url}&quot;' os.system(com) os.remove(videourl) os.remove(audiourl) 这里解释一下。videourl是视频文件的全路径，audiourl是音频文件的全路径，mp4url则是生成的有声音的视频的位置，然后通过调用ffmpeg来对音频和视频文件进行合成，没有使用ffmpeg的同学们可以采用格式工厂的方式对这两个文件操作进行操作也是可以的。 分析页面使用到了这三个匹配规则 123456# 拿到&lt;script&gt;中的内容valink = re.compile(r'&lt;script&gt;(.*?)&lt;/script&gt;')# 拿到window.__playinfo__后面的内容infoink = re.compile(r'window.__playinfo__=(.*)')# 这个是为了拿到视频的名字 可以选择不用nameink = re.compile(r'window.__INITIAL_STATE__=(.*);\\(function') valink用于取出下图这个部分的内容infoink是为了把window._playinfo_去掉，这样拿到的就是可以转换为json的字符串这样就可以拿到视频和音频的url了代码如下 12345678910111213141516171819def downloadVideo(url): # 获取网页源代码 html = requests.get(url).text # 拿到script的内容 info = re.findall(valink, html) # 第一个就是playinfo的那个script info2 = str(info[0]) # 这一个是存放视频信息的scripte info3 = str(info[1]) nameAnd=re.findall(nameink,info3) # 把window._playinfo_去掉，拿到一个可以转换为json的字符串 videoAndAudio = re.findall(infoink, info2) # str转json jsonobject = json.loads(videoAndAudio[0]) nameobject = json.loads(nameAnd[0]) name=nameobject[&quot;videoData&quot;][&quot;title&quot;] # 获取视频和音频的链接 videoFile = jsonobject[&quot;data&quot;][&quot;dash&quot;][&quot;video&quot;][0][&quot;baseUrl&quot;] audioFile = jsonobject[&quot;data&quot;][&quot;dash&quot;][&quot;audio&quot;][0][&quot;baseUrl&quot;] 拿到链接之后就可以开始下载了，记住下载的时候要设置好header 成果 总结全部代码就不贴了，兄弟们按照这个流程就可以实现，过程还是比较简单的，自己用用就好。希望和大家一起进步","link":"/hexo_blog/2021/08/20/python%E7%88%AC%E5%8F%96b%E7%AB%99%E8%A7%86%E9%A2%91/"},{"title":"ts视频下载","text":"前言之前一直爬取的内容都是完整的文件，例如一整个mp3或则mp4，但是目前很多视频网站都开始采用ts流媒体视频的方式进行视频的展示，不知道你有没有这样的体验，兴致勃勃的打开一个电影网站，准备开始施展爬虫大法查看xhr请求之后，本以为可以找到一个返回mp4的接口，没想到返回的是这一堆ts文件今天我们就来聊一聊怎么下载这些ts文件并将他们拼接为一个mp4 开发工具ffmpeg，pycharm 解决思路首先打开谷歌浏览器，F12，查看xhr请求，这一步相信兄弟们已经轻车熟路了。如下图有两个诡异的m3u8，木错，这就是今天我们的突破口，一般第一个m3u8中存储的都是第二个m3u8文件的url，第二个m3u8文件则是存储的ts文件的urll。因为我们这次主要是讲怎么下载ts文件，所以直接用解析第二个m3u8文件，即可。双击这个请求，就可以查看详情，其中Request URL就是调用的接口或则远程文件，直接调用则会下载该m3u8文件，然后解析一下，拿到ts的url列表就可以进行下载了。先看一下这个m3u8文件的内容很明显文件中存储的不是ts文件的完整地址，需要我们根据实际情况进行拼接就可以，查看的方式就是点击ts文件xhr请求进行查看如下图，很明显，红框圈中的就是我们要拼接在文件名之前的。这就拿到了真实的ts文件地址。那么开整代码吧 代码实现解析m3u8文件，获取ts下载列表要使用到m3u8这个库来解析m3u8文件 123456789101112import m3u8tss = []order = []#realurl就是存储ts文件地址的m3u8文件的url ，这样返回的数据是json格式的，方便读取数据data = m3u8.load(realurl).data# appendurl就是要拼接在前面的那个地址 这样存入tss的ts文件地址都是真实地址# order的作用是在将多个ts文件合成一个mp4时，由这个order提供各ts文件拼接的顺序for i in data[&quot;segments&quot;]: tss.append(appendurl + &quot;/&quot; + i[&quot;uri&quot;]) order.append(i[&quot;uri&quot;]) 到现在为止，ts文件拼接的顺序以及ts文件的真实地址就全部拿到了 多线程下载ts文件，以及ts文件顺序的存储有一说一，这些ts文件不仅多，而且小，如果我们只是一个线程下载文件，未免太浪费时间了，而且效率太低了，这次我们采用多线程的方式进行大量ts文件的下载 总代码12345678910111213141516171819202122232425262728293031323334def download(url, name): #记录创立的线程 task_list = [] # 获取ts的真实地址和顺序 tss, order = getTss(url) # 这里将ts文件顺序存储在m3u8，至于为啥这么做，因为ts文件数量太多了 file = open(&quot;E://file//order.m3u8&quot;, 'w') # 这里将下载ts文件的本地路径输入到order.m3u8之中 for i in order: file.write(f&quot;file 'E:\\\\file\\\\ts\\\\&quot; + i + &quot;'&quot;); file.write(&quot;\\n&quot;) #线程池的创立 pool = ThreadPoolExecutor(max_workers=50) for i in range(0, len(order)): # 启动多个线程下载文件 task_list.append(pool.submit(FileDownload.downloadFile, 'E://file//ts//' + order[i], tss[i])) # 判断所有下载线程是否全部结束 while (True): if len(task_list) == 0: break for i in task_list: if i.done(): task_list.remove(i) # 进行多个ts文件的合并 VideoUtil.mixTss(name) # 合并结束之后把ts文件都删了，不然太占空间了 for u in order: turl = f&quot;E:\\\\file\\\\ts\\\\&quot; + u os.remove(turl) ts文件顺序存储到本地文件中主要代码 1234# 这里将下载ts文件的本地路径输入到order.m3u8之中 for i in order: file.write(f&quot;file 'E:\\\\file\\\\ts\\\\&quot; + i + &quot;'&quot;); file.write(&quot;\\n&quot;) 最终文件中存储的内容最好按照这种格式存入，之前在网上找的其他格式都会报错，但这个是ok的 多线程下载ts文件yysy，多线程真的强，尤其是下载这些小文件，多线程真的是绝了 本文采用线程池的方式，为什么采用线程池呢，因为线程池可以帮我们保留一段时间空闲线程，可以减少线程创建和销毁所耗费的时间，大大提高多线程的效率，同时可以帮助我们限制线程的数量主要代码 123456789101112#线程池的创立 pool = ThreadPoolExecutor(max_workers=50) for i in range(0, len(order)): # 启动多个线程下载文件 task_list.append(pool.submit(FileDownload.downloadFile, 'E://file//ts//' + order[i], tss[i])) # 判断所有下载线程是否全部结束 while (True): if len(task_list) == 0: break for i in task_list: if i.done(): task_list.remove(i) ts文件合成mp4主要思路就是利用刚刚生成的那个ts顺序文件（order.m3u8），按照文件中的顺序进行ts文件的拼接。 这里拼接ts文件时还是要使用ffmpeg，没有的兄弟们可以看下这个安装一下ffmpeg安装教程主要代码 12345def mixTss(name): com = r'D:\\\\tool\\\\ffmpeg\\\\bin\\\\ffmpeg.exe -f concat -safe 0 -i E:\\\\file\\\\order.m3u8 -c copy E:\\\\file\\\\video2\\\\{}.mp4'.format( name) os.system(com) 这里解释一下D:\\tool\\ffmpeg\\bin\\ffmpeg.exe ： 本地ffmpeg的位置，设置了环境变量直接ffmpeg即可 E:\\file\\order.m3u8：刚刚生成的存储ts文件的顺序的文件路径 E:\\file\\video2\\{}.mp4：视频最终合成之后存放的位置 至此，ts视频的下载以及合成一个mp4就实现了 成果ts文件这是下载过程中截的图，有一说一，看着这些文件爆炸式增加，还挺爽 mp4文件具体就不给你们康了，你们猜猜是啥 总结总之没有想象的这么难，做之前以为很复杂，其实还好，最后欢迎各位大佬指点。","link":"/hexo_blog/2021/08/20/ts%E8%A7%86%E9%A2%91%E4%B8%8B%E8%BD%BD/"},{"title":"个人介绍","text":"博主介绍XiaoW 下面就是我的照片，献丑了 说明该个人博客展示的博客多来自我个人的csdn博客","link":"/hexo_blog/2021/08/19/%E4%B8%AA%E4%BA%BA%E4%BB%8B%E7%BB%8D/"},{"title":"python爬取十万张表情包","text":"前言事情要从几天前说起，我有一个朋友，他在和他喜欢的小姐姐聊天时，聊天的气氛一直非常尬，这时他就想发点表情包来缓和一下气氛，但一看自己的表情包收藏都是这样的。。。。。。这发过去，基本就直接和小姐姐说拜拜了，然后他就向我求救问我有没有表情包，表情包我是没有，但网站有呀，来来，爬虫整起。 分析页面今天爬取的网站是斗图吧，有一说一表情包是真的多，看这惊人的页数接下来就该看看怎么拿到表情包图片的url了，首先打开谷歌浏览器，然后点F12进入爬虫快乐模式然后完成下图的操作，先点击1号箭头，然后再选中一个表情包即可，红色框中就是我们要爬取的对象，其中表情包的src就在里面现在我们就搞清楚了怎么拿到表情包的url了，就开始写代码了 具体实现解析页面获取网页内容这里就是获取爬取网页的信息 123456789101112def askURL(url): head = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36&quot; } req = urllib.request.Request(url=url, headers=head) html = &quot;&quot; try: response = urllib.request.urlopen(req) html = response.read() except Exception as result: print(result) return html 解析网页内容123456789101112131415161718192021222324# 取出图片src的正则式imglink = re.compile( r'&lt;img alt=&quot;(.*?)&quot; class=&quot;img-responsive lazy image_dta&quot; data-backup=&quot;.*?&quot; data-original=&quot;(.*?)&quot; referrerpolicy=&quot;no-referrer&quot; src=&quot;.*?&quot;/&gt;', re.S)def getimgsrcs(url): html = askURL(url) bs = BeautifulSoup(html, &quot;html.parser&quot;) names = [] srcs = [] # 找到所有的img标签 for item in bs.find_all('img'): item = str(item) # 根据上面的正则表达式规则把图片的src以及图片名拿下来 imgsrc = re.findall(imglink, item) # 这里是因为拿取的img标签可能不是我们想要的，所以匹配正则规则之后可能返回空值，因此判断一下 if (len(imgsrc) != 0): imgname = &quot;&quot; if imgsrc[0][0] != '': imgname = imgsrc[0][0] + '.' + getFileType(imgsrc[0][1]) else: imgname = getFileName(imgsrc[0][1]) names.append(imgname) srcs.append(imgsrc[0][1]) return names, srcs 到现在为止，已经拿到了所有的图片的链接和名字，那么就可以开始下载了 文件下载多线程下载因为文件实在有点多，所以最好采用多线程的方式下载，我这里只是给了一个样例，大家按照这个逻辑写一下就好 1234pool = ThreadPoolExecutor(max_workers=50) for j in range(len(names)): pool.submit(FileDownload.downloadFile, urls[j], filelocation[j]) 成果 总共是爬了十万多张表情包，这次咱也是表情包大户了 总结很简单的一个爬虫，适合我这样的初学者练练手，如果对爬虫有兴趣的话可以看看我的爬虫专栏的其他文章，说不定也有你喜欢的 爬虫专栏，快来点我呀 两行代码爬取微博热搜，并实现邮件提醒功能，妈妈再也不用担心我吃不到瓜了 爬虫基础 python爬取4k小姐姐图片 人生苦短 我用python python爬b站视频 人生苦短 我用python Python爬取美女图片 爬虫基础 有缘再写，侵权立删","link":"/hexo_blog/2021/08/24/python%E7%88%AC%E5%8F%96%E5%8D%81%E4%B8%87%E5%BC%A0%E8%A1%A8%E6%83%85%E5%8C%85/"},{"title":"玩爬虫不能不知道you-get，快进来看看","text":"@TOC 什么是you-get首先咱们先从字面上分析一下，各位兄弟搬好小板凳，接下来小王利用三年级英语水平给大家翻译一下 首先you的意思是你，get有拿的意思，众所周知英语擅长倒装，没错连起来就是–拿来吧你引入结束，现在回归正题，其实you-get就是一个别人替我们写好的一个下载神器，可以特别方便的在网络上下载资源，具体怎么使用，咱么继续往下看。 you-get的安装前提条件，电脑需要安装python环境，看过博主之前的爬虫文章的兄弟们相信都具备了这个条件，那么接下来就是安装you-get随便打开一个cmd，输入以下内容 1pip3 install --upgrade you-get 安装没有报错，显示内容与与下图显示大致相同，即成功 you-get的使用复制资源，然后在你想要存储文件的目录下打开cmd，也就是在地址栏输入cmd即可，如下图在cmd中输入以下内容 1you-get 视频链接 如下图所示即下载成功打开下载的文件就可以使用了 更多高级操作大家可以在cmd中输入以下内容进行查看 1you-get -h 总结一个小工具的使用，很简单，之前文章中很多人评论，我就专门看了看，最近有时间就写成了博客，下次有缘再写 推荐下自己的爬虫专栏，有几篇基础的爬虫文章，有兴趣的兄弟们可以看看基础爬虫专栏","link":"/hexo_blog/2021/10/22/%E7%8E%A9%E7%88%AC%E8%99%AB%E4%B8%8D%E8%83%BD%E4%B8%8D%E7%9F%A5%E9%81%93you-get%EF%BC%8C%E5%BF%AB%E8%BF%9B%E6%9D%A5%E7%9C%8B%E7%9C%8B/"},{"title":"jvm","text":"JVMjava二进制字节码运行环境 一次编写，到处运行的基础 jvm对外提供了一致的运行环境 自动内存管理机制—垃圾回收功能 数组下标越界检查 有些语言无法进行检查，可能导致越界的数组数据占据了其他程序的空间 多态 jre与jvm jvm+基础类库——–jre jre+编译工具——–jdk 内存结构 java源码—–》二进制字节码——–》解释器翻译为机器语言——–》cpu来执行 程序计数器：记录下一条jvm指令的执行地址， 例如指令如下 121 lalalal2 啦啦啦啦啦 1执行时，会将2放入程序计数器中，待1执行结束之后就在程序计数器中取得2进行执行，这样依次进行执行 一般是使用寄存器来实现的 特点 线程私有 每一个线程都有自己的程序计数器，当分给该线程的时间片结束之后，假如线程还未完成，则需要进行记录下一条指令的地址，等到重新分配时间片时可以继续执行该程序 不会内存溢出 栈每个线程需要一个栈，存放着多个栈帧，一个栈帧对应一个方法，每个方法运行时需要的内存 栈帧 参数 局部变量 返回地址 一个栈中可以有多个栈帧 每个线程只能有一个活动栈帧（） 这里方法调用的栈可以在idea中直观看到 tips 垃圾回收不涉及到栈内存 栈内存大小可通过 -Xss size 来设置程序栈大小 windows取决于虚拟内存大小 linux/macos/Oracle默认为1024kb 并不是栈越大越好，栈越大可运行的线程越少 idea通过以下方式进行设置 局部变量是否线程安全 就看变量是线程私有的还是共享的 线程私有就不需要考虑线程是否安全，共享的话就需要考虑 如果方法内的局部变量未逃离方法作用范围，就是线程安全的，例如如果该变量作为返回值，那么其他线程就可能会拿到这个变量，那么就会导致不安全 栈内存溢出 栈内存放的栈帧数量超出了栈的大小就回导致占内存溢出===》递归就可以做到，一直递归不停，就会产生栈内存溢出 栈帧过大也会导致栈内存溢出 线程运行诊断定位 1234top：定位哪一个进程对cpu占用高ps H -eo pid,tid,%cpu | grep 进程id：进一步定位哪一个线程引起的 jstack 进程id：列出该进程的所有的线程的信息 长时间未输出结果 jstack 进程id 列出所有的线程信息，可以展示出死锁信息 本地方法栈本地方法不是由java编写的，因为java无法直接与计算机底层进行交互，因此需要通过本地方法来进行对底层的交互，一般本地方法是由c或c++编写的。 这些本地方法利用的就是本地方法栈 堆 线程共享的，需要考虑线程安全问题 new创建的对象都是存放在堆 有垃圾回收机制 堆内存溢出不断生成新对象，并且所有对象一直在使用，就会导致堆内存溢出 修改堆空间大小 1-Xmx 8m 以下代码可以用来测试堆空间是否溢出的问题 1234567891011121314151617public static void main(String[] args) { int count=0; String s=&quot;123&quot;; try { List&lt;String&gt; list =new LinkedList&lt;&gt;(); count=0; while(true){ s=s+s; list.add(s); count++; } } catch (Exception e){ e.printStackTrace(); System.out.println(count); } } 启示 服务器内存越跑越小，可能是因为有一些内存未被来得及回收 堆内存诊断 jps工具：查看系统中有哪些java进程 1jps jmap工具：查看堆内存占用情况 1jmap -heap 进程id 测试代码 12345678910111213141516public void testThread(){ try { System.out.println(&quot;1&quot;); Thread.sleep(30000); byte array[]=new byte[1024*1024*10]; System.out.println(&quot;2&quot;); Thread.sleep(30000); array=null; System.gc(); System.out.println(&quot;3&quot;); Thread.sleep(30000); } catch (InterruptedException e) { e.printStackTrace(); } } jconsole工具：有ui的，多功能的检测工具 1jconsole jvisualvm 方法区存放方法，构造器，成员属性之类的数据 方法区在虚拟机启动时就创建，逻辑上是堆的组成部分，但不同的厂商不一定按照这个实现 方法区溢出运行时常量池常量池： 就是一张常量表，虚拟机指令根据这张常量表找到要执行的类名和方法名，参数类型，字面量等信息 运行时常量池： 常量池是*.class中的，当该类被加载，他的常量池信息就会放入运行时常量池中，并且把里面的符号地址变为真实地址 反编译1javap -v Main.class 这里编译的class文件在out文件夹下 如下图就是常量池 常量池加载过程 最开始时常量池中是没有数据的，是在一步步加载中填入的，是一种懒加载机制 常量池存放常量的结构是hash表，每次需要常量时就会以常量在hash表中查找，若不存在则创建 常量池与串池的区别运行常量池（constant pool）中存放的仅仅是符号，而并非对象，串池（StringTable）中存放的则是字符串对象，作用就是防止创建重复的字符对象 1.6和1.8中常量池和串池存放位置的差别 StringTable（串池）的垃圾回收1-Xmx16m -XX: +PrintStringTablestatistics -XX: +PrintGCDetails -verbose:gc -Xmx16m ：设置堆的大小 -XX: +PrintStringTablestatistics ：打印串池中的对象信息 -XX: +PrintGCDetails -verbose:gc : 若存在垃圾回收，则进行打印信息 -XX: StringTableSize=200000 : 因为串池的结构是数组加链表这种方式，数组中的一个关键字称为一个桶，这里就是设计桶的数量，桶的数量越大性能越好，但相对的占用空间就可能过大，造成资源浪费 StringTable性能调优 可以适当调大STringTable的数组长度也就是桶的数量，可以减少冲突从而使得查找效率得到提升 使用串池可对系统性能进行调优，若是new出来的字符串对象只存在堆中，并不会进入串池中，这时若是存在大量的重复的字符串对象，可以采用串池来对这些数据进行去重，所谓去重就是将利用串池的特性将大量的重复的字符串对象只存储一个字符串对象，其他对象只是对其的引用 直接内存操作系统内存 ByteBuffer为什么读写更快使用ByteBuffer实际上就是通过直接内存进行读取 传统io操作 因为java无法直接访问系统资源，因此需要再建立一个java缓冲区，整个过程就是：本地文件==》系统缓存==》java缓存==》使用 直接内存的io方式 此时文件直接放入直接内存缓冲区中，java可以直接读取，减少了一层缓冲区，从而使得速度得到提升 直接内存的溢出因为DM不受java垃圾机制管理，因此可能会出现内存溢出问题 测试代码 直接内存分配与释放的原理通过代码来申请直接内存的大小，这里直接内存不受jvm管理，因此需要在任务管理器里查看 1234ByteBuffer byteBuffer=ByteBuffer.allocateDirect(1024*1024*1024); System.out.println(&quot;try&quot;); System.in.read(); System.out.println(&quot;end&quot;); 直接内存的回收是通过unsafe对象来进行回收的 禁用显示回收的影响1System.gc（） //显式的垃圾回收 关闭显示垃圾回收机制，即System.gc（）无效 1-XX:+DisableExplicitGC 垃圾回收如何判断对象可以回收引用计数法即有一个引用该对象，则计数器加一，为0则释放， 弊端 循环引用：即A引用B,B也引用A，没有其他引用他们，但是他们互相引用，都无法释放，就会导致内存泄漏 可达性分析算法（java中使用的垃圾回收机制）根对象：肯定不可以当作垃圾回收的对象 如果一个对象没有被根对象引用，就可以回收 解析 扫描堆中的对象，看是否能够沿着GC Root对象为起点的引用链找到该对象，找不到，表示可以回收 抓取当前堆使用的快照 1jmap -dump :format=b,live,file=1.bin 21384 -dump ==》存储 format=b ==》存储二进制文件 live ==》只记录那些未被垃圾回收的内容 file=1.bin 设置存储文件 21384 进程id（jps获取活动的java的进程id） mat查看gc root对象 System class 系统对象 Busy Monitor 加锁的对象 Thread 活动线程中的对象，局部对象所引用的对象可左gcroot，同时参数中对象也是可以作为gcroot对象 可以作为GC Root的对象 System class 系统对象 Busy Monitor 加锁的对象 Thread 活动线程中的对象，局部对象所引用的对象可左gcroot，同时参数中对象也是可以作为gcroot对象 四种引用 强引用例如new出来的就是强引用 特点 只要沿着gc root链可以找到该对象，就无法被垃圾回收，例B对A-A4，以及ByteBuffer 只要没有直接或则间接对其强引用之后就可以垃圾回收了 软引用特点 只要未被gc root直接引用，垃圾回收时就会自动回收，例从C到软引用再到A2，当然此时需要B不在引用A2时，就可以发生垃圾回收 应用场景 强引用下导致堆空间溢出 12345678/** * 强引用会导致堆空间不够用 */ int _1M=1024*1024; List&lt;byte[]&gt; list=new LinkedList&lt;&gt;(); for(int i=0;i&lt;5;i++){ list.add(new byte[_1M*2]); } 软引用下 在这种方式下其实就是使用软引用进行嵌套强引用，也就是SoftReference嵌套byte数组，从而达到软引用的目的，这样一旦出现堆内存不够就会进行释放软引用对象 12345List&lt;SoftReference&lt;byte[]&gt;&gt; list=new LinkedList&lt;&gt;();for (int i = 0; i &lt;100 ; i++) { SoftReference softReference=new SoftReference(new byte[_1M*2]); list.add(softReference);} 这个过程中一旦出现了堆空间不够，就会清理软引用对象引用的对象，但是此时软引用对象还在，虽然占据内存比较小，但最好还是清理一下 使用引用队列进行处理，下方代码，关联了软引用队列，软引用关联的对象回收时，软引用对象会加入队列中，从而实现回收 这里我个人的理解就是判断这些软引用有没有引用其他对象，如果没有，则将其在队列中删除，从而将队列对软引用对象的强引用解除掉，从而实现对象的回收 1234567891011121314/** * 关联了软引用队列，软引用关联的对象回收时，软引用对象会加入队列中，从而实现回收 */ ReferenceQueue&lt;byte[]&gt; referenceQueue=new ReferenceQueue&lt;&gt;(); List&lt;SoftReference&lt;byte[]&gt;&gt; list=new LinkedList&lt;&gt;(); for (int i = 0; i &lt;10 ; i++) { SoftReference softReference=new SoftReference(new byte[_1M*2],referenceQueue); list.add(softReference); } Reference&lt;? extends byte[]&gt; poll = referenceQueue.poll(); while(poll!=null){ referenceQueue.remove(); poll=referenceQueue.poll(); } 弱引用特点 当没有强引用时，若内存不够会回收软引用的对象，无论够不够都会回收弱引用对象 释放之后，因为软弱引用仍占用空间，因此需将二者放入引用队列中，进行循环依次释放空间 应用实例 虚引用（必须配合引用队列） 之前的bytebuffer就是需要一个虚引用对象Cleaner，因为ByteBuffer若是在强引用引用结束之后，会对其进行回收，但是此时直接内存不由jvm管理，这就需要把虚引用对象放置在引用队列中，从而实现对直接内存的回收（虚引用对象就是Cleaner，来调用Unsafe的Free memory（）来进行释放） 终结器引用（必须配合引用队列）例如A对象重写了finalize，并且A即将被垃圾回收，会调用finalize方法，将放置一个终结器引用到队列中，会有一个优先级很低的线程会来检查队列中有无需要释放的引用，从而实现对象的回收 垃圾回收算法标记清除算法 判断哪些对象未被gcroot对象引用，对其进行标记 对标记对象进行清除，将对象的首地址存储在队列中，在新的对象分配地址时，会在队列中进行查找，判断有无空间，在进行分配 优点 清除速度快 缺点 会产生大量的碎片空间，导致总剩余空间虽然足够，但有些大空间对象仍无法分配到足够的内存，导致内存溢出 标记整理 判断哪些对象未被gcroot对象直接或间接引用，对其进行标记 清楚时，将可用的对象向前移动，从而使得内存空间更见紧凑，从而实现空间更加连续 优点 没有内存碎片 缺点 耗费时间较多，例如如果有引用对象引用就是将移动的对象，需要修改大量内容，造成浪费时间 复制算法 划分成两片区域，将from中存活的对对象复制到to中，待复制结束之后就对from所有的对象进行回收，然后交换from与to的位置 优点 没有碎片空间 缺点 需要占用双倍的内存空间 小结三种算法都会协同工作 大对象直接到老年代超过新生代大小时，直接到老年代中存放 分代回收 长时间使用的放在老年代中，用完即弃的放在新生代中，也可以认为重要的，常用的在老年代中，而不常使用的在新生代中 清理时先清理新生代，如果内存实在不够，再开始清理老年代 新生代 最开始对象存放在伊甸园中 一旦伊甸园中内存占满之后，就会开始触发垃圾回收（Minor GC，新生代回收） 先进行标记，然后将存活的对象复制到幸存区，将复制的对象寿命+1.然后交换from与to的位置，伊甸园剩余的对象就可以销毁了 然后又可以向伊甸园中分配对象，直到伊甸园又满了，继续执行上述操作，并且也需要测试from中有没有可以回收的，最后在进行回收伊甸园中与幸存区的对象 幸存区中的寿命超过一个阈值（最大寿命15次，存放寿命的数据是4bit，存放在对象里，4位）之后就会晋升到老年代中，若新生代承受不下了，即使没有到达阈值，也会放在老年代中 minor gc会引发 stop the world，在垃圾回收时需暂停其他用户的线程，直到垃圾回收之后在恢复其他线程的运行 老年代 当老年代中内存不足，会先触发minor gc，如果之后空间仍不足，则会触发老年代回收（Full GC），这次回收会将老年代中和新生代中的对象进行回收，也会引起stop the world，并且持续时间更长 老年代中存活的对象很多，并且采用的算法可能是标记清除或标记清理，时间会长。 若full gc 之后仍无从充足空间，则full of mememory error GC参数 含义 参数 堆初始大小 -Xms 堆最大大小 -Xmx或-XX:MaxHeapSize =size 新生代大小 -Xmn或(-XX:NewSize-size + -XX:MaxNewSize-size ) 幸存区比例(动态) -XX:InitialSurvivorRatio-ratio和-XX:+UseAdaptiveSizePolicy 幸存区比例（ratio是指伊甸园所在比例） -XX:SurvivorRatio= ratio 晋升阈值 -XX:MaxTenuringThreshold=threshold 晋升详情 -XX:+PrintTenuringDistribution GC详情 -XX:+PrintGCDetails -verbose:ge FullGC前MinorGC -XX:+ScavengeBeforeFullGC 幸存区比例不会变化的垃圾回收器 -XX: +UseSerialGC 串行垃圾回收器（新生代是复制算法，老年是标记整理算法） -XX:+UseSerialGC= Serial + Serialold 解析垃圾回收信息设置的虚拟机参数： -Xms20M -Xmx20M -Xmn10M -XX:+UseSerialGC -XX:+PrintGCDetails -verbose:ge 控制台信息 new generation : 新生代 这里总容量9m的原因是因为默认认为to不可以被占用，因此就默认减去了1m eden ： 伊甸园 from ：幸存区的from区 to ： 幸存区的 to 区 tenured generatioin ： 老年代 meta space : 元空间 后面的数字就是内存地址 默认的伊甸园占的比例是0.8 内存溢出在子线程会不会引起主线程结束子线程的内存溢出并不会引起主线程结束 垃圾回收器串行 单线程 适用于堆内存小，适合个人电脑 虚拟机参数 -XX:+UseSerialGC= Serial + Serialold 所有的线程须达到安全点之后才可以执行垃圾回收 其他线程需要等到垃圾回收线程结束之后才可以开始继续运行 吞吐量优先（垃圾回收时间占用总时间越少，吞吐量越高） 优先 适用于堆内存较大，多核cpu 单位时间内stw时间最短（总体时间） 虚拟机配置（1.8默认的） 并行的 新生代的（复制算法） 老年代的（标记整理算法） -XX:+UseParallelGC -XX: +UseParallelOldGC 开启上述其中一个，另一个会自动开启 设置垃圾回收线程个数 -XX: ParallelGCThreads=n 自适应调整新生代的大小，晋升阈值也会受影响 -XX;+UseAdaptivesizePolicy 调整吞吐量的目标，调整垃圾回收与总时间的占比 -XX:GCTimeRatio=ratio（垃圾回收时间比例=1/（1+ratio）） 最大暂停毫秒数，最大是200ms -XX:MaxGCPauseMillis=ms 也是所有线程到达安全点之后，就会开始多线程开始回收，线程个数与cpu核数有关，核数有多少，线程的上限就是多少 响应时间优先 多线程 适用于堆内存较大，多核cpu 注重垃圾回收单次stop the world（stw）时间尽可能短 并发进行执行垃圾回收 垃圾回收与普通线程同时进行，两类线程互相争夺时间片 老年代 （标记清除） 新生代 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC - SerialOld 一旦出现并发问题，老年代GC会退化为串行GC 设置并行线程数 设置并发线程数，建议设置为并行线程数的四分之一 -XX :ParallelGCThreads=n -XX:ConcGCThreads=threads 只要老年代到达percent之后，就进行清理，是给浮动垃圾留空间 -XX:CMSInitiating0ccupancyFraction=percent 做重新标记之前，先把新生代做一次垃圾回收 这里的原因是因为有新生代要回收的有很多，而且可能新生代还引用了老年代，但这些新生代本身就已经要被清除了，所以即使我们通过他们发现了一些老年代不能进行回收，但是后期这些新生代本身就要进行回收，实际上了做了无用功，因此可以提前对其进行清理，从而达到减少时间的目的 -XX:+CMSScavengeBeforeRemark 初始标记只会标记根对象，此时很快，但是会引发stw 并发标记会标记间接或直接引用的对象，此时与用户线程并发运行 并发标记后会及逆行重新标记，会引起stw（因为并发标记时，标记的内容可能地址会进行改变，因此需重新标记） 重新标记之后会进行并发处理 并发清理过程中，其他线程可能又会有新垃圾（浮动垃圾），这些垃圾下次处理，因此需要专门设置浮动垃圾空间 笔记 字符串字符串拼接123String a=&quot;123&quot;String b=&quot;55&quot;String c=a+b c的赋值其实是先调用Stringbuilder的toString方法生成一个新的String对象，然后返回给c 但是如下图这样 1234String a=&quot;a&quot;String b=&quot;b&quot;String c=&quot;a&quot;+&quot;b&quot;;String d=&quot;ab&quot;; 此时，c==d是true，因为在编译时，javac会默认认为”a”+”b”就是”ab”，因此直接调用常量池的内容就可以 主动将字符串对象放入串池itern（）：将字符串对象放入串池，若不存在，则放入，否则不进行放入 垃圾回收 原来的占用内存-&gt;回收后的内存，Full GC表示垃圾回收资源太少，因此采用更加强烈的垃圾回收，即软链接垃圾回收 初次回收时会将所有的弱引用对象引用的对象回收掉，若是回收之后内存依然不够，会对软引用在进行回收 并发与并行并发：并发是指两个或多个事件在同一时间间隔发生 并行：并行是指两个或者多个事件在同一时刻发生。","link":"/hexo_blog/2022/01/21/jvm/"}],"tags":[{"name":"Harmony","slug":"Harmony","link":"/hexo_blog/tags/Harmony/"},{"name":"爬虫","slug":"爬虫","link":"/hexo_blog/tags/%E7%88%AC%E8%99%AB/"},{"name":"python","slug":"python","link":"/hexo_blog/tags/python/"},{"name":"工具","slug":"工具","link":"/hexo_blog/tags/%E5%B7%A5%E5%85%B7/"},{"name":"随笔","slug":"随笔","link":"/hexo_blog/tags/%E9%9A%8F%E7%AC%94/"}],"categories":[{"name":"java","slug":"java","link":"/hexo_blog/categories/java/"},{"name":"爬虫","slug":"爬虫","link":"/hexo_blog/categories/%E7%88%AC%E8%99%AB/"},{"name":"python","slug":"python","link":"/hexo_blog/categories/python/"},{"name":"随笔","slug":"随笔","link":"/hexo_blog/categories/%E9%9A%8F%E7%AC%94/"}]}