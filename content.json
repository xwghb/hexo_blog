{"pages":[],"posts":[{"title":"Harmony开发样例","text":"@[TOC](开发一个可以看小姐姐照片的鸿蒙应用 鸿蒙开发入门) 效果图先整张效果图，丑点是丑点，但可以用，买不起鸿蒙系统手机的我，只配用虚拟机。 前言要说目前最火的手机操作系统，要我来看的话那必然是鸿蒙无疑。16号刚刚结束了第五次鸿蒙内测，在看到这次的内测名单之后，居然有970的机器，这是不是说明俺这手里奋战了三年的荣耀play也可以生鸿蒙了，但现实是970三孤儿果然名不虚传，还是没有我们。那么言归正传，这次突然想做一个基于鸿蒙的小demo，然后又想到了我之前爬取的小姐姐图片链接还没有用武之地，这俩是不是可以结合一下？上次有这样的想法还是上一次，那么就做一个看小姐姐的小demo吧，开整开整。 实现思路之前在网上看到有直接把图片下载下来然后放进项目中的，这个很明显不适合我，不仅占的空间大，而且图片还得手动更新，这可不行 我们采用的是调用图片链接接口获取所有的图片链接，一个链接只是一个字符串要比图片占的空间小太多了，将这些链接存储在内存中，有兴趣的可以存在数据库里，然后每次随机获取一条链接就可以，由这条链接获取图片信息，将图片渲染到页面就可以。 整个流程简单的一塌糊涂，总结一下就是 拿取图片链接 由链接获取图片信息 渲染至显示页面 具体实现建立项目这个比较基础了，就不说了，如果不大了解的兄弟们，直接去官方文档看看就可以，建立流程非常简单。 建立http链接拿取图片链接设置网络权限我们需要访问网络，就必须要设置网络权限，来到config.json文件中，添加以下内容 1234567891011121314&quot;reqPermissions&quot;: [ { &quot;name&quot;: &quot;ohos.permission.INTERNET&quot; }, { &quot;name&quot;: &quot;com.wxr.xiaowpic.DataAbilityShellProvider.PROVIDER&quot; }, { &quot;name&quot;: &quot;ohos.permission.GET_NETWORK_INFO&quot; }, { &quot;name&quot;: &quot;ohos.permission.SET_NETWORK_INFO&quot; } ], 直接在module中添加如上内容，如下图 设置允许http请求这里注意，重点哈，鸿蒙默认的是发起https请求，因此如果我们发起http请求是会报错的，这里需要修改一下还是在config.json下，在deviceConfig中添加以下内容 12345&quot;default&quot;: { &quot;network&quot;: { &quot;cleartextTraffic&quot;: true } } 如下图所示，这里吐槽一下，我再寻找怎么设置允许发起http响应的时候，发现好多文章都一样，而且都不适用于我这个项目，还有的人复制别人的文章也能复制错，绝了。 发起http请求，并获取返回的数据json数据解析这里要使用到alibaba的fastjson工具类，在build.gradle引入如下依赖 1implementation group: 'com.alibaba', name: 'fastjson', version: '1.2.73' 如下图所示 发起请求，获得响应内容这里使用的是大佬封装好的专门用于请求接口的一个工具，ZZRHttp，同样需要引入依赖，引入过程和上面fastjson引入过程一致。 1implementation 'com.zzrv5.zzrhttp:ZZRHttp:1.0.1' 获取接口数据的具体实现如下，这里解释一下，https://2fd8e89d.cpolar.io/getAll这个接口地址，是获取图片链接的，是我本地的服务，所以大家如果需要的话，我可以把我的服务代码发给你们，包括存储图片链接的数据库。 12345678910111213141516171819202122232425ZZRHttp.get(&quot;https://2fd8e89d.cpolar.io/getAll&quot;, new ZZRCallBack.CallBackString() { @Override public void onFailure(int code, String errorMessage) { //http访问出错，此部分在主线程中工作,可以更新UI等操做。 MyLabel.error(&quot;访问图片链接接口出错&quot;); new ToastDialog(getContext()).setText(&quot;网络连接出问题了，请稍后重试&quot;).show(); } @Override public void onResponse(String response) { //http访问成功，此部分在主线程中工作，可以更新UI等操作。 MyLabel.info(&quot;获取图片链接成功&quot;); new ToastDialog(getContext()).setText(&quot;正在初始化，稍后&quot;).show(); //将字符串转换为json对象 JSONObject jsonObject = JSONObject.parseObject(response); //将其中返回的图片链接转换为列表 JSONArray info = (JSONArray) jsonObject.get(&quot;info&quot;); MyLabel.info(&quot;拿取数据量：&quot; + info.size()); info.forEach(item -&gt; { srcs.add(item.toString()); }); new ToastDialog(getContext()).setText(&quot;初始化成功，开始你的快乐吧&quot;).show(); MyLabel.info(&quot;内存中数据数量&quot; + srcs.size()); } }); 将获取的图片链接放入内存中就是声明一个静态列表变量，目的是为了下次获取图片链接时可以直接在这里拿取图片链接 123456789101112131415161718192021222324252627282930313233343536373839public void initData() { PicDao picDao = new PicDaoImpl(getContext()); //使用后台线程进行初始化 TaskDispatcher refreshUITask = createParallelTaskDispatcher(&quot;&quot;, TaskPriority.DEFAULT); refreshUITask.syncDispatch(() -&gt; {// List&lt;PicSrc&gt; list = picDao.list(); //判断内存中有无数据 if (srcs.size() == 0) { MyLabel.info(&quot;内存中没东西，第一次打开应用&quot;); MyLabel.info(&quot;调用图片接口获取图片链接列表&quot;); ZZRHttp.get(&quot;https://2fd8e89d.cpolar.io/getAll&quot;, new ZZRCallBack.CallBackString() { @Override public void onFailure(int code, String errorMessage) { //http访问出错，此部分在主线程中工作,可以更新UI等操做。 MyLabel.error(&quot;访问图片链接接口出错&quot;); new ToastDialog(getContext()).setText(&quot;网络连接出问题了，请稍后重试&quot;).show(); } @Override public void onResponse(String response) { //http访问成功，此部分在主线程中工作，可以更新UI等操作。 MyLabel.info(&quot;获取图片链接成功&quot;); new ToastDialog(getContext()).setText(&quot;正在初始化，稍后&quot;).show(); JSONObject jsonObject = JSONObject.parseObject(response); JSONArray info = (JSONArray) jsonObject.get(&quot;info&quot;); MyLabel.info(&quot;拿取数据量：&quot; + info.size()); info.forEach(item -&gt; { srcs.add(item.toString()); }); new ToastDialog(getContext()).setText(&quot;初始化成功，开始你的快乐吧&quot;).show(); MyLabel.info(&quot;内存中数据数量&quot; + srcs.size()); } }); } else { MyLabel.info(&quot;已经有内容了&quot;); } }); } 获取网络图片并展示在页面上http请求工具类这个类的主要作用就是发起http请求，并返回响应字节流，其实就是获取图片的字节流，代码如下 12345678910111213141516171819202122232425262728293031323334353637package com.wxr.xiaowpic.util;import com.wxr.xiaowpic.label.MyLabel;import com.zzrv5.mylibrary.ZZRCallBack;import com.zzrv5.mylibrary.ZZRHttp;import ohos.hiviewdfx.HiLog;import ohos.utils.net.Uri;import java.io.InputStream;import java.net.HttpURLConnection;import java.net.MalformedURLException;import java.net.URL;import java.net.URLConnection;public class HttpUtils { //url就是要访问的网络资源，methodType就是请求方式 public static InputStream getInput(String url,String methodType){ InputStream inputStream = null; try { URL url1=new URL(url); HttpURLConnection urlConnection = (HttpURLConnection) url1.openConnection(); urlConnection.setRequestMethod(methodType); urlConnection.connect(); int rescode=urlConnection.getResponseCode(); if(rescode==HttpURLConnection.HTTP_OK){ inputStream=urlConnection.getInputStream(); } } catch (Exception e) { HiLog.error(MyLabel.LABEL_LOG,e.getMessage()); HiLog.error(MyLabel.LABEL_LOG,e.getCause().toString()); } return inputStream; } } 字节流转图片工具类没啥好说的，复制就可以用 1234567891011121314151617181920212223242526272829303132333435package com.wxr.xiaowpic.util;import com.wxr.xiaowpic.label.MyLabel;import ohos.hiviewdfx.HiLog;import ohos.hiviewdfx.HiLogLabel;import ohos.media.image.ImageSource;import ohos.media.image.PixelMap;import java.io.InputStream;public class ImageUtils { private static final HiLogLabel LABEL_LOG = new HiLogLabel(3, 0xD001100, &quot;XiaoW&quot;); public static PixelMap createPixelMap(String imageUrl) {//获取图片字节流信息 InputStream inputStream = HttpUtils.getInput(imageUrl,&quot;GET&quot;); PixelMap pixelMap=null; ImageSource.SourceOptions sourceOptions = new ImageSource.SourceOptions(); sourceOptions.formatHint = &quot;image/jpeg&quot;; HiLog.info(MyLabel.LABEL_LOG,(inputStream==null)+&quot;&quot;); try { ImageSource imageSource = ImageSource.create(inputStream,sourceOptions); pixelMap = imageSource.createPixelmap(null); } catch (Exception e){ HiLog.info(LABEL_LOG,e.getMessage()); } return pixelMap; }} 图片展示在页面这里采用的是按钮点击之后进行图片的渲染，其中图片链接是在我们获取的图片链接随机读取一个，然后将该照片渲染至页面 123456789101112131415161718192021222324button.setClickedListener(new Component.ClickedListener() { @Override public void onClick(Component component) { TaskDispatcher refreshUITask = createParallelTaskDispatcher(&quot;&quot;, TaskPriority.DEFAULT); refreshUITask.syncDispatch(() -&gt; { //在链接列表中随机取一个数据 int index = (int) (Math.random() * srcs.size()); MyLabel.info(srcs.get(index)); String url=srcs.get(index); MyLabel.info(&quot;开始获取图片&quot;); //访问线上图片 PixelMap pixelMap = ImageUtils.createPixelMap(url); getContext().getUITaskDispatcher().asyncDispatch(new Runnable() { @Override public void run() { //Image组件填充位图数据，ui界面更新 image.setPixelMap(pixelMap); pixelMap.release(); } }); }); } }); 总结之前没有自己做过移动端的demo，总之收获还是不少的，所以期间出了不少问题，需要全部代码的兄弟们私信就好。","link":"/hexo_blog/2021/08/19/Harmony%E5%BC%80%E5%8F%91%E6%A0%B7%E4%BE%8B/"},{"title":"Python爬取美女图片","text":"简述作为一个考研狗，每天除了日复一日的复习外，偶尔也想给自己寻找一些生活的小乐趣，今天突然想到了自己曾经稍微接触的爬虫，想看看可以爬取些图片放到电脑上，就花了些时间改了改之前的爬虫代码，爬取了一部分照片先量一下战绩吧。照片不多但也算是自己的一次爬虫小经验。 实现思路爬虫的网页很简单，照片真实路径都在页面中直接可以拿到主要流程就是先进入照片浏览的主页，每个照片的主页都会链接几个照片页面，像下面这样，每个图片都会链接一个网页图片链接的网页如下图所示但是这个页面显示的图片还是不够高清，这个网站有一个规律，更高清的照片存放的网页就在现在这个页面的路径后跟一个 -1920x1080 的htm中，进入这个htm之后展示的照片才是我们要的，拿到图片的url就直接下载就好，就这样一直循环，所有的照片就都下载下来了。 关键代码文件下载1234567891011121314151617181920212223242526272829303132333435363738394041import requestsimport timedef downloadFile(name, url): try: headers = {'Proxy-Connection': 'keep-alive'} r = requests.get(url, stream=True, headers=headers) print(&quot;=========================&quot;) print(r) length = float(r.headers['Content-length']) f = open(name, 'wb') count = 0 count_tmp = 0 time1 = time.time() for chunk in r.iter_content(chunk_size=512): if chunk: f.write(chunk) # 写入文件 count += len(chunk) # 累加长度 # 计算时间 两秒打印一次 if time.time() - time1 &gt; 2: p = count / length * 100 speed = (count - count_tmp) / 1024 / 1024 / 2 count_tmp = count print(name + ': ' + formatFloat(p) + '%' + ' Speed: ' + formatFloat(speed) + 'M/S') time1 = time.time() f.close() return 1; except: print(&quot;出现异常&quot;) return 0;def formatFloat(num): return '{:.2f}'.format(num)if __name__ == '__main__': downloadFile('D://file//photo//hd.jpg', 'https://browser9.qhimg.com/bdr/__85/t01753453b660de14e9.jpg') 文件下载没什么好说的，复制就可以用，这里做了一个异常捕获的处理，因为可能出现连接不上资源，或则目标服务器强制关闭连接的可能，做这个异常处理就是为了判断有没有异常出现，从而进行相应的处理 爬虫代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# -*- codeing = utf-8 -*-# @Time : 2021/6/19 23:01# @Author : xiaow# @File : PhotoSpider.py# @Software : PyCharmfrom bs4 import BeautifulSoup # 网页解析import xlwt # excel操作import sqlite3 # 数据库操作from api import spider2 as spiderimport timefrom api import FileDownload as fdimport re # 正则表达式imglink = re.compile(r'&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot; title=&quot;.*?&quot;&gt;&lt;img alt=&quot;.*?&quot; src=&quot;.*?&quot;/&gt;&lt;b&gt;.*?&lt;/b&gt;&lt;/a&gt;', re.S)img2link = re.compile(r'&lt;a href=&quot;(.*?)&quot; target=&quot;_blank&quot;&gt;.*?&lt;span&gt;（1680x1050）&lt;/span&gt;&lt;/a&gt;', re.S)img3link = re.compile(r'&lt;img alt=&quot;.*?&quot; src=&quot;(.*?)&quot; title=&quot;.*?&quot;/&gt;', re.S)# 获取照片页面路径def getPhoto(url): srcs = [] html = spider.askURL(url); bs = BeautifulSoup(html, &quot;html.parser&quot;); for item in bs.find_all('a', target=&quot;_blank&quot;): item = str(item) src = re.findall(imglink, item) if (len(src) != 0): srcs.append(&quot;http://www.netbian.com&quot; + src[0]) return srcs;# 照片主页显示的照片不够清楚，这里根据这个网站存储照片的规律，拼接了一个地址，这个地址的照片比较高清一些def getPhotoUrl(url): purls = []; url3 = &quot;http://www&quot;; url2 = url.split(&quot;.&quot;) for j in range(1, len(url2)): if j == len(url2) - 2: url3 = url3 + &quot;.&quot; + url2[j] + &quot;-1920x1080&quot; else: url3 = url3 + &quot;.&quot; + url2[j] return (url3)# 下载照片def downloadPhoto(url): html = spider.askURL(url); bs = BeautifulSoup(html, &quot;html.parser&quot;); for item in bs.find_all(&quot;img&quot;): item=str(item) itemsrc=re.findall(img3link,item) if(len(itemsrc)!=0): return itemsrc[0]if __name__ == '__main__': src = &quot;http://www.netbian.com/mei/index_&quot;; # 拼接照片主页的路径 for i in range(2,163): time.sleep(5) src2 = &quot;&quot;; src2=src+str(i)+&quot;.htm&quot; urls=getPhoto(src2) for j in range(len(urls)): time.sleep(3) fd.downloadFile('D://file//photo//hd'+str(time.time())+&quot;.jpg&quot;,downloadPhoto(getPhotoUrl(urls[j]))) 成果展示几张照片吧 更新解析网页的封装类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#-*- codeing = utf-8 -*-#@Time : 2021/3/1 16:16#@Author : xiaow#@File : spider2.py#@Software : PyCharmimport re # 正则表达式import sysimport urllib.request, urllib.error # 指定url，获取网页数据from bs4 import BeautifulSoup # 网页解析import xlwt # excel操作import sqlite3 # 数据库操作baseurl = 'https://movie.douban.com/top250?start='imglink = re.compile(r'&lt;a href=&quot;.*?&quot; title=&quot;.*?&quot;&gt;', re.S)# titlelink = re.compile(r'&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;')# findlink = re.compile(r'&lt;a href=&quot;(.*?)&quot;&gt;') # 创建正则表达式 表示规则# 1.爬取网页def getData(url): urllist = [] valuelist = [] # 2.解析数据 img = [] src = [] title = [] for i in range(0, 10): url = baseurl + str(i * 25) html = askURL(url) bs = BeautifulSoup(html, &quot;html.parser&quot;) print(bs) # urllist.append(bs.a.attrs[&quot;href&quot;]) # valuelist.append(bs.a.string) # return urllist, valuelist for item in bs.find_all('div', class_=&quot;item&quot;): # 查找div 并且该div应满足class=item # print(item) item = str(item) # titlel = re.findall(titlelink, item) # title.append(titlel) # srcl = re.findall(findlink, item) # 正则表达式进行筛选 # for s in srcl: # src.append(s) imgl = re.findall(imglink, item) # 正则表达式进行筛选 for i in imgl: img.append(i) return title, img, src;# 得到一个url的网页内容1def askURL(url): head = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36&quot;, &quot;Cookie&quot;: '_ga=GA1.2.1191993538.1623990557; _gid=GA1.2.176559558.1623990557; HstCfa3699098=1623990557028; HstCmu3699098=1623990557028; HstCnv3699098=1; HstCns3699098=1; newurl=0; __dtsu=10401623990557D693AE61F09F524965; pbnfgecookieinforecord=%2C64-32128%2C64-32129%2C; HstCla3699098=1623991353818; HstPn3699098=7; HstPt3699098=7' } req = urllib.request.Request(url=url, headers=head) html = &quot;&quot; try: response = urllib.request.urlopen(req) html = response.read() except Exception as result: print(result) return html# 3.保存数据def savaData(savepath): pass","link":"/hexo_blog/2021/08/20/Python%E7%88%AC%E5%8F%96%E7%BE%8E%E5%A5%B3%E5%9B%BE%E7%89%87/"},{"title":"python爬取b站视频","text":"[TOC] 起因不知道兄弟们有没有遇到过b站视频经常下架的问题，就比如我现在想在b站找一个老师的课程，运气好可以找到，但经常看了一段时间之后可能就会下架，然后继续找，过不了多久又会下架，这样的循环搞得我好烦呀，这时候我突然想起来万能的python，python爬b站视频咋样？说干就干 前期准备python环境必不可少，同时需要ffmpeg，一台可以上网的电脑 分析首先随便打开一个视频，然后右键查看网页源代码由于展示的源代码格式有点乱，所以我选择复制到vscode中查看这时候看着就舒服多了，细心的朋友可能已经发现baseUrl，然后我们复制一下这个baseUrl，在浏览器中打开康康，果然不出所料还是老实巴交的用python访问吧，header头设置好之后就可以直接访问了，至于header头怎么设置，相信兄弟们都懂，一切就绪之后可以拿到这个文件，值得注意的是b站的视频文件和音频文件是分开存储的，因此我们还需要下载音频文件，音频文件url的位置和视频文件url的位置靠很近，就在audio这里存放着 这样音频文件和视频文件的地址我么都拿到了，随后就可以开始下载了，下载之后使用ffmpeg将两个文件操作一下就可以了。这里有ffmpeg的安装教程，大家可以看一下，ffmpeg安装教程，欢迎三连 代码音频和视频整合的工具类1234567891011import os# 声音视频结合def videoMixAudio(videourl, audiourl, mp4url): com = f'D:\\\\tool\\\\ffmpeg\\\\bin\\\\ffmpeg.exe -i &quot;{audiourl}&quot; -i &quot;{videourl}&quot; ' \\ f'-acodec copy -vcodec copy &quot;{mp4url}&quot;' os.system(com) os.remove(videourl) os.remove(audiourl) 这里解释一下。videourl是视频文件的全路径，audiourl是音频文件的全路径，mp4url则是生成的有声音的视频的位置，然后通过调用ffmpeg来对音频和视频文件进行合成，没有使用ffmpeg的同学们可以采用格式工厂的方式对这两个文件操作进行操作也是可以的。 分析页面使用到了这三个匹配规则 123456# 拿到&lt;script&gt;中的内容valink = re.compile(r'&lt;script&gt;(.*?)&lt;/script&gt;')# 拿到window.__playinfo__后面的内容infoink = re.compile(r'window.__playinfo__=(.*)')# 这个是为了拿到视频的名字 可以选择不用nameink = re.compile(r'window.__INITIAL_STATE__=(.*);\\(function') valink用于取出下图这个部分的内容infoink是为了把window._playinfo_去掉，这样拿到的就是可以转换为json的字符串这样就可以拿到视频和音频的url了代码如下 12345678910111213141516171819def downloadVideo(url): # 获取网页源代码 html = requests.get(url).text # 拿到script的内容 info = re.findall(valink, html) # 第一个就是playinfo的那个script info2 = str(info[0]) # 这一个是存放视频信息的scripte info3 = str(info[1]) nameAnd=re.findall(nameink,info3) # 把window._playinfo_去掉，拿到一个可以转换为json的字符串 videoAndAudio = re.findall(infoink, info2) # str转json jsonobject = json.loads(videoAndAudio[0]) nameobject = json.loads(nameAnd[0]) name=nameobject[&quot;videoData&quot;][&quot;title&quot;] # 获取视频和音频的链接 videoFile = jsonobject[&quot;data&quot;][&quot;dash&quot;][&quot;video&quot;][0][&quot;baseUrl&quot;] audioFile = jsonobject[&quot;data&quot;][&quot;dash&quot;][&quot;audio&quot;][0][&quot;baseUrl&quot;] 拿到链接之后就可以开始下载了，记住下载的时候要设置好header 成果 总结全部代码就不贴了，兄弟们按照这个流程就可以实现，过程还是比较简单的，自己用用就好。希望和大家一起进步","link":"/hexo_blog/2021/08/20/python%E7%88%AC%E5%8F%96b%E7%AB%99%E8%A7%86%E9%A2%91/"},{"title":"个人介绍","text":"博主介绍XiaoW 下面就是我的照片，献丑了 说明该个人博客展示的博客多来自我个人的csdn博客","link":"/hexo_blog/2021/08/19/%E4%B8%AA%E4%BA%BA%E4%BB%8B%E7%BB%8D/"},{"title":"python爬取十万张表情包","text":"前言事情要从几天前说起，我有一个朋友，他在和他喜欢的小姐姐聊天时，聊天的气氛一直非常尬，这时他就想发点表情包来缓和一下气氛，但一看自己的表情包收藏都是这样的。。。。。。这发过去，基本就直接和小姐姐说拜拜了，然后他就向我求救问我有没有表情包，表情包我是没有，但网站有呀，来来，爬虫整起。 分析页面今天爬取的网站是斗图吧，有一说一表情包是真的多，看这惊人的页数接下来就该看看怎么拿到表情包图片的url了，首先打开谷歌浏览器，然后点F12进入爬虫快乐模式然后完成下图的操作，先点击1号箭头，然后再选中一个表情包即可，红色框中就是我们要爬取的对象，其中表情包的src就在里面现在我们就搞清楚了怎么拿到表情包的url了，就开始写代码了 具体实现解析页面获取网页内容这里就是获取爬取网页的信息 123456789101112def askURL(url): head = { &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.106 Safari/537.36&quot; } req = urllib.request.Request(url=url, headers=head) html = &quot;&quot; try: response = urllib.request.urlopen(req) html = response.read() except Exception as result: print(result) return html 解析网页内容123456789101112131415161718192021222324# 取出图片src的正则式imglink = re.compile( r'&lt;img alt=&quot;(.*?)&quot; class=&quot;img-responsive lazy image_dta&quot; data-backup=&quot;.*?&quot; data-original=&quot;(.*?)&quot; referrerpolicy=&quot;no-referrer&quot; src=&quot;.*?&quot;/&gt;', re.S)def getimgsrcs(url): html = askURL(url) bs = BeautifulSoup(html, &quot;html.parser&quot;) names = [] srcs = [] # 找到所有的img标签 for item in bs.find_all('img'): item = str(item) # 根据上面的正则表达式规则把图片的src以及图片名拿下来 imgsrc = re.findall(imglink, item) # 这里是因为拿取的img标签可能不是我们想要的，所以匹配正则规则之后可能返回空值，因此判断一下 if (len(imgsrc) != 0): imgname = &quot;&quot; if imgsrc[0][0] != '': imgname = imgsrc[0][0] + '.' + getFileType(imgsrc[0][1]) else: imgname = getFileName(imgsrc[0][1]) names.append(imgname) srcs.append(imgsrc[0][1]) return names, srcs 到现在为止，已经拿到了所有的图片的链接和名字，那么就可以开始下载了 文件下载多线程下载因为文件实在有点多，所以最好采用多线程的方式下载，我这里只是给了一个样例，大家按照这个逻辑写一下就好 1234pool = ThreadPoolExecutor(max_workers=50) for j in range(len(names)): pool.submit(FileDownload.downloadFile, urls[j], filelocation[j]) 成果 总共是爬了十万多张表情包，这次咱也是表情包大户了 总结很简单的一个爬虫，适合我这样的初学者练练手，如果对爬虫有兴趣的话可以看看我的爬虫专栏的其他文章，说不定也有你喜欢的 爬虫专栏，快来点我呀 两行代码爬取微博热搜，并实现邮件提醒功能，妈妈再也不用担心我吃不到瓜了 爬虫基础 python爬取4k小姐姐图片 人生苦短 我用python python爬b站视频 人生苦短 我用python Python爬取美女图片 爬虫基础 有缘再写，侵权立删","link":"/hexo_blog/2021/08/24/python%E7%88%AC%E5%8F%96%E5%8D%81%E4%B8%87%E5%BC%A0%E8%A1%A8%E6%83%85%E5%8C%85/"},{"title":"ts视频下载","text":"前言之前一直爬取的内容都是完整的文件，例如一整个mp3或则mp4，但是目前很多视频网站都开始采用ts流媒体视频的方式进行视频的展示，不知道你有没有这样的体验，兴致勃勃的打开一个电影网站，准备开始施展爬虫大法查看xhr请求之后，本以为可以找到一个返回mp4的接口，没想到返回的是这一堆ts文件今天我们就来聊一聊怎么下载这些ts文件并将他们拼接为一个mp4 开发工具ffmpeg，pycharm 解决思路首先打开谷歌浏览器，F12，查看xhr请求，这一步相信兄弟们已经轻车熟路了。如下图有两个诡异的m3u8，木错，这就是今天我们的突破口，一般第一个m3u8中存储的都是第二个m3u8文件的url，第二个m3u8文件则是存储的ts文件的urll。因为我们这次主要是讲怎么下载ts文件，所以直接用解析第二个m3u8文件，即可。双击这个请求，就可以查看详情，其中Request URL就是调用的接口或则远程文件，直接调用则会下载该m3u8文件，然后解析一下，拿到ts的url列表就可以进行下载了。先看一下这个m3u8文件的内容很明显文件中存储的不是ts文件的完整地址，需要我们根据实际情况进行拼接就可以，查看的方式就是点击ts文件xhr请求进行查看如下图，很明显，红框圈中的就是我们要拼接在文件名之前的。这就拿到了真实的ts文件地址。那么开整代码吧 代码实现解析m3u8文件，获取ts下载列表要使用到m3u8这个库来解析m3u8文件 123456789101112import m3u8tss = []order = []#realurl就是存储ts文件地址的m3u8文件的url ，这样返回的数据是json格式的，方便读取数据data = m3u8.load(realurl).data# appendurl就是要拼接在前面的那个地址 这样存入tss的ts文件地址都是真实地址# order的作用是在将多个ts文件合成一个mp4时，由这个order提供各ts文件拼接的顺序for i in data[&quot;segments&quot;]: tss.append(appendurl + &quot;/&quot; + i[&quot;uri&quot;]) order.append(i[&quot;uri&quot;]) 到现在为止，ts文件拼接的顺序以及ts文件的真实地址就全部拿到了 多线程下载ts文件，以及ts文件顺序的存储有一说一，这些ts文件不仅多，而且小，如果我们只是一个线程下载文件，未免太浪费时间了，而且效率太低了，这次我们采用多线程的方式进行大量ts文件的下载 总代码12345678910111213141516171819202122232425262728293031323334def download(url, name): #记录创立的线程 task_list = [] # 获取ts的真实地址和顺序 tss, order = getTss(url) # 这里将ts文件顺序存储在m3u8，至于为啥这么做，因为ts文件数量太多了 file = open(&quot;E://file//order.m3u8&quot;, 'w') # 这里将下载ts文件的本地路径输入到order.m3u8之中 for i in order: file.write(f&quot;file 'E:\\\\file\\\\ts\\\\&quot; + i + &quot;'&quot;); file.write(&quot;\\n&quot;) #线程池的创立 pool = ThreadPoolExecutor(max_workers=50) for i in range(0, len(order)): # 启动多个线程下载文件 task_list.append(pool.submit(FileDownload.downloadFile, 'E://file//ts//' + order[i], tss[i])) # 判断所有下载线程是否全部结束 while (True): if len(task_list) == 0: break for i in task_list: if i.done(): task_list.remove(i) # 进行多个ts文件的合并 VideoUtil.mixTss(name) # 合并结束之后把ts文件都删了，不然太占空间了 for u in order: turl = f&quot;E:\\\\file\\\\ts\\\\&quot; + u os.remove(turl) ts文件顺序存储到本地文件中主要代码 1234# 这里将下载ts文件的本地路径输入到order.m3u8之中 for i in order: file.write(f&quot;file 'E:\\\\file\\\\ts\\\\&quot; + i + &quot;'&quot;); file.write(&quot;\\n&quot;) 最终文件中存储的内容最好按照这种格式存入，之前在网上找的其他格式都会报错，但这个是ok的 多线程下载ts文件yysy，多线程真的强，尤其是下载这些小文件，多线程真的是绝了 本文采用线程池的方式，为什么采用线程池呢，因为线程池可以帮我们保留一段时间空闲线程，可以减少线程创建和销毁所耗费的时间，大大提高多线程的效率，同时可以帮助我们限制线程的数量主要代码 123456789101112#线程池的创立 pool = ThreadPoolExecutor(max_workers=50) for i in range(0, len(order)): # 启动多个线程下载文件 task_list.append(pool.submit(FileDownload.downloadFile, 'E://file//ts//' + order[i], tss[i])) # 判断所有下载线程是否全部结束 while (True): if len(task_list) == 0: break for i in task_list: if i.done(): task_list.remove(i) ts文件合成mp4主要思路就是利用刚刚生成的那个ts顺序文件（order.m3u8），按照文件中的顺序进行ts文件的拼接。 这里拼接ts文件时还是要使用ffmpeg，没有的兄弟们可以看下这个安装一下ffmpeg安装教程主要代码 12345def mixTss(name): com = r'D:\\\\tool\\\\ffmpeg\\\\bin\\\\ffmpeg.exe -f concat -safe 0 -i E:\\\\file\\\\order.m3u8 -c copy E:\\\\file\\\\video2\\\\{}.mp4'.format( name) os.system(com) 这里解释一下D:\\tool\\ffmpeg\\bin\\ffmpeg.exe ： 本地ffmpeg的位置，设置了环境变量直接ffmpeg即可 E:\\file\\order.m3u8：刚刚生成的存储ts文件的顺序的文件路径 E:\\file\\video2\\{}.mp4：视频最终合成之后存放的位置 至此，ts视频的下载以及合成一个mp4就实现了 成果ts文件这是下载过程中截的图，有一说一，看着这些文件爆炸式增加，还挺爽 mp4文件具体就不给你们康了，你们猜猜是啥 总结总之没有想象的这么难，做之前以为很复杂，其实还好，最后欢迎各位大佬指点。","link":"/hexo_blog/2021/08/20/ts%E8%A7%86%E9%A2%91%E4%B8%8B%E8%BD%BD/"}],"tags":[{"name":"Harmony","slug":"Harmony","link":"/hexo_blog/tags/Harmony/"},{"name":"爬虫","slug":"爬虫","link":"/hexo_blog/tags/%E7%88%AC%E8%99%AB/"},{"name":"python","slug":"python","link":"/hexo_blog/tags/python/"},{"name":"随笔","slug":"随笔","link":"/hexo_blog/tags/%E9%9A%8F%E7%AC%94/"},{"name":"工具","slug":"工具","link":"/hexo_blog/tags/%E5%B7%A5%E5%85%B7/"}],"categories":[{"name":"java","slug":"java","link":"/hexo_blog/categories/java/"},{"name":"爬虫","slug":"爬虫","link":"/hexo_blog/categories/%E7%88%AC%E8%99%AB/"},{"name":"随笔","slug":"随笔","link":"/hexo_blog/categories/%E9%9A%8F%E7%AC%94/"},{"name":"python","slug":"python","link":"/hexo_blog/categories/python/"}]}